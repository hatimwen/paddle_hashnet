2022-04-19 09:06:59,372 
Namespace(alpha=0.1, batch_size=64, bit=16, ckp=None, crop_size=224, data={'train_set': {'list_path': 'data/coco/train.txt', 'batch_size': 64}, 'database': {'list_path': 'data/coco/database.txt', 'batch_size': 64}, 'test': {'list_path': 'data/coco/test.txt', 'batch_size': 64}}, data_path='/home/super/public/wht/datasets/COCO2014/', dataset='coco', de_step=50, debug_steps=50, epoch=150, eval=False, eval_epoch=10, last_epoch=0, learning_rate=0.001, log_path='logs_new/', model='HashNet', momentum=0.9, num_class=80, num_train=10000, optimizer='SGD', resize_size=256, resume=None, save_path='checkpoints_new/', seed=0, step_continuation=20, topK=5000, weight_decay=0.0005)
2022-04-19 09:06:59,372 ----- world_size = 2, local_rank = 0
2022-04-19 09:07:00,557 ----- Total # of train batch (single gpu): 157
2022-04-19 09:07:00,558 ----- Total # of test batch (single gpu): 79
2022-04-19 09:07:00,558 ----- Total # of base batch (single gpu): 1754
2022-04-19 09:07:00,558 Start training from epoch 1.
2022-04-19 09:07:07,463 Epoch[001/150], Step[0000/0157], Loss: 1.3377
2022-04-19 09:07:16,509 Epoch[001/150], Step[0050/0157], Loss: 1.1441
2022-04-19 09:07:27,002 Epoch[001/150], Step[0100/0157], Loss: 1.2656
2022-04-19 09:07:36,537 Epoch[001/150], Step[0150/0157], Loss: 1.3171
2022-04-19 09:07:37,900 HashNet[ 1/150] bit:16, lr:0.002000000, scale:1.000, train loss:1.212
2022-04-19 09:07:43,696 Epoch[002/150], Step[0000/0157], Loss: 1.3082
2022-04-19 09:07:53,737 Epoch[002/150], Step[0050/0157], Loss: 1.2286
2022-04-19 09:08:04,177 Epoch[002/150], Step[0100/0157], Loss: 1.1453
2022-04-19 09:08:13,912 Epoch[002/150], Step[0150/0157], Loss: 1.1479
2022-04-19 09:08:15,117 HashNet[ 2/150] bit:16, lr:0.002000000, scale:1.000, train loss:1.196
2022-04-19 09:08:20,853 Epoch[003/150], Step[0000/0157], Loss: 1.1324
2022-04-19 09:08:30,584 Epoch[003/150], Step[0050/0157], Loss: 1.1374
2022-04-19 09:08:40,827 Epoch[003/150], Step[0100/0157], Loss: 1.1185
2022-04-19 09:08:50,741 Epoch[003/150], Step[0150/0157], Loss: 1.1670
2022-04-19 09:08:52,087 HashNet[ 3/150] bit:16, lr:0.002000000, scale:1.000, train loss:1.134
2022-04-19 09:08:57,821 Epoch[004/150], Step[0000/0157], Loss: 1.1062
2022-04-19 09:09:07,529 Epoch[004/150], Step[0050/0157], Loss: 1.1154
2022-04-19 09:09:16,971 Epoch[004/150], Step[0100/0157], Loss: 1.1053
2022-04-19 09:09:26,697 Epoch[004/150], Step[0150/0157], Loss: 1.1059
2022-04-19 09:09:28,138 HashNet[ 4/150] bit:16, lr:0.002000000, scale:1.000, train loss:1.110
2022-04-19 09:09:33,765 Epoch[005/150], Step[0000/0157], Loss: 1.1168
2022-04-19 09:09:44,514 Epoch[005/150], Step[0050/0157], Loss: 1.1086
2022-04-19 09:09:54,439 Epoch[005/150], Step[0100/0157], Loss: 1.0665
2022-04-19 09:10:04,627 Epoch[005/150], Step[0150/0157], Loss: 1.0617
2022-04-19 09:10:05,571 HashNet[ 5/150] bit:16, lr:0.002000000, scale:1.000, train loss:1.095
2022-04-19 09:10:11,445 Epoch[006/150], Step[0000/0157], Loss: 1.0709
2022-04-19 09:10:21,678 Epoch[006/150], Step[0050/0157], Loss: 1.1067
2022-04-19 09:10:31,633 Epoch[006/150], Step[0100/0157], Loss: 1.0400
2022-04-19 09:10:42,174 Epoch[006/150], Step[0150/0157], Loss: 1.0635
2022-04-19 09:10:43,330 HashNet[ 6/150] bit:16, lr:0.002000000, scale:1.000, train loss:1.054
2022-04-19 09:10:49,003 Epoch[007/150], Step[0000/0157], Loss: 1.0457
2022-04-19 09:10:59,100 Epoch[007/150], Step[0050/0157], Loss: 1.0321
2022-04-19 09:11:09,533 Epoch[007/150], Step[0100/0157], Loss: 1.0746
2022-04-19 09:11:19,037 Epoch[007/150], Step[0150/0157], Loss: 1.0417
2022-04-19 09:11:20,369 HashNet[ 7/150] bit:16, lr:0.002000000, scale:1.000, train loss:1.029
2022-04-19 09:11:25,959 Epoch[008/150], Step[0000/0157], Loss: 1.0287
2022-04-19 09:11:36,945 Epoch[008/150], Step[0050/0157], Loss: 0.9936
2022-04-19 09:11:48,541 Epoch[008/150], Step[0100/0157], Loss: 1.0419
2022-04-19 09:11:59,412 Epoch[008/150], Step[0150/0157], Loss: 1.0200
2022-04-19 09:12:00,870 HashNet[ 8/150] bit:16, lr:0.002000000, scale:1.000, train loss:1.017
2022-04-19 09:12:07,475 Epoch[009/150], Step[0000/0157], Loss: 1.0003
2022-04-19 09:12:18,492 Epoch[009/150], Step[0050/0157], Loss: 1.0423
2022-04-19 09:12:29,553 Epoch[009/150], Step[0100/0157], Loss: 1.0179
2022-04-19 09:12:41,102 Epoch[009/150], Step[0150/0157], Loss: 0.9887
2022-04-19 09:12:42,352 HashNet[ 9/150] bit:16, lr:0.002000000, scale:1.000, train loss:1.005
2022-04-19 09:12:49,110 Epoch[010/150], Step[0000/0157], Loss: 0.9955
2022-04-19 09:12:59,659 Epoch[010/150], Step[0050/0157], Loss: 1.0080
2022-04-19 09:13:11,294 Epoch[010/150], Step[0100/0157], Loss: 0.9774
2022-04-19 09:13:22,657 Epoch[010/150], Step[0150/0157], Loss: 0.9782
2022-04-19 09:13:23,868 HashNet[10/150] bit:16, lr:0.002000000, scale:1.000, train loss:0.995
2022-04-19 09:13:23,868 ----- Validation after Epoch: 10
2022-04-19 09:20:36,485 save in checkpoints_new//train-16-20220419-09-06-59/bit_16
2022-04-19 09:20:37,042 Max mAP so far: 0.6037 at epoch_10
2022-04-19 09:20:37,042 ----- Save BEST model: checkpoints_new//train-16-20220419-09-06-59/bit_16/coco.pdparams
2022-04-19 09:20:37,042 ----- Save BEST optim: checkpoints_new//train-16-20220419-09-06-59/bit_16/coco.pdopt
2022-04-19 09:20:37,042 HashNet epoch:10, bit:16, dataset:coco, MAP:0.604, Best MAP(e10): 0.604
2022-04-19 09:20:43,250 Epoch[011/150], Step[0000/0157], Loss: 1.0196
2022-04-19 09:20:52,934 Epoch[011/150], Step[0050/0157], Loss: 1.0022
2022-04-19 09:21:03,208 Epoch[011/150], Step[0100/0157], Loss: 0.9911
2022-04-19 09:21:12,650 Epoch[011/150], Step[0150/0157], Loss: 0.9737
2022-04-19 09:21:13,657 HashNet[11/150] bit:16, lr:0.002000000, scale:1.000, train loss:0.988
2022-04-19 09:21:19,452 Epoch[012/150], Step[0000/0157], Loss: 0.9679
2022-04-19 09:21:29,585 Epoch[012/150], Step[0050/0157], Loss: 0.9496
2022-04-19 09:21:39,188 Epoch[012/150], Step[0100/0157], Loss: 0.9617
2022-04-19 09:21:49,536 Epoch[012/150], Step[0150/0157], Loss: 0.9870
2022-04-19 09:21:50,798 HashNet[12/150] bit:16, lr:0.002000000, scale:1.000, train loss:0.982
2022-04-19 09:21:56,668 Epoch[013/150], Step[0000/0157], Loss: 0.9837
2022-04-19 09:22:06,387 Epoch[013/150], Step[0050/0157], Loss: 0.9848
2022-04-19 09:22:16,367 Epoch[013/150], Step[0100/0157], Loss: 0.9765
2022-04-19 09:22:26,420 Epoch[013/150], Step[0150/0157], Loss: 0.9536
2022-04-19 09:22:27,724 HashNet[13/150] bit:16, lr:0.002000000, scale:1.000, train loss:0.974
2022-04-19 09:22:33,502 Epoch[014/150], Step[0000/0157], Loss: 0.9724
2022-04-19 09:22:42,922 Epoch[014/150], Step[0050/0157], Loss: 0.9777
2022-04-19 09:22:52,872 Epoch[014/150], Step[0100/0157], Loss: 1.0021
2022-04-19 09:23:02,312 Epoch[014/150], Step[0150/0157], Loss: 0.9282
2022-04-19 09:23:03,458 HashNet[14/150] bit:16, lr:0.002000000, scale:1.000, train loss:0.966
2022-04-19 09:23:09,161 Epoch[015/150], Step[0000/0157], Loss: 0.9547
2022-04-19 09:23:19,224 Epoch[015/150], Step[0050/0157], Loss: 0.9410
2022-04-19 09:23:29,284 Epoch[015/150], Step[0100/0157], Loss: 0.9410
2022-04-19 09:23:39,190 Epoch[015/150], Step[0150/0157], Loss: 0.9516
2022-04-19 09:23:40,538 HashNet[15/150] bit:16, lr:0.002000000, scale:1.000, train loss:0.966
2022-04-19 09:23:46,304 Epoch[016/150], Step[0000/0157], Loss: 0.9641
2022-04-19 09:23:56,124 Epoch[016/150], Step[0050/0157], Loss: 0.9393
2022-04-19 09:24:05,862 Epoch[016/150], Step[0100/0157], Loss: 0.9809
2022-04-19 09:24:15,863 Epoch[016/150], Step[0150/0157], Loss: 0.9696
2022-04-19 09:24:17,086 HashNet[16/150] bit:16, lr:0.002000000, scale:1.000, train loss:0.962
2022-04-19 09:24:22,844 Epoch[017/150], Step[0000/0157], Loss: 0.9479
2022-04-19 09:24:33,259 Epoch[017/150], Step[0050/0157], Loss: 0.9546
2022-04-19 09:24:44,332 Epoch[017/150], Step[0100/0157], Loss: 0.9474
2022-04-19 09:24:55,976 Epoch[017/150], Step[0150/0157], Loss: 0.9467
2022-04-19 09:24:57,191 HashNet[17/150] bit:16, lr:0.002000000, scale:1.000, train loss:0.954
2022-04-19 09:25:03,556 Epoch[018/150], Step[0000/0157], Loss: 0.9491
2022-04-19 09:25:14,180 Epoch[018/150], Step[0050/0157], Loss: 0.9670
2022-04-19 09:25:25,358 Epoch[018/150], Step[0100/0157], Loss: 0.9545
2022-04-19 09:25:36,346 Epoch[018/150], Step[0150/0157], Loss: 0.9836
2022-04-19 09:25:37,636 HashNet[18/150] bit:16, lr:0.002000000, scale:1.000, train loss:0.952
2022-04-19 09:25:44,193 Epoch[019/150], Step[0000/0157], Loss: 0.9191
2022-04-19 09:25:54,612 Epoch[019/150], Step[0050/0157], Loss: 0.9574
2022-04-19 09:26:05,467 Epoch[019/150], Step[0100/0157], Loss: 0.9431
2022-04-19 09:26:16,548 Epoch[019/150], Step[0150/0157], Loss: 0.9831
2022-04-19 09:26:18,284 HashNet[19/150] bit:16, lr:0.002000000, scale:1.000, train loss:0.949
2022-04-19 09:26:24,224 Epoch[020/150], Step[0000/0157], Loss: 0.9093
2022-04-19 09:26:34,093 Epoch[020/150], Step[0050/0157], Loss: 0.9279
2022-04-19 09:26:44,224 Epoch[020/150], Step[0100/0157], Loss: 0.9351
2022-04-19 09:26:53,832 Epoch[020/150], Step[0150/0157], Loss: 0.9311
2022-04-19 09:26:54,996 HashNet[20/150] bit:16, lr:0.002000000, scale:1.000, train loss:0.943
2022-04-19 09:26:54,997 ----- Validation after Epoch: 20
2022-04-19 09:34:12,580 save in checkpoints_new//train-16-20220419-09-06-59/bit_16
2022-04-19 09:34:14,726 Max mAP so far: 0.6101 at epoch_20
2022-04-19 09:34:14,726 ----- Save BEST model: checkpoints_new//train-16-20220419-09-06-59/bit_16/coco.pdparams
2022-04-19 09:34:14,726 ----- Save BEST optim: checkpoints_new//train-16-20220419-09-06-59/bit_16/coco.pdopt
2022-04-19 09:34:14,726 HashNet epoch:20, bit:16, dataset:coco, MAP:0.610, Best MAP(e20): 0.610
2022-04-19 09:34:20,386 Epoch[021/150], Step[0000/0157], Loss: 0.9348
2022-04-19 09:34:31,025 Epoch[021/150], Step[0050/0157], Loss: 0.9336
2022-04-19 09:34:41,013 Epoch[021/150], Step[0100/0157], Loss: 0.9689
2022-04-19 09:34:50,838 Epoch[021/150], Step[0150/0157], Loss: 0.9226
2022-04-19 09:34:51,869 HashNet[21/150] bit:16, lr:0.002000000, scale:1.414, train loss:0.936
2022-04-19 09:34:57,654 Epoch[022/150], Step[0000/0157], Loss: 0.9305
2022-04-19 09:35:07,214 Epoch[022/150], Step[0050/0157], Loss: 0.9020
2022-04-19 09:35:16,802 Epoch[022/150], Step[0100/0157], Loss: 0.9219
2022-04-19 09:35:26,852 Epoch[022/150], Step[0150/0157], Loss: 0.9190
2022-04-19 09:35:28,241 HashNet[22/150] bit:16, lr:0.002000000, scale:1.414, train loss:0.932
2022-04-19 09:35:34,376 Epoch[023/150], Step[0000/0157], Loss: 0.9844
2022-04-19 09:35:44,654 Epoch[023/150], Step[0050/0157], Loss: 0.9016
2022-04-19 09:35:54,634 Epoch[023/150], Step[0100/0157], Loss: 0.9159
2022-04-19 09:36:04,785 Epoch[023/150], Step[0150/0157], Loss: 0.9371
2022-04-19 09:36:05,787 HashNet[23/150] bit:16, lr:0.002000000, scale:1.414, train loss:0.926
2022-04-19 09:36:11,680 Epoch[024/150], Step[0000/0157], Loss: 0.9420
2022-04-19 09:36:21,371 Epoch[024/150], Step[0050/0157], Loss: 0.9398
2022-04-19 09:36:31,867 Epoch[024/150], Step[0100/0157], Loss: 0.8956
2022-04-19 09:36:41,952 Epoch[024/150], Step[0150/0157], Loss: 0.9173
2022-04-19 09:36:42,976 HashNet[24/150] bit:16, lr:0.002000000, scale:1.414, train loss:0.920
2022-04-19 09:36:49,051 Epoch[025/150], Step[0000/0157], Loss: 0.9159
2022-04-19 09:36:59,031 Epoch[025/150], Step[0050/0157], Loss: 0.9403
2022-04-19 09:37:09,263 Epoch[025/150], Step[0100/0157], Loss: 0.8929
2022-04-19 09:37:19,002 Epoch[025/150], Step[0150/0157], Loss: 0.8987
2022-04-19 09:37:20,098 HashNet[25/150] bit:16, lr:0.002000000, scale:1.414, train loss:0.917
2022-04-19 09:37:25,904 Epoch[026/150], Step[0000/0157], Loss: 0.9196
2022-04-19 09:37:36,080 Epoch[026/150], Step[0050/0157], Loss: 0.9110
2022-04-19 09:37:47,187 Epoch[026/150], Step[0100/0157], Loss: 0.9127
2022-04-19 09:37:58,480 Epoch[026/150], Step[0150/0157], Loss: 0.9326
2022-04-19 09:37:59,623 HashNet[26/150] bit:16, lr:0.002000000, scale:1.414, train loss:0.918
2022-04-19 09:38:06,276 Epoch[027/150], Step[0000/0157], Loss: 0.8900
2022-04-19 09:38:17,439 Epoch[027/150], Step[0050/0157], Loss: 0.9283
2022-04-19 09:38:29,038 Epoch[027/150], Step[0100/0157], Loss: 0.9029
2022-04-19 09:38:39,780 Epoch[027/150], Step[0150/0157], Loss: 0.9328
2022-04-19 09:38:41,065 HashNet[27/150] bit:16, lr:0.002000000, scale:1.414, train loss:0.912
2022-04-19 09:38:47,116 Epoch[028/150], Step[0000/0157], Loss: 0.9703
2022-04-19 09:38:58,200 Epoch[028/150], Step[0050/0157], Loss: 0.9165
2022-04-19 09:39:08,939 Epoch[028/150], Step[0100/0157], Loss: 0.9280
2022-04-19 09:39:20,518 Epoch[028/150], Step[0150/0157], Loss: 0.9081
2022-04-19 09:39:21,833 HashNet[28/150] bit:16, lr:0.002000000, scale:1.414, train loss:0.909
2022-04-19 09:39:27,758 Epoch[029/150], Step[0000/0157], Loss: 0.9004
2022-04-19 09:39:37,343 Epoch[029/150], Step[0050/0157], Loss: 0.8888
2022-04-19 09:39:47,347 Epoch[029/150], Step[0100/0157], Loss: 0.9413
2022-04-19 09:39:57,482 Epoch[029/150], Step[0150/0157], Loss: 0.9088
2022-04-19 09:39:58,624 HashNet[29/150] bit:16, lr:0.002000000, scale:1.414, train loss:0.906
2022-04-19 09:40:05,655 Epoch[030/150], Step[0000/0157], Loss: 0.9240
2022-04-19 09:40:16,922 Epoch[030/150], Step[0050/0157], Loss: 0.9109
2022-04-19 09:40:28,049 Epoch[030/150], Step[0100/0157], Loss: 0.9035
2022-04-19 09:40:39,963 Epoch[030/150], Step[0150/0157], Loss: 0.9069
2022-04-19 09:40:41,282 HashNet[30/150] bit:16, lr:0.002000000, scale:1.414, train loss:0.906
2022-04-19 09:40:41,282 ----- Validation after Epoch: 30
2022-04-19 09:47:48,755 save in checkpoints_new//train-16-20220419-09-06-59/bit_16
2022-04-19 09:47:50,963 Max mAP so far: 0.6129 at epoch_30
2022-04-19 09:47:50,963 ----- Save BEST model: checkpoints_new//train-16-20220419-09-06-59/bit_16/coco.pdparams
2022-04-19 09:47:50,963 ----- Save BEST optim: checkpoints_new//train-16-20220419-09-06-59/bit_16/coco.pdopt
2022-04-19 09:47:50,964 HashNet epoch:30, bit:16, dataset:coco, MAP:0.613, Best MAP(e30): 0.613
2022-04-19 09:47:56,938 Epoch[031/150], Step[0000/0157], Loss: 0.8723
2022-04-19 09:48:06,778 Epoch[031/150], Step[0050/0157], Loss: 0.9040
2022-04-19 09:48:17,152 Epoch[031/150], Step[0100/0157], Loss: 0.8818
2022-04-19 09:48:26,309 Epoch[031/150], Step[0150/0157], Loss: 0.8877
2022-04-19 09:48:27,531 HashNet[31/150] bit:16, lr:0.002000000, scale:1.414, train loss:0.905
2022-04-19 09:48:33,490 Epoch[032/150], Step[0000/0157], Loss: 0.8873
2022-04-19 09:48:43,055 Epoch[032/150], Step[0050/0157], Loss: 0.9027
2022-04-19 09:48:52,883 Epoch[032/150], Step[0100/0157], Loss: 0.9124
2022-04-19 09:49:02,863 Epoch[032/150], Step[0150/0157], Loss: 0.8714
2022-04-19 09:49:04,100 HashNet[32/150] bit:16, lr:0.002000000, scale:1.414, train loss:0.901
2022-04-19 09:49:09,751 Epoch[033/150], Step[0000/0157], Loss: 0.8692
2022-04-19 09:49:19,172 Epoch[033/150], Step[0050/0157], Loss: 0.8753
2022-04-19 09:49:29,633 Epoch[033/150], Step[0100/0157], Loss: 0.9112
2022-04-19 09:49:39,663 Epoch[033/150], Step[0150/0157], Loss: 0.9142
2022-04-19 09:49:40,723 HashNet[33/150] bit:16, lr:0.002000000, scale:1.414, train loss:0.899
2022-04-19 09:49:46,547 Epoch[034/150], Step[0000/0157], Loss: 0.9494
2022-04-19 09:49:56,956 Epoch[034/150], Step[0050/0157], Loss: 0.9156
2022-04-19 09:50:06,815 Epoch[034/150], Step[0100/0157], Loss: 0.8772
2022-04-19 09:50:16,661 Epoch[034/150], Step[0150/0157], Loss: 0.8918
2022-04-19 09:50:17,773 HashNet[34/150] bit:16, lr:0.002000000, scale:1.414, train loss:0.896
2022-04-19 09:50:23,557 Epoch[035/150], Step[0000/0157], Loss: 0.8539
2022-04-19 09:50:33,710 Epoch[035/150], Step[0050/0157], Loss: 0.8619
2022-04-19 09:50:44,115 Epoch[035/150], Step[0100/0157], Loss: 0.9092
2022-04-19 09:50:55,330 Epoch[035/150], Step[0150/0157], Loss: 0.8623
2022-04-19 09:50:56,463 HashNet[35/150] bit:16, lr:0.002000000, scale:1.414, train loss:0.893
2022-04-19 09:51:03,377 Epoch[036/150], Step[0000/0157], Loss: 0.8729
2022-04-19 09:51:14,076 Epoch[036/150], Step[0050/0157], Loss: 0.9139
2022-04-19 09:51:25,373 Epoch[036/150], Step[0100/0157], Loss: 0.8915
2022-04-19 09:51:36,862 Epoch[036/150], Step[0150/0157], Loss: 0.8889
2022-04-19 09:51:37,933 HashNet[36/150] bit:16, lr:0.002000000, scale:1.414, train loss:0.891
2022-04-19 09:51:44,335 Epoch[037/150], Step[0000/0157], Loss: 0.9411
2022-04-19 09:51:55,139 Epoch[037/150], Step[0050/0157], Loss: 0.9214
2022-04-19 09:52:06,912 Epoch[037/150], Step[0100/0157], Loss: 0.8947
2022-04-19 09:52:18,267 Epoch[037/150], Step[0150/0157], Loss: 0.8947
2022-04-19 09:52:19,581 HashNet[37/150] bit:16, lr:0.002000000, scale:1.414, train loss:0.888
2022-04-19 09:52:26,068 Epoch[038/150], Step[0000/0157], Loss: 0.9212
2022-04-19 09:52:37,006 Epoch[038/150], Step[0050/0157], Loss: 0.8700
2022-04-19 09:52:47,691 Epoch[038/150], Step[0100/0157], Loss: 0.8537
2022-04-19 09:52:58,819 Epoch[038/150], Step[0150/0157], Loss: 0.9066
2022-04-19 09:53:00,220 HashNet[38/150] bit:16, lr:0.002000000, scale:1.414, train loss:0.887
2022-04-19 09:53:06,701 Epoch[039/150], Step[0000/0157], Loss: 0.8765
2022-04-19 09:53:17,534 Epoch[039/150], Step[0050/0157], Loss: 0.9059
2022-04-19 09:53:28,747 Epoch[039/150], Step[0100/0157], Loss: 0.8743
2022-04-19 09:53:39,984 Epoch[039/150], Step[0150/0157], Loss: 0.8414
2022-04-19 09:53:41,187 HashNet[39/150] bit:16, lr:0.002000000, scale:1.414, train loss:0.884
2022-04-19 09:53:47,814 Epoch[040/150], Step[0000/0157], Loss: 0.9070
2022-04-19 09:53:59,264 Epoch[040/150], Step[0050/0157], Loss: 0.8833
2022-04-19 09:54:10,104 Epoch[040/150], Step[0100/0157], Loss: 0.9010
2022-04-19 09:54:21,089 Epoch[040/150], Step[0150/0157], Loss: 0.8827
2022-04-19 09:54:22,391 HashNet[40/150] bit:16, lr:0.002000000, scale:1.414, train loss:0.882
2022-04-19 09:54:22,391 ----- Validation after Epoch: 40
2022-04-19 10:01:25,611 HashNet epoch:40, bit:16, dataset:coco, MAP:0.612, Best MAP(e30): 0.613
2022-04-19 10:01:31,330 Epoch[041/150], Step[0000/0157], Loss: 0.8739
2022-04-19 10:01:41,794 Epoch[041/150], Step[0050/0157], Loss: 0.8627
2022-04-19 10:01:51,954 Epoch[041/150], Step[0100/0157], Loss: 0.8635
2022-04-19 10:02:02,774 Epoch[041/150], Step[0150/0157], Loss: 0.8840
2022-04-19 10:02:03,806 HashNet[41/150] bit:16, lr:0.002000000, scale:1.732, train loss:0.879
2022-04-19 10:02:09,422 Epoch[042/150], Step[0000/0157], Loss: 0.8746
2022-04-19 10:02:19,651 Epoch[042/150], Step[0050/0157], Loss: 0.8804
2022-04-19 10:02:30,180 Epoch[042/150], Step[0100/0157], Loss: 0.8974
2022-04-19 10:02:40,760 Epoch[042/150], Step[0150/0157], Loss: 0.8584
2022-04-19 10:02:41,810 HashNet[42/150] bit:16, lr:0.002000000, scale:1.732, train loss:0.875
2022-04-19 10:02:47,878 Epoch[043/150], Step[0000/0157], Loss: 0.8273
2022-04-19 10:02:57,864 Epoch[043/150], Step[0050/0157], Loss: 0.8732
2022-04-19 10:03:07,782 Epoch[043/150], Step[0100/0157], Loss: 0.8738
2022-04-19 10:03:17,207 Epoch[043/150], Step[0150/0157], Loss: 0.9155
2022-04-19 10:03:18,486 HashNet[43/150] bit:16, lr:0.002000000, scale:1.732, train loss:0.875
2022-04-19 10:03:24,362 Epoch[044/150], Step[0000/0157], Loss: 0.8493
2022-04-19 10:03:34,520 Epoch[044/150], Step[0050/0157], Loss: 0.8678
2022-04-19 10:03:45,031 Epoch[044/150], Step[0100/0157], Loss: 0.8449
2022-04-19 10:03:55,750 Epoch[044/150], Step[0150/0157], Loss: 0.9013
2022-04-19 10:03:56,909 HashNet[44/150] bit:16, lr:0.002000000, scale:1.732, train loss:0.874
2022-04-19 10:04:03,118 Epoch[045/150], Step[0000/0157], Loss: 0.8581
2022-04-19 10:04:15,737 Epoch[045/150], Step[0050/0157], Loss: 0.8708
2022-04-19 10:04:26,067 Epoch[045/150], Step[0100/0157], Loss: 0.8737
2022-04-19 10:04:36,565 Epoch[045/150], Step[0150/0157], Loss: 0.8530
2022-04-19 10:04:37,861 HashNet[45/150] bit:16, lr:0.002000000, scale:1.732, train loss:0.873
2022-04-19 10:04:44,041 Epoch[046/150], Step[0000/0157], Loss: 0.8960
2022-04-19 10:04:55,948 Epoch[046/150], Step[0050/0157], Loss: 0.8653
2022-04-19 10:05:08,370 Epoch[046/150], Step[0100/0157], Loss: 0.8374
2022-04-19 10:05:20,026 Epoch[046/150], Step[0150/0157], Loss: 0.8905
2022-04-19 10:05:21,228 HashNet[46/150] bit:16, lr:0.002000000, scale:1.732, train loss:0.874
2022-04-19 10:05:27,341 Epoch[047/150], Step[0000/0157], Loss: 0.8531
2022-04-19 10:05:39,943 Epoch[047/150], Step[0050/0157], Loss: 0.8853
2022-04-19 10:05:50,596 Epoch[047/150], Step[0100/0157], Loss: 0.8706
2022-04-19 10:06:02,014 Epoch[047/150], Step[0150/0157], Loss: 0.8674
2022-04-19 10:06:03,202 HashNet[47/150] bit:16, lr:0.002000000, scale:1.732, train loss:0.872
2022-04-19 10:06:09,484 Epoch[048/150], Step[0000/0157], Loss: 0.8549
2022-04-19 10:06:20,472 Epoch[048/150], Step[0050/0157], Loss: 0.8570
2022-04-19 10:06:31,910 Epoch[048/150], Step[0100/0157], Loss: 0.8713
2022-04-19 10:06:42,757 Epoch[048/150], Step[0150/0157], Loss: 0.8514
2022-04-19 10:06:43,919 HashNet[48/150] bit:16, lr:0.002000000, scale:1.732, train loss:0.868
2022-04-19 10:06:50,111 Epoch[049/150], Step[0000/0157], Loss: 0.8473
2022-04-19 10:07:01,428 Epoch[049/150], Step[0050/0157], Loss: 0.8705
2022-04-19 10:07:12,760 Epoch[049/150], Step[0100/0157], Loss: 0.8597
2022-04-19 10:07:23,874 Epoch[049/150], Step[0150/0157], Loss: 0.8740
2022-04-19 10:07:25,454 HashNet[49/150] bit:16, lr:0.002000000, scale:1.732, train loss:0.867
2022-04-19 10:07:31,481 Epoch[050/150], Step[0000/0157], Loss: 0.8666
2022-04-19 10:07:41,172 Epoch[050/150], Step[0050/0157], Loss: 0.8578
2022-04-19 10:07:50,776 Epoch[050/150], Step[0100/0157], Loss: 0.8849
2022-04-19 10:08:00,861 Epoch[050/150], Step[0150/0157], Loss: 0.8760
2022-04-19 10:08:02,078 HashNet[50/150] bit:16, lr:0.002000000, scale:1.732, train loss:0.867
2022-04-19 10:08:02,078 ----- Validation after Epoch: 50
2022-04-19 10:15:00,192 save in checkpoints_new//train-16-20220419-09-06-59/bit_16
2022-04-19 10:15:02,732 Max mAP so far: 0.6162 at epoch_50
2022-04-19 10:15:02,732 ----- Save BEST model: checkpoints_new//train-16-20220419-09-06-59/bit_16/coco.pdparams
2022-04-19 10:15:02,732 ----- Save BEST optim: checkpoints_new//train-16-20220419-09-06-59/bit_16/coco.pdopt
2022-04-19 10:15:02,732 HashNet epoch:50, bit:16, dataset:coco, MAP:0.616, Best MAP(e50): 0.616
2022-04-19 10:15:08,768 Epoch[051/150], Step[0000/0157], Loss: 0.8965
2022-04-19 10:15:18,534 Epoch[051/150], Step[0050/0157], Loss: 0.8703
2022-04-19 10:15:28,306 Epoch[051/150], Step[0100/0157], Loss: 0.8705
2022-04-19 10:15:37,659 Epoch[051/150], Step[0150/0157], Loss: 0.9019
2022-04-19 10:15:38,679 HashNet[51/150] bit:16, lr:0.001000000, scale:1.732, train loss:0.862
2022-04-19 10:15:44,747 Epoch[052/150], Step[0000/0157], Loss: 0.8657
2022-04-19 10:15:54,789 Epoch[052/150], Step[0050/0157], Loss: 0.8638
2022-04-19 10:16:05,624 Epoch[052/150], Step[0100/0157], Loss: 0.8547
2022-04-19 10:16:15,085 Epoch[052/150], Step[0150/0157], Loss: 0.8567
2022-04-19 10:16:16,099 HashNet[52/150] bit:16, lr:0.001000000, scale:1.732, train loss:0.861
2022-04-19 10:16:21,937 Epoch[053/150], Step[0000/0157], Loss: 0.8816
2022-04-19 10:16:31,831 Epoch[053/150], Step[0050/0157], Loss: 0.8586
2022-04-19 10:16:42,751 Epoch[053/150], Step[0100/0157], Loss: 0.8590
2022-04-19 10:16:52,563 Epoch[053/150], Step[0150/0157], Loss: 0.8371
2022-04-19 10:16:54,001 HashNet[53/150] bit:16, lr:0.001000000, scale:1.732, train loss:0.859
2022-04-19 10:17:00,579 Epoch[054/150], Step[0000/0157], Loss: 0.8951
2022-04-19 10:17:11,543 Epoch[054/150], Step[0050/0157], Loss: 0.8315
2022-04-19 10:17:22,124 Epoch[054/150], Step[0100/0157], Loss: 0.8622
2022-04-19 10:17:33,424 Epoch[054/150], Step[0150/0157], Loss: 0.8974
2022-04-19 10:17:34,762 HashNet[54/150] bit:16, lr:0.001000000, scale:1.732, train loss:0.856
2022-04-19 10:17:41,048 Epoch[055/150], Step[0000/0157], Loss: 0.8611
2022-04-19 10:17:51,853 Epoch[055/150], Step[0050/0157], Loss: 0.8379
2022-04-19 10:18:03,358 Epoch[055/150], Step[0100/0157], Loss: 0.8943
2022-04-19 10:18:14,357 Epoch[055/150], Step[0150/0157], Loss: 0.8568
2022-04-19 10:18:15,520 HashNet[55/150] bit:16, lr:0.001000000, scale:1.732, train loss:0.857
2022-04-19 10:18:21,913 Epoch[056/150], Step[0000/0157], Loss: 0.8319
2022-04-19 10:18:34,570 Epoch[056/150], Step[0050/0157], Loss: 0.9039
2022-04-19 10:18:48,270 Epoch[056/150], Step[0100/0157], Loss: 0.8336
2022-04-19 10:19:00,425 Epoch[056/150], Step[0150/0157], Loss: 0.8397
2022-04-19 10:19:01,889 HashNet[56/150] bit:16, lr:0.001000000, scale:1.732, train loss:0.859
2022-04-19 10:19:08,264 Epoch[057/150], Step[0000/0157], Loss: 0.8424
2022-04-19 10:19:19,298 Epoch[057/150], Step[0050/0157], Loss: 0.8117
2022-04-19 10:19:30,817 Epoch[057/150], Step[0100/0157], Loss: 0.8580
2022-04-19 10:19:42,642 Epoch[057/150], Step[0150/0157], Loss: 0.8513
2022-04-19 10:19:43,802 HashNet[57/150] bit:16, lr:0.001000000, scale:1.732, train loss:0.858
2022-04-19 10:19:50,282 Epoch[058/150], Step[0000/0157], Loss: 0.8074
2022-04-19 10:20:01,243 Epoch[058/150], Step[0050/0157], Loss: 0.8650
2022-04-19 10:20:12,603 Epoch[058/150], Step[0100/0157], Loss: 0.8429
2022-04-19 10:20:23,958 Epoch[058/150], Step[0150/0157], Loss: 0.8430
2022-04-19 10:20:25,200 HashNet[58/150] bit:16, lr:0.001000000, scale:1.732, train loss:0.855
2022-04-19 10:20:31,169 Epoch[059/150], Step[0000/0157], Loss: 0.8625
2022-04-19 10:20:41,364 Epoch[059/150], Step[0050/0157], Loss: 0.8437
2022-04-19 10:20:50,861 Epoch[059/150], Step[0100/0157], Loss: 0.8375
2022-04-19 10:21:00,887 Epoch[059/150], Step[0150/0157], Loss: 0.9057
2022-04-19 10:21:02,146 HashNet[59/150] bit:16, lr:0.001000000, scale:1.732, train loss:0.856
2022-04-19 10:21:08,078 Epoch[060/150], Step[0000/0157], Loss: 0.8230
2022-04-19 10:21:17,966 Epoch[060/150], Step[0050/0157], Loss: 0.8600
2022-04-19 10:21:27,902 Epoch[060/150], Step[0100/0157], Loss: 0.8630
2022-04-19 10:21:38,058 Epoch[060/150], Step[0150/0157], Loss: 0.8763
2022-04-19 10:21:39,101 HashNet[60/150] bit:16, lr:0.001000000, scale:1.732, train loss:0.856
2022-04-19 10:21:39,101 ----- Validation after Epoch: 60
2022-04-19 10:28:41,639 HashNet epoch:60, bit:16, dataset:coco, MAP:0.614, Best MAP(e50): 0.616
2022-04-19 10:28:47,374 Epoch[061/150], Step[0000/0157], Loss: 0.8341
2022-04-19 10:28:57,203 Epoch[061/150], Step[0050/0157], Loss: 0.8227
2022-04-19 10:29:06,663 Epoch[061/150], Step[0100/0157], Loss: 0.9170
2022-04-19 10:29:16,545 Epoch[061/150], Step[0150/0157], Loss: 0.8537
2022-04-19 10:29:17,728 HashNet[61/150] bit:16, lr:0.001000000, scale:2.000, train loss:0.855
2022-04-19 10:29:23,429 Epoch[062/150], Step[0000/0157], Loss: 0.8386
2022-04-19 10:29:33,302 Epoch[062/150], Step[0050/0157], Loss: 0.8412
2022-04-19 10:29:43,606 Epoch[062/150], Step[0100/0157], Loss: 0.8442
2022-04-19 10:29:53,218 Epoch[062/150], Step[0150/0157], Loss: 0.8634
2022-04-19 10:29:54,625 HashNet[62/150] bit:16, lr:0.001000000, scale:2.000, train loss:0.853
2022-04-19 10:30:00,917 Epoch[063/150], Step[0000/0157], Loss: 0.8278
2022-04-19 10:30:11,467 Epoch[063/150], Step[0050/0157], Loss: 0.8731
2022-04-19 10:30:23,076 Epoch[063/150], Step[0100/0157], Loss: 0.8787
2022-04-19 10:30:34,092 Epoch[063/150], Step[0150/0157], Loss: 0.8095
2022-04-19 10:30:35,346 HashNet[63/150] bit:16, lr:0.001000000, scale:2.000, train loss:0.852
2022-04-19 10:30:41,873 Epoch[064/150], Step[0000/0157], Loss: 0.8442
2022-04-19 10:30:53,152 Epoch[064/150], Step[0050/0157], Loss: 0.8407
2022-04-19 10:31:04,169 Epoch[064/150], Step[0100/0157], Loss: 0.8161
2022-04-19 10:31:15,291 Epoch[064/150], Step[0150/0157], Loss: 0.8441
2022-04-19 10:31:16,667 HashNet[64/150] bit:16, lr:0.001000000, scale:2.000, train loss:0.850
2022-04-19 10:31:24,236 Epoch[065/150], Step[0000/0157], Loss: 0.8791
2022-04-19 10:31:37,576 Epoch[065/150], Step[0050/0157], Loss: 0.8555
2022-04-19 10:31:51,549 Epoch[065/150], Step[0100/0157], Loss: 0.8462
2022-04-19 10:32:03,828 Epoch[065/150], Step[0150/0157], Loss: 0.8587
2022-04-19 10:32:04,964 HashNet[65/150] bit:16, lr:0.001000000, scale:2.000, train loss:0.853
2022-04-19 10:32:11,507 Epoch[066/150], Step[0000/0157], Loss: 0.8543
2022-04-19 10:32:22,852 Epoch[066/150], Step[0050/0157], Loss: 0.8597
2022-04-19 10:32:34,222 Epoch[066/150], Step[0100/0157], Loss: 0.8445
2022-04-19 10:32:46,058 Epoch[066/150], Step[0150/0157], Loss: 0.8656
2022-04-19 10:32:47,122 HashNet[66/150] bit:16, lr:0.001000000, scale:2.000, train loss:0.852
2022-04-19 10:32:54,053 Epoch[067/150], Step[0000/0157], Loss: 0.8750
2022-04-19 10:33:05,229 Epoch[067/150], Step[0050/0157], Loss: 0.8645
2022-04-19 10:33:16,273 Epoch[067/150], Step[0100/0157], Loss: 0.8355
2022-04-19 10:33:26,367 Epoch[067/150], Step[0150/0157], Loss: 0.8148
2022-04-19 10:33:27,428 HashNet[67/150] bit:16, lr:0.001000000, scale:2.000, train loss:0.850
2022-04-19 10:33:33,474 Epoch[068/150], Step[0000/0157], Loss: 0.8733
2022-04-19 10:33:43,401 Epoch[068/150], Step[0050/0157], Loss: 0.8535
2022-04-19 10:33:53,508 Epoch[068/150], Step[0100/0157], Loss: 0.8063
2022-04-19 10:34:03,245 Epoch[068/150], Step[0150/0157], Loss: 0.8735
2022-04-19 10:34:04,420 HashNet[68/150] bit:16, lr:0.001000000, scale:2.000, train loss:0.850
2022-04-19 10:34:10,242 Epoch[069/150], Step[0000/0157], Loss: 0.8634
2022-04-19 10:34:21,152 Epoch[069/150], Step[0050/0157], Loss: 0.8975
2022-04-19 10:34:30,597 Epoch[069/150], Step[0100/0157], Loss: 0.8769
2022-04-19 10:34:40,241 Epoch[069/150], Step[0150/0157], Loss: 0.8239
2022-04-19 10:34:41,705 HashNet[69/150] bit:16, lr:0.001000000, scale:2.000, train loss:0.849
2022-04-19 10:34:47,763 Epoch[070/150], Step[0000/0157], Loss: 0.8852
2022-04-19 10:34:57,818 Epoch[070/150], Step[0050/0157], Loss: 0.8832
2022-04-19 10:35:07,968 Epoch[070/150], Step[0100/0157], Loss: 0.8649
2022-04-19 10:35:17,960 Epoch[070/150], Step[0150/0157], Loss: 0.8591
2022-04-19 10:35:19,055 HashNet[70/150] bit:16, lr:0.001000000, scale:2.000, train loss:0.849
2022-04-19 10:35:19,055 ----- Validation after Epoch: 70
2022-04-19 10:42:18,983 HashNet epoch:70, bit:16, dataset:coco, MAP:0.612, Best MAP(e50): 0.616
2022-04-19 10:42:24,812 Epoch[071/150], Step[0000/0157], Loss: 0.8354
2022-04-19 10:42:34,548 Epoch[071/150], Step[0050/0157], Loss: 0.8458
2022-04-19 10:42:43,982 Epoch[071/150], Step[0100/0157], Loss: 0.8321
2022-04-19 10:42:54,436 Epoch[071/150], Step[0150/0157], Loss: 0.8663
2022-04-19 10:42:55,527 HashNet[71/150] bit:16, lr:0.001000000, scale:2.000, train loss:0.847
2022-04-19 10:43:01,133 Epoch[072/150], Step[0000/0157], Loss: 0.8752
2022-04-19 10:43:11,726 Epoch[072/150], Step[0050/0157], Loss: 0.8331
2022-04-19 10:43:22,746 Epoch[072/150], Step[0100/0157], Loss: 0.8343
2022-04-19 10:43:33,607 Epoch[072/150], Step[0150/0157], Loss: 0.8195
2022-04-19 10:43:34,968 HashNet[72/150] bit:16, lr:0.001000000, scale:2.000, train loss:0.845
2022-04-19 10:43:41,394 Epoch[073/150], Step[0000/0157], Loss: 0.8630
2022-04-19 10:43:52,014 Epoch[073/150], Step[0050/0157], Loss: 0.8648
2022-04-19 10:44:03,272 Epoch[073/150], Step[0100/0157], Loss: 0.8399
2022-04-19 10:44:14,630 Epoch[073/150], Step[0150/0157], Loss: 0.8833
2022-04-19 10:44:16,309 HashNet[73/150] bit:16, lr:0.001000000, scale:2.000, train loss:0.845
2022-04-19 10:44:23,692 Epoch[074/150], Step[0000/0157], Loss: 0.8392
2022-04-19 10:44:37,806 Epoch[074/150], Step[0050/0157], Loss: 0.8154
2022-04-19 10:44:51,695 Epoch[074/150], Step[0100/0157], Loss: 0.8557
2022-04-19 10:45:05,667 Epoch[074/150], Step[0150/0157], Loss: 0.8445
2022-04-19 10:45:07,187 HashNet[74/150] bit:16, lr:0.001000000, scale:2.000, train loss:0.847
2022-04-19 10:45:14,621 Epoch[075/150], Step[0000/0157], Loss: 0.8588
2022-04-19 10:45:25,391 Epoch[075/150], Step[0050/0157], Loss: 0.8507
2022-04-19 10:45:36,208 Epoch[075/150], Step[0100/0157], Loss: 0.8517
2022-04-19 10:45:47,473 Epoch[075/150], Step[0150/0157], Loss: 0.8571
2022-04-19 10:45:48,669 HashNet[75/150] bit:16, lr:0.001000000, scale:2.000, train loss:0.847
2022-04-19 10:45:55,630 Epoch[076/150], Step[0000/0157], Loss: 0.8251
2022-04-19 10:46:06,631 Epoch[076/150], Step[0050/0157], Loss: 0.8028
2022-04-19 10:46:17,232 Epoch[076/150], Step[0100/0157], Loss: 0.8289
2022-04-19 10:46:27,502 Epoch[076/150], Step[0150/0157], Loss: 0.8447
2022-04-19 10:46:28,537 HashNet[76/150] bit:16, lr:0.001000000, scale:2.000, train loss:0.847
2022-04-19 10:46:34,281 Epoch[077/150], Step[0000/0157], Loss: 0.8738
2022-04-19 10:46:44,194 Epoch[077/150], Step[0050/0157], Loss: 0.8273
2022-04-19 10:46:54,226 Epoch[077/150], Step[0100/0157], Loss: 0.8509
2022-04-19 10:47:04,792 Epoch[077/150], Step[0150/0157], Loss: 0.8480
2022-04-19 10:47:05,862 HashNet[77/150] bit:16, lr:0.001000000, scale:2.000, train loss:0.846
2022-04-19 10:47:11,653 Epoch[078/150], Step[0000/0157], Loss: 0.8162
2022-04-19 10:47:21,772 Epoch[078/150], Step[0050/0157], Loss: 0.8746
2022-04-19 10:47:31,944 Epoch[078/150], Step[0100/0157], Loss: 0.8647
2022-04-19 10:47:42,076 Epoch[078/150], Step[0150/0157], Loss: 0.8490
2022-04-19 10:47:43,197 HashNet[78/150] bit:16, lr:0.001000000, scale:2.000, train loss:0.846
2022-04-19 10:47:49,111 Epoch[079/150], Step[0000/0157], Loss: 0.8350
2022-04-19 10:47:59,067 Epoch[079/150], Step[0050/0157], Loss: 0.8244
2022-04-19 10:48:09,007 Epoch[079/150], Step[0100/0157], Loss: 0.8408
2022-04-19 10:48:19,001 Epoch[079/150], Step[0150/0157], Loss: 0.8417
2022-04-19 10:48:20,280 HashNet[79/150] bit:16, lr:0.001000000, scale:2.000, train loss:0.846
2022-04-19 10:48:26,065 Epoch[080/150], Step[0000/0157], Loss: 0.8576
2022-04-19 10:48:36,102 Epoch[080/150], Step[0050/0157], Loss: 0.8616
2022-04-19 10:48:46,449 Epoch[080/150], Step[0100/0157], Loss: 0.8522
2022-04-19 10:48:56,507 Epoch[080/150], Step[0150/0157], Loss: 0.8787
2022-04-19 10:48:57,552 HashNet[80/150] bit:16, lr:0.001000000, scale:2.000, train loss:0.846
2022-04-19 10:48:57,552 ----- Validation after Epoch: 80
2022-04-19 10:55:59,941 HashNet epoch:80, bit:16, dataset:coco, MAP:0.611, Best MAP(e50): 0.616
2022-04-19 10:56:05,755 Epoch[081/150], Step[0000/0157], Loss: 0.8708
2022-04-19 10:56:16,307 Epoch[081/150], Step[0050/0157], Loss: 0.8451
2022-04-19 10:56:27,039 Epoch[081/150], Step[0100/0157], Loss: 0.8501
2022-04-19 10:56:38,689 Epoch[081/150], Step[0150/0157], Loss: 0.8676
2022-04-19 10:56:39,992 HashNet[81/150] bit:16, lr:0.001000000, scale:2.236, train loss:0.843
2022-04-19 10:56:46,628 Epoch[082/150], Step[0000/0157], Loss: 0.8591
2022-04-19 10:56:58,748 Epoch[082/150], Step[0050/0157], Loss: 0.8761
2022-04-19 10:57:13,135 Epoch[082/150], Step[0100/0157], Loss: 0.8492
2022-04-19 10:57:26,368 Epoch[082/150], Step[0150/0157], Loss: 0.8690
2022-04-19 10:57:27,858 HashNet[82/150] bit:16, lr:0.001000000, scale:2.236, train loss:0.844
2022-04-19 10:57:35,350 Epoch[083/150], Step[0000/0157], Loss: 0.8023
2022-04-19 10:57:49,343 Epoch[083/150], Step[0050/0157], Loss: 0.8390
2022-04-19 10:58:04,182 Epoch[083/150], Step[0100/0157], Loss: 0.8424
2022-04-19 10:58:18,908 Epoch[083/150], Step[0150/0157], Loss: 0.8902
2022-04-19 10:58:20,092 HashNet[83/150] bit:16, lr:0.001000000, scale:2.236, train loss:0.843
2022-04-19 10:58:27,617 Epoch[084/150], Step[0000/0157], Loss: 0.8212
2022-04-19 10:58:37,767 Epoch[084/150], Step[0050/0157], Loss: 0.8495
2022-04-19 10:58:49,024 Epoch[084/150], Step[0100/0157], Loss: 0.8546
2022-04-19 10:59:00,085 Epoch[084/150], Step[0150/0157], Loss: 0.8387
2022-04-19 10:59:01,386 HashNet[84/150] bit:16, lr:0.001000000, scale:2.236, train loss:0.842
2022-04-19 10:59:06,981 Epoch[085/150], Step[0000/0157], Loss: 0.8206
2022-04-19 10:59:17,275 Epoch[085/150], Step[0050/0157], Loss: 0.8227
2022-04-19 10:59:27,361 Epoch[085/150], Step[0100/0157], Loss: 0.8312
2022-04-19 10:59:37,612 Epoch[085/150], Step[0150/0157], Loss: 0.8492
2022-04-19 10:59:38,898 HashNet[85/150] bit:16, lr:0.001000000, scale:2.236, train loss:0.842
2022-04-19 10:59:45,015 Epoch[086/150], Step[0000/0157], Loss: 0.8370
2022-04-19 10:59:54,735 Epoch[086/150], Step[0050/0157], Loss: 0.8371
2022-04-19 11:00:04,672 Epoch[086/150], Step[0100/0157], Loss: 0.8547
2022-04-19 11:00:15,462 Epoch[086/150], Step[0150/0157], Loss: 0.8400
2022-04-19 11:00:16,575 HashNet[86/150] bit:16, lr:0.001000000, scale:2.236, train loss:0.842
2022-04-19 11:00:22,320 Epoch[087/150], Step[0000/0157], Loss: 0.8276
2022-04-19 11:00:32,428 Epoch[087/150], Step[0050/0157], Loss: 0.8461
2022-04-19 11:00:42,562 Epoch[087/150], Step[0100/0157], Loss: 0.8146
2022-04-19 11:00:52,487 Epoch[087/150], Step[0150/0157], Loss: 0.8289
2022-04-19 11:00:53,808 HashNet[87/150] bit:16, lr:0.001000000, scale:2.236, train loss:0.842
2022-04-19 11:00:59,524 Epoch[088/150], Step[0000/0157], Loss: 0.8355
2022-04-19 11:01:09,553 Epoch[088/150], Step[0050/0157], Loss: 0.8580
2022-04-19 11:01:19,937 Epoch[088/150], Step[0100/0157], Loss: 0.8467
2022-04-19 11:01:29,831 Epoch[088/150], Step[0150/0157], Loss: 0.8273
2022-04-19 11:01:30,911 HashNet[88/150] bit:16, lr:0.001000000, scale:2.236, train loss:0.841
2022-04-19 11:01:36,860 Epoch[089/150], Step[0000/0157], Loss: 0.9050
2022-04-19 11:01:46,320 Epoch[089/150], Step[0050/0157], Loss: 0.8461
2022-04-19 11:01:56,868 Epoch[089/150], Step[0100/0157], Loss: 0.8131
2022-04-19 11:02:06,603 Epoch[089/150], Step[0150/0157], Loss: 0.8235
2022-04-19 11:02:08,293 HashNet[89/150] bit:16, lr:0.001000000, scale:2.236, train loss:0.843
2022-04-19 11:02:14,061 Epoch[090/150], Step[0000/0157], Loss: 0.8884
2022-04-19 11:02:24,678 Epoch[090/150], Step[0050/0157], Loss: 0.8781
2022-04-19 11:02:34,186 Epoch[090/150], Step[0100/0157], Loss: 0.8450
2022-04-19 11:02:44,171 Epoch[090/150], Step[0150/0157], Loss: 0.8992
2022-04-19 11:02:45,606 HashNet[90/150] bit:16, lr:0.001000000, scale:2.236, train loss:0.845
2022-04-19 11:02:45,607 ----- Validation after Epoch: 90
2022-04-19 11:09:46,099 save in checkpoints_new//train-16-20220419-09-06-59/bit_16
2022-04-19 11:09:48,534 Max mAP so far: 0.6183 at epoch_90
2022-04-19 11:09:48,534 ----- Save BEST model: checkpoints_new//train-16-20220419-09-06-59/bit_16/coco.pdparams
2022-04-19 11:09:48,534 ----- Save BEST optim: checkpoints_new//train-16-20220419-09-06-59/bit_16/coco.pdopt
2022-04-19 11:09:48,534 HashNet epoch:90, bit:16, dataset:coco, MAP:0.618, Best MAP(e90): 0.618
2022-04-19 11:09:57,534 Epoch[091/150], Step[0000/0157], Loss: 0.8589
2022-04-19 11:10:10,488 Epoch[091/150], Step[0050/0157], Loss: 0.8268
2022-04-19 11:10:24,577 Epoch[091/150], Step[0100/0157], Loss: 0.8553
2022-04-19 11:10:39,360 Epoch[091/150], Step[0150/0157], Loss: 0.8692
2022-04-19 11:10:40,709 HashNet[91/150] bit:16, lr:0.001000000, scale:2.236, train loss:0.843
2022-04-19 11:10:47,936 Epoch[092/150], Step[0000/0157], Loss: 0.8521
2022-04-19 11:11:00,941 Epoch[092/150], Step[0050/0157], Loss: 0.7977
2022-04-19 11:11:14,403 Epoch[092/150], Step[0100/0157], Loss: 0.8659
2022-04-19 11:11:30,172 Epoch[092/150], Step[0150/0157], Loss: 0.8670
2022-04-19 11:11:31,693 HashNet[92/150] bit:16, lr:0.001000000, scale:2.236, train loss:0.843
2022-04-19 11:11:39,475 Epoch[093/150], Step[0000/0157], Loss: 0.8399
2022-04-19 11:11:50,948 Epoch[093/150], Step[0050/0157], Loss: 0.8490
2022-04-19 11:12:01,309 Epoch[093/150], Step[0100/0157], Loss: 0.8337
2022-04-19 11:12:10,897 Epoch[093/150], Step[0150/0157], Loss: 0.8340
2022-04-19 11:12:12,044 HashNet[93/150] bit:16, lr:0.001000000, scale:2.236, train loss:0.840
2022-04-19 11:12:17,695 Epoch[094/150], Step[0000/0157], Loss: 0.8293
2022-04-19 11:12:27,889 Epoch[094/150], Step[0050/0157], Loss: 0.8037
2022-04-19 11:12:38,259 Epoch[094/150], Step[0100/0157], Loss: 0.8346
2022-04-19 11:12:47,858 Epoch[094/150], Step[0150/0157], Loss: 0.8447
2022-04-19 11:12:49,072 HashNet[94/150] bit:16, lr:0.001000000, scale:2.236, train loss:0.839
2022-04-19 11:12:54,905 Epoch[095/150], Step[0000/0157], Loss: 0.8426
2022-04-19 11:13:04,577 Epoch[095/150], Step[0050/0157], Loss: 0.8164
2022-04-19 11:13:15,828 Epoch[095/150], Step[0100/0157], Loss: 0.8562
2022-04-19 11:13:25,723 Epoch[095/150], Step[0150/0157], Loss: 0.8273
2022-04-19 11:13:26,834 HashNet[95/150] bit:16, lr:0.001000000, scale:2.236, train loss:0.841
2022-04-19 11:13:32,761 Epoch[096/150], Step[0000/0157], Loss: 0.8377
2022-04-19 11:13:43,214 Epoch[096/150], Step[0050/0157], Loss: 0.8294
2022-04-19 11:13:53,147 Epoch[096/150], Step[0100/0157], Loss: 0.8508
2022-04-19 11:14:03,327 Epoch[096/150], Step[0150/0157], Loss: 0.8351
2022-04-19 11:14:04,318 HashNet[96/150] bit:16, lr:0.001000000, scale:2.236, train loss:0.843
2022-04-19 11:14:10,097 Epoch[097/150], Step[0000/0157], Loss: 0.8693
2022-04-19 11:14:20,333 Epoch[097/150], Step[0050/0157], Loss: 0.8363
2022-04-19 11:14:30,418 Epoch[097/150], Step[0100/0157], Loss: 0.8266
2022-04-19 11:14:39,904 Epoch[097/150], Step[0150/0157], Loss: 0.8396
2022-04-19 11:14:41,219 HashNet[97/150] bit:16, lr:0.001000000, scale:2.236, train loss:0.841
2022-04-19 11:14:47,007 Epoch[098/150], Step[0000/0157], Loss: 0.8357
2022-04-19 11:14:56,713 Epoch[098/150], Step[0050/0157], Loss: 0.8381
2022-04-19 11:15:06,996 Epoch[098/150], Step[0100/0157], Loss: 0.7803
2022-04-19 11:15:16,378 Epoch[098/150], Step[0150/0157], Loss: 0.8180
2022-04-19 11:15:17,393 HashNet[98/150] bit:16, lr:0.001000000, scale:2.236, train loss:0.840
2022-04-19 11:15:23,037 Epoch[099/150], Step[0000/0157], Loss: 0.8537
2022-04-19 11:15:33,359 Epoch[099/150], Step[0050/0157], Loss: 0.8500
2022-04-19 11:15:43,877 Epoch[099/150], Step[0100/0157], Loss: 0.8338
2022-04-19 11:15:53,978 Epoch[099/150], Step[0150/0157], Loss: 0.8563
2022-04-19 11:15:55,161 HashNet[99/150] bit:16, lr:0.001000000, scale:2.236, train loss:0.840
2022-04-19 11:16:01,023 Epoch[100/150], Step[0000/0157], Loss: 0.8077
2022-04-19 11:16:11,531 Epoch[100/150], Step[0050/0157], Loss: 0.8379
2022-04-19 11:16:21,493 Epoch[100/150], Step[0100/0157], Loss: 0.8143
2022-04-19 11:16:31,862 Epoch[100/150], Step[0150/0157], Loss: 0.7985
2022-04-19 11:16:32,969 HashNet[100/150] bit:16, lr:0.001000000, scale:2.236, train loss:0.839
2022-04-19 11:16:32,970 ----- Validation after Epoch: 100
2022-04-19 11:23:41,750 HashNet epoch:100, bit:16, dataset:coco, MAP:0.614, Best MAP(e90): 0.618
2022-04-19 11:23:49,584 Epoch[101/150], Step[0000/0157], Loss: 0.8251
2022-04-19 11:24:03,558 Epoch[101/150], Step[0050/0157], Loss: 0.8412
2022-04-19 11:24:16,799 Epoch[101/150], Step[0100/0157], Loss: 0.8279
2022-04-19 11:24:31,248 Epoch[101/150], Step[0150/0157], Loss: 0.8057
2022-04-19 11:24:32,527 HashNet[101/150] bit:16, lr:0.000500000, scale:2.449, train loss:0.837
2022-04-19 11:24:39,104 Epoch[102/150], Step[0000/0157], Loss: 0.8388
2022-04-19 11:24:50,733 Epoch[102/150], Step[0050/0157], Loss: 0.8325
2022-04-19 11:25:01,453 Epoch[102/150], Step[0100/0157], Loss: 0.8479
2022-04-19 11:25:12,036 Epoch[102/150], Step[0150/0157], Loss: 0.8288
2022-04-19 11:25:13,145 HashNet[102/150] bit:16, lr:0.000500000, scale:2.449, train loss:0.837
2022-04-19 11:25:19,000 Epoch[103/150], Step[0000/0157], Loss: 0.8498
2022-04-19 11:25:28,656 Epoch[103/150], Step[0050/0157], Loss: 0.8288
2022-04-19 11:25:38,646 Epoch[103/150], Step[0100/0157], Loss: 0.8045
2022-04-19 11:25:48,592 Epoch[103/150], Step[0150/0157], Loss: 0.8180
2022-04-19 11:25:49,864 HashNet[103/150] bit:16, lr:0.000500000, scale:2.449, train loss:0.835
2022-04-19 11:25:55,567 Epoch[104/150], Step[0000/0157], Loss: 0.8464
2022-04-19 11:26:05,894 Epoch[104/150], Step[0050/0157], Loss: 0.8310
2022-04-19 11:26:16,459 Epoch[104/150], Step[0100/0157], Loss: 0.8075
2022-04-19 11:26:26,655 Epoch[104/150], Step[0150/0157], Loss: 0.8420
2022-04-19 11:26:27,825 HashNet[104/150] bit:16, lr:0.000500000, scale:2.449, train loss:0.839
2022-04-19 11:26:33,776 Epoch[105/150], Step[0000/0157], Loss: 0.8386
2022-04-19 11:26:44,009 Epoch[105/150], Step[0050/0157], Loss: 0.8205
2022-04-19 11:26:54,204 Epoch[105/150], Step[0100/0157], Loss: 0.8428
2022-04-19 11:27:04,306 Epoch[105/150], Step[0150/0157], Loss: 0.8257
2022-04-19 11:27:05,474 HashNet[105/150] bit:16, lr:0.000500000, scale:2.449, train loss:0.836
2022-04-19 11:27:11,076 Epoch[106/150], Step[0000/0157], Loss: 0.8100
2022-04-19 11:27:20,878 Epoch[106/150], Step[0050/0157], Loss: 0.8218
2022-04-19 11:27:31,800 Epoch[106/150], Step[0100/0157], Loss: 0.7972
2022-04-19 11:27:41,773 Epoch[106/150], Step[0150/0157], Loss: 0.8520
2022-04-19 11:27:42,818 HashNet[106/150] bit:16, lr:0.000500000, scale:2.449, train loss:0.836
2022-04-19 11:27:48,530 Epoch[107/150], Step[0000/0157], Loss: 0.8693
2022-04-19 11:27:59,312 Epoch[107/150], Step[0050/0157], Loss: 0.8004
2022-04-19 11:28:09,508 Epoch[107/150], Step[0100/0157], Loss: 0.8631
2022-04-19 11:28:19,223 Epoch[107/150], Step[0150/0157], Loss: 0.8550
2022-04-19 11:28:20,432 HashNet[107/150] bit:16, lr:0.000500000, scale:2.449, train loss:0.836
2022-04-19 11:28:26,339 Epoch[108/150], Step[0000/0157], Loss: 0.8259
2022-04-19 11:28:36,505 Epoch[108/150], Step[0050/0157], Loss: 0.8513
2022-04-19 11:28:46,731 Epoch[108/150], Step[0100/0157], Loss: 0.8411
2022-04-19 11:28:56,279 Epoch[108/150], Step[0150/0157], Loss: 0.8511
2022-04-19 11:28:57,628 HashNet[108/150] bit:16, lr:0.000500000, scale:2.449, train loss:0.835
2022-04-19 11:29:03,309 Epoch[109/150], Step[0000/0157], Loss: 0.8383
2022-04-19 11:29:13,242 Epoch[109/150], Step[0050/0157], Loss: 0.8752
2022-04-19 11:29:23,733 Epoch[109/150], Step[0100/0157], Loss: 0.8505
2022-04-19 11:29:34,177 Epoch[109/150], Step[0150/0157], Loss: 0.8424
2022-04-19 11:29:35,219 HashNet[109/150] bit:16, lr:0.000500000, scale:2.449, train loss:0.835
2022-04-19 11:29:40,889 Epoch[110/150], Step[0000/0157], Loss: 0.8409
2022-04-19 11:29:51,038 Epoch[110/150], Step[0050/0157], Loss: 0.8214
2022-04-19 11:30:00,871 Epoch[110/150], Step[0100/0157], Loss: 0.8313
2022-04-19 11:30:10,935 Epoch[110/150], Step[0150/0157], Loss: 0.8228
2022-04-19 11:30:12,346 HashNet[110/150] bit:16, lr:0.000500000, scale:2.449, train loss:0.836
2022-04-19 11:30:12,347 ----- Validation after Epoch: 110
2022-04-19 11:37:24,566 HashNet epoch:110, bit:16, dataset:coco, MAP:0.617, Best MAP(e90): 0.618
2022-04-19 11:37:35,167 Epoch[111/150], Step[0000/0157], Loss: 0.8264
2022-04-19 11:37:46,815 Epoch[111/150], Step[0050/0157], Loss: 0.7996
2022-04-19 11:37:58,098 Epoch[111/150], Step[0100/0157], Loss: 0.8471
2022-04-19 11:38:08,795 Epoch[111/150], Step[0150/0157], Loss: 0.8300
2022-04-19 11:38:10,005 HashNet[111/150] bit:16, lr:0.000500000, scale:2.449, train loss:0.835
2022-04-19 11:38:16,634 Epoch[112/150], Step[0000/0157], Loss: 0.8546
2022-04-19 11:38:26,528 Epoch[112/150], Step[0050/0157], Loss: 0.8213
2022-04-19 11:38:36,811 Epoch[112/150], Step[0100/0157], Loss: 0.7961
2022-04-19 11:38:47,190 Epoch[112/150], Step[0150/0157], Loss: 0.8577
2022-04-19 11:38:48,203 HashNet[112/150] bit:16, lr:0.000500000, scale:2.449, train loss:0.834
2022-04-19 11:38:53,911 Epoch[113/150], Step[0000/0157], Loss: 0.8025
2022-04-19 11:39:04,558 Epoch[113/150], Step[0050/0157], Loss: 0.8294
2022-04-19 11:39:14,292 Epoch[113/150], Step[0100/0157], Loss: 0.8301
2022-04-19 11:39:24,125 Epoch[113/150], Step[0150/0157], Loss: 0.8491
2022-04-19 11:39:25,595 HashNet[113/150] bit:16, lr:0.000500000, scale:2.449, train loss:0.834
2022-04-19 11:39:31,403 Epoch[114/150], Step[0000/0157], Loss: 0.8347
2022-04-19 11:39:42,044 Epoch[114/150], Step[0050/0157], Loss: 0.8330
2022-04-19 11:39:51,694 Epoch[114/150], Step[0100/0157], Loss: 0.8500
2022-04-19 11:40:01,889 Epoch[114/150], Step[0150/0157], Loss: 0.8145
2022-04-19 11:40:03,027 HashNet[114/150] bit:16, lr:0.000500000, scale:2.449, train loss:0.835
2022-04-19 11:40:09,062 Epoch[115/150], Step[0000/0157], Loss: 0.8391
2022-04-19 11:40:19,215 Epoch[115/150], Step[0050/0157], Loss: 0.7949
2022-04-19 11:40:29,025 Epoch[115/150], Step[0100/0157], Loss: 0.8488
2022-04-19 11:40:38,581 Epoch[115/150], Step[0150/0157], Loss: 0.8444
2022-04-19 11:40:39,857 HashNet[115/150] bit:16, lr:0.000500000, scale:2.449, train loss:0.834
2022-04-19 11:40:45,672 Epoch[116/150], Step[0000/0157], Loss: 0.8385
2022-04-19 11:40:55,758 Epoch[116/150], Step[0050/0157], Loss: 0.8344
2022-04-19 11:41:06,349 Epoch[116/150], Step[0100/0157], Loss: 0.8385
2022-04-19 11:41:15,718 Epoch[116/150], Step[0150/0157], Loss: 0.8229
2022-04-19 11:41:17,000 HashNet[116/150] bit:16, lr:0.000500000, scale:2.449, train loss:0.834
2022-04-19 11:41:22,954 Epoch[117/150], Step[0000/0157], Loss: 0.8214
2022-04-19 11:41:32,111 Epoch[117/150], Step[0050/0157], Loss: 0.8276
2022-04-19 11:41:41,715 Epoch[117/150], Step[0100/0157], Loss: 0.8206
2022-04-19 11:41:51,844 Epoch[117/150], Step[0150/0157], Loss: 0.8291
2022-04-19 11:41:53,087 HashNet[117/150] bit:16, lr:0.000500000, scale:2.449, train loss:0.834
2022-04-19 11:41:58,631 Epoch[118/150], Step[0000/0157], Loss: 0.8202
2022-04-19 11:42:08,621 Epoch[118/150], Step[0050/0157], Loss: 0.7909
2022-04-19 11:42:18,791 Epoch[118/150], Step[0100/0157], Loss: 0.8274
2022-04-19 11:42:29,321 Epoch[118/150], Step[0150/0157], Loss: 0.8585
2022-04-19 11:42:30,369 HashNet[118/150] bit:16, lr:0.000500000, scale:2.449, train loss:0.834
2022-04-19 11:42:36,210 Epoch[119/150], Step[0000/0157], Loss: 0.8205
2022-04-19 11:42:45,941 Epoch[119/150], Step[0050/0157], Loss: 0.8267
2022-04-19 11:42:56,203 Epoch[119/150], Step[0100/0157], Loss: 0.8234
2022-04-19 11:43:05,543 Epoch[119/150], Step[0150/0157], Loss: 0.8522
2022-04-19 11:43:06,800 HashNet[119/150] bit:16, lr:0.000500000, scale:2.449, train loss:0.833
2022-04-19 11:43:12,574 Epoch[120/150], Step[0000/0157], Loss: 0.8296
2022-04-19 11:43:23,029 Epoch[120/150], Step[0050/0157], Loss: 0.8284
2022-04-19 11:43:32,445 Epoch[120/150], Step[0100/0157], Loss: 0.8149
2022-04-19 11:43:42,684 Epoch[120/150], Step[0150/0157], Loss: 0.8501
2022-04-19 11:43:43,690 HashNet[120/150] bit:16, lr:0.000500000, scale:2.449, train loss:0.833
2022-04-19 11:43:43,690 ----- Validation after Epoch: 120
2022-04-19 11:51:02,825 HashNet epoch:120, bit:16, dataset:coco, MAP:0.616, Best MAP(e90): 0.618
2022-04-19 11:51:09,476 Epoch[121/150], Step[0000/0157], Loss: 0.8619
2022-04-19 11:51:20,956 Epoch[121/150], Step[0050/0157], Loss: 0.8519
2022-04-19 11:51:31,994 Epoch[121/150], Step[0100/0157], Loss: 0.8289
2022-04-19 11:51:43,220 Epoch[121/150], Step[0150/0157], Loss: 0.7922
2022-04-19 11:51:44,427 HashNet[121/150] bit:16, lr:0.000500000, scale:2.646, train loss:0.834
2022-04-19 11:51:51,331 Epoch[122/150], Step[0000/0157], Loss: 0.8234
2022-04-19 11:52:01,561 Epoch[122/150], Step[0050/0157], Loss: 0.8213
2022-04-19 11:52:11,531 Epoch[122/150], Step[0100/0157], Loss: 0.8318
2022-04-19 11:52:21,567 Epoch[122/150], Step[0150/0157], Loss: 0.8284
2022-04-19 11:52:22,794 HashNet[122/150] bit:16, lr:0.000500000, scale:2.646, train loss:0.834
2022-04-19 11:52:28,746 Epoch[123/150], Step[0000/0157], Loss: 0.8664
2022-04-19 11:52:38,794 Epoch[123/150], Step[0050/0157], Loss: 0.8137
2022-04-19 11:52:48,327 Epoch[123/150], Step[0100/0157], Loss: 0.8390
2022-04-19 11:52:58,305 Epoch[123/150], Step[0150/0157], Loss: 0.8553
2022-04-19 11:52:59,582 HashNet[123/150] bit:16, lr:0.000500000, scale:2.646, train loss:0.835
2022-04-19 11:53:05,662 Epoch[124/150], Step[0000/0157], Loss: 0.8580
2022-04-19 11:53:15,584 Epoch[124/150], Step[0050/0157], Loss: 0.8539
2022-04-19 11:53:25,293 Epoch[124/150], Step[0100/0157], Loss: 0.8281
2022-04-19 11:53:35,331 Epoch[124/150], Step[0150/0157], Loss: 0.8286
2022-04-19 11:53:36,781 HashNet[124/150] bit:16, lr:0.000500000, scale:2.646, train loss:0.834
2022-04-19 11:53:42,556 Epoch[125/150], Step[0000/0157], Loss: 0.8519
2022-04-19 11:53:52,848 Epoch[125/150], Step[0050/0157], Loss: 0.8519
2022-04-19 11:54:03,550 Epoch[125/150], Step[0100/0157], Loss: 0.8353
2022-04-19 11:54:13,576 Epoch[125/150], Step[0150/0157], Loss: 0.8244
2022-04-19 11:54:14,864 HashNet[125/150] bit:16, lr:0.000500000, scale:2.646, train loss:0.834
2022-04-19 11:54:20,769 Epoch[126/150], Step[0000/0157], Loss: 0.8218
2022-04-19 11:54:30,838 Epoch[126/150], Step[0050/0157], Loss: 0.8417
2022-04-19 11:54:41,291 Epoch[126/150], Step[0100/0157], Loss: 0.8204
2022-04-19 11:54:51,194 Epoch[126/150], Step[0150/0157], Loss: 0.8248
2022-04-19 11:54:52,433 HashNet[126/150] bit:16, lr:0.000500000, scale:2.646, train loss:0.833
2022-04-19 11:54:58,260 Epoch[127/150], Step[0000/0157], Loss: 0.8079
2022-04-19 11:55:08,401 Epoch[127/150], Step[0050/0157], Loss: 0.8333
2022-04-19 11:55:17,550 Epoch[127/150], Step[0100/0157], Loss: 0.8843
2022-04-19 11:55:27,403 Epoch[127/150], Step[0150/0157], Loss: 0.8250
2022-04-19 11:55:28,554 HashNet[127/150] bit:16, lr:0.000500000, scale:2.646, train loss:0.832
2022-04-19 11:55:34,416 Epoch[128/150], Step[0000/0157], Loss: 0.8604
2022-04-19 11:55:44,529 Epoch[128/150], Step[0050/0157], Loss: 0.8335
2022-04-19 11:55:55,429 Epoch[128/150], Step[0100/0157], Loss: 0.8167
2022-04-19 11:56:05,120 Epoch[128/150], Step[0150/0157], Loss: 0.8140
2022-04-19 11:56:06,364 HashNet[128/150] bit:16, lr:0.000500000, scale:2.646, train loss:0.832
2022-04-19 11:56:12,191 Epoch[129/150], Step[0000/0157], Loss: 0.8111
2022-04-19 11:56:22,875 Epoch[129/150], Step[0050/0157], Loss: 0.8408
2022-04-19 11:56:32,451 Epoch[129/150], Step[0100/0157], Loss: 0.8316
2022-04-19 11:56:42,538 Epoch[129/150], Step[0150/0157], Loss: 0.8261
2022-04-19 11:56:43,616 HashNet[129/150] bit:16, lr:0.000500000, scale:2.646, train loss:0.833
2022-04-19 11:56:49,407 Epoch[130/150], Step[0000/0157], Loss: 0.8083
2022-04-19 11:57:00,034 Epoch[130/150], Step[0050/0157], Loss: 0.7967
2022-04-19 11:57:09,728 Epoch[130/150], Step[0100/0157], Loss: 0.8247
2022-04-19 11:57:19,801 Epoch[130/150], Step[0150/0157], Loss: 0.8451
2022-04-19 11:57:20,808 HashNet[130/150] bit:16, lr:0.000500000, scale:2.646, train loss:0.834
2022-04-19 11:57:20,809 ----- Validation after Epoch: 130
2022-04-19 12:04:38,402 HashNet epoch:130, bit:16, dataset:coco, MAP:0.613, Best MAP(e90): 0.618
2022-04-19 12:04:49,325 Epoch[131/150], Step[0000/0157], Loss: 0.7922
2022-04-19 12:04:59,824 Epoch[131/150], Step[0050/0157], Loss: 0.8524
2022-04-19 12:05:10,924 Epoch[131/150], Step[0100/0157], Loss: 0.8249
2022-04-19 12:05:22,307 Epoch[131/150], Step[0150/0157], Loss: 0.8599
2022-04-19 12:05:23,259 HashNet[131/150] bit:16, lr:0.000500000, scale:2.646, train loss:0.833
2022-04-19 12:05:29,025 Epoch[132/150], Step[0000/0157], Loss: 0.8179
2022-04-19 12:05:39,284 Epoch[132/150], Step[0050/0157], Loss: 0.8176
2022-04-19 12:05:49,259 Epoch[132/150], Step[0100/0157], Loss: 0.8012
2022-04-19 12:05:59,255 Epoch[132/150], Step[0150/0157], Loss: 0.8801
2022-04-19 12:06:00,621 HashNet[132/150] bit:16, lr:0.000500000, scale:2.646, train loss:0.833
2022-04-19 12:06:06,375 Epoch[133/150], Step[0000/0157], Loss: 0.8248
2022-04-19 12:06:16,583 Epoch[133/150], Step[0050/0157], Loss: 0.8398
2022-04-19 12:06:27,353 Epoch[133/150], Step[0100/0157], Loss: 0.8074
2022-04-19 12:06:37,411 Epoch[133/150], Step[0150/0157], Loss: 0.8112
2022-04-19 12:06:38,576 HashNet[133/150] bit:16, lr:0.000500000, scale:2.646, train loss:0.832
2022-04-19 12:06:44,318 Epoch[134/150], Step[0000/0157], Loss: 0.8209
2022-04-19 12:06:54,543 Epoch[134/150], Step[0050/0157], Loss: 0.8178
2022-04-19 12:07:04,369 Epoch[134/150], Step[0100/0157], Loss: 0.8473
2022-04-19 12:07:14,542 Epoch[134/150], Step[0150/0157], Loss: 0.8187
2022-04-19 12:07:15,836 HashNet[134/150] bit:16, lr:0.000500000, scale:2.646, train loss:0.831
2022-04-19 12:07:21,747 Epoch[135/150], Step[0000/0157], Loss: 0.8188
2022-04-19 12:07:31,542 Epoch[135/150], Step[0050/0157], Loss: 0.8842
2022-04-19 12:07:41,988 Epoch[135/150], Step[0100/0157], Loss: 0.8505
2022-04-19 12:07:51,605 Epoch[135/150], Step[0150/0157], Loss: 0.8259
2022-04-19 12:07:52,810 HashNet[135/150] bit:16, lr:0.000500000, scale:2.646, train loss:0.832
2022-04-19 12:07:58,802 Epoch[136/150], Step[0000/0157], Loss: 0.8494
2022-04-19 12:08:08,674 Epoch[136/150], Step[0050/0157], Loss: 0.8176
2022-04-19 12:08:18,583 Epoch[136/150], Step[0100/0157], Loss: 0.8174
2022-04-19 12:08:28,599 Epoch[136/150], Step[0150/0157], Loss: 0.8486
2022-04-19 12:08:29,839 HashNet[136/150] bit:16, lr:0.000500000, scale:2.646, train loss:0.832
2022-04-19 12:08:35,665 Epoch[137/150], Step[0000/0157], Loss: 0.8621
2022-04-19 12:08:45,480 Epoch[137/150], Step[0050/0157], Loss: 0.8666
2022-04-19 12:08:55,250 Epoch[137/150], Step[0100/0157], Loss: 0.8369
2022-04-19 12:09:05,505 Epoch[137/150], Step[0150/0157], Loss: 0.8420
2022-04-19 12:09:06,581 HashNet[137/150] bit:16, lr:0.000500000, scale:2.646, train loss:0.832
2022-04-19 12:09:12,556 Epoch[138/150], Step[0000/0157], Loss: 0.8436
2022-04-19 12:09:22,407 Epoch[138/150], Step[0050/0157], Loss: 0.8016
2022-04-19 12:09:32,638 Epoch[138/150], Step[0100/0157], Loss: 0.8197
2022-04-19 12:09:42,710 Epoch[138/150], Step[0150/0157], Loss: 0.8400
2022-04-19 12:09:43,889 HashNet[138/150] bit:16, lr:0.000500000, scale:2.646, train loss:0.831
2022-04-19 12:09:49,760 Epoch[139/150], Step[0000/0157], Loss: 0.8441
2022-04-19 12:10:00,414 Epoch[139/150], Step[0050/0157], Loss: 0.8178
2022-04-19 12:10:10,751 Epoch[139/150], Step[0100/0157], Loss: 0.8215
2022-04-19 12:10:20,161 Epoch[139/150], Step[0150/0157], Loss: 0.8061
2022-04-19 12:10:21,253 HashNet[139/150] bit:16, lr:0.000500000, scale:2.646, train loss:0.831
2022-04-19 12:10:27,283 Epoch[140/150], Step[0000/0157], Loss: 0.8213
2022-04-19 12:10:37,434 Epoch[140/150], Step[0050/0157], Loss: 0.8358
2022-04-19 12:10:47,262 Epoch[140/150], Step[0100/0157], Loss: 0.8297
2022-04-19 12:10:57,080 Epoch[140/150], Step[0150/0157], Loss: 0.8401
2022-04-19 12:10:58,452 HashNet[140/150] bit:16, lr:0.000500000, scale:2.646, train loss:0.831
2022-04-19 12:10:58,453 ----- Validation after Epoch: 140
2022-04-19 12:18:20,868 save in checkpoints_new//train-16-20220419-09-06-59/bit_16
2022-04-19 12:18:23,087 Max mAP so far: 0.6188 at epoch_140
2022-04-19 12:18:23,087 ----- Save BEST model: checkpoints_new//train-16-20220419-09-06-59/bit_16/coco.pdparams
2022-04-19 12:18:23,087 ----- Save BEST optim: checkpoints_new//train-16-20220419-09-06-59/bit_16/coco.pdopt
2022-04-19 12:18:23,087 HashNet epoch:140, bit:16, dataset:coco, MAP:0.619, Best MAP(e140): 0.619
2022-04-19 12:18:32,804 Epoch[141/150], Step[0000/0157], Loss: 0.8368
2022-04-19 12:18:43,803 Epoch[141/150], Step[0050/0157], Loss: 0.8489
2022-04-19 12:18:53,979 Epoch[141/150], Step[0100/0157], Loss: 0.8134
2022-04-19 12:19:04,843 Epoch[141/150], Step[0150/0157], Loss: 0.8324
2022-04-19 12:19:06,073 HashNet[141/150] bit:16, lr:0.000500000, scale:2.828, train loss:0.831
2022-04-19 12:19:11,792 Epoch[142/150], Step[0000/0157], Loss: 0.8086
2022-04-19 12:19:22,091 Epoch[142/150], Step[0050/0157], Loss: 0.8477
2022-04-19 12:19:32,751 Epoch[142/150], Step[0100/0157], Loss: 0.8277
2022-04-19 12:19:42,362 Epoch[142/150], Step[0150/0157], Loss: 0.8472
2022-04-19 12:19:43,459 HashNet[142/150] bit:16, lr:0.000500000, scale:2.828, train loss:0.831
2022-04-19 12:19:49,184 Epoch[143/150], Step[0000/0157], Loss: 0.8317
2022-04-19 12:19:59,063 Epoch[143/150], Step[0050/0157], Loss: 0.8143
2022-04-19 12:20:08,973 Epoch[143/150], Step[0100/0157], Loss: 0.8272
2022-04-19 12:20:18,976 Epoch[143/150], Step[0150/0157], Loss: 0.8169
2022-04-19 12:20:20,157 HashNet[143/150] bit:16, lr:0.000500000, scale:2.828, train loss:0.831
2022-04-19 12:20:25,830 Epoch[144/150], Step[0000/0157], Loss: 0.8101
2022-04-19 12:20:36,615 Epoch[144/150], Step[0050/0157], Loss: 0.8223
2022-04-19 12:20:46,456 Epoch[144/150], Step[0100/0157], Loss: 0.7987
2022-04-19 12:20:56,442 Epoch[144/150], Step[0150/0157], Loss: 0.8311
2022-04-19 12:20:57,423 HashNet[144/150] bit:16, lr:0.000500000, scale:2.828, train loss:0.830
2022-04-19 12:21:03,364 Epoch[145/150], Step[0000/0157], Loss: 0.7977
2022-04-19 12:21:13,709 Epoch[145/150], Step[0050/0157], Loss: 0.8195
2022-04-19 12:21:23,483 Epoch[145/150], Step[0100/0157], Loss: 0.8395
2022-04-19 12:21:33,543 Epoch[145/150], Step[0150/0157], Loss: 0.8323
2022-04-19 12:21:34,607 HashNet[145/150] bit:16, lr:0.000500000, scale:2.828, train loss:0.829
2022-04-19 12:21:40,626 Epoch[146/150], Step[0000/0157], Loss: 0.8229
2022-04-19 12:21:50,138 Epoch[146/150], Step[0050/0157], Loss: 0.8039
2022-04-19 12:22:00,405 Epoch[146/150], Step[0100/0157], Loss: 0.8418
2022-04-19 12:22:10,153 Epoch[146/150], Step[0150/0157], Loss: 0.8307
2022-04-19 12:22:11,478 HashNet[146/150] bit:16, lr:0.000500000, scale:2.828, train loss:0.831
2022-04-19 12:22:17,352 Epoch[147/150], Step[0000/0157], Loss: 0.8123
2022-04-19 12:22:27,593 Epoch[147/150], Step[0050/0157], Loss: 0.8749
2022-04-19 12:22:37,935 Epoch[147/150], Step[0100/0157], Loss: 0.8472
2022-04-19 12:22:48,084 Epoch[147/150], Step[0150/0157], Loss: 0.7977
2022-04-19 12:22:49,135 HashNet[147/150] bit:16, lr:0.000500000, scale:2.828, train loss:0.831
2022-04-19 12:22:55,001 Epoch[148/150], Step[0000/0157], Loss: 0.8127
2022-04-19 12:23:05,251 Epoch[148/150], Step[0050/0157], Loss: 0.8166
2022-04-19 12:23:15,365 Epoch[148/150], Step[0100/0157], Loss: 0.8380
2022-04-19 12:23:25,771 Epoch[148/150], Step[0150/0157], Loss: 0.8218
2022-04-19 12:23:27,056 HashNet[148/150] bit:16, lr:0.000500000, scale:2.828, train loss:0.831
2022-04-19 12:23:32,563 Epoch[149/150], Step[0000/0157], Loss: 0.8389
2022-04-19 12:23:43,061 Epoch[149/150], Step[0050/0157], Loss: 0.7981
2022-04-19 12:23:53,124 Epoch[149/150], Step[0100/0157], Loss: 0.8449
2022-04-19 12:24:03,354 Epoch[149/150], Step[0150/0157], Loss: 0.8232
2022-04-19 12:24:04,471 HashNet[149/150] bit:16, lr:0.000500000, scale:2.828, train loss:0.831
2022-04-19 12:24:10,148 Epoch[150/150], Step[0000/0157], Loss: 0.8317
2022-04-19 12:24:20,405 Epoch[150/150], Step[0050/0157], Loss: 0.8174
2022-04-19 12:24:30,640 Epoch[150/150], Step[0100/0157], Loss: 0.8291
2022-04-19 12:24:40,709 Epoch[150/150], Step[0150/0157], Loss: 0.8270
2022-04-19 12:24:41,699 HashNet[150/150] bit:16, lr:0.000500000, scale:2.828, train loss:0.830
2022-04-19 12:24:41,699 ----- Validation after Epoch: 150
2022-04-19 12:32:08,280 save in checkpoints_new//train-16-20220419-09-06-59/bit_16
2022-04-19 12:32:10,527 Max mAP so far: 0.6188 at epoch_150
2022-04-19 12:32:10,527 ----- Save BEST model: checkpoints_new//train-16-20220419-09-06-59/bit_16/coco.pdparams
2022-04-19 12:32:10,527 ----- Save BEST optim: checkpoints_new//train-16-20220419-09-06-59/bit_16/coco.pdopt
2022-04-19 12:32:10,527 HashNet epoch:150, bit:16, dataset:coco, MAP:0.619, Best MAP(e150): 0.619
2022-04-19 12:32:10,527 Training completed for HashNet(16).
2022-04-19 12:32:10,527 Best MAP(e150): 0.619
