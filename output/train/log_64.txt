2022-04-17 18:45:05,688 
Namespace(alpha=0.1, batch_size=64, bit=64, ckp=None, crop_size=224, data={'train_set': {'list_path': 'data/coco/train.txt', 'batch_size': 64}, 'database': {'list_path': 'data/coco/database.txt', 'batch_size': 64}, 'test': {'list_path': 'data/coco/test.txt', 'batch_size': 64}}, data_path='/home/super/public/wht/datasets/COCO2014/', dataset='coco', de_step=50, debug_steps=50, epoch=150, eval=False, eval_epoch=10, last_epoch=0, learning_rate=0.001, log_path='logs_new/', model='HashNet', momentum=0.9, num_class=80, num_train=10000, optimizer='SGD', resize_size=256, resume=None, save_path='checkpoints_new/', seed=2000, step_continuation=20, topK=5000, weight_decay=0.0005)
2022-04-17 18:45:05,689 ----- world_size = 2, local_rank = 0
2022-04-17 18:45:07,176 ----- Total # of train batch (single gpu): 157
2022-04-17 18:45:07,176 ----- Total # of test batch (single gpu): 79
2022-04-17 18:45:07,176 ----- Total # of base batch (single gpu): 1754
2022-04-17 18:45:07,177 Start training from epoch 1.
2022-04-17 18:45:12,451 Epoch[001/150], Step[0000/0157], Loss: 1.2567
2022-04-17 18:45:18,799 Epoch[001/150], Step[0050/0157], Loss: 1.2265
2022-04-17 18:45:25,683 Epoch[001/150], Step[0100/0157], Loss: 1.2029
2022-04-17 18:45:32,815 Epoch[001/150], Step[0150/0157], Loss: 1.1149
2022-04-17 18:45:33,704 HashNet[ 1/150] bit:64, lr:0.002000000, scale:1.000, train loss:1.151
2022-04-17 18:45:38,219 Epoch[002/150], Step[0000/0157], Loss: 1.0802
2022-04-17 18:45:45,119 Epoch[002/150], Step[0050/0157], Loss: 1.1069
2022-04-17 18:45:52,339 Epoch[002/150], Step[0100/0157], Loss: 1.0915
2022-04-17 18:45:59,294 Epoch[002/150], Step[0150/0157], Loss: 0.9997
2022-04-17 18:46:00,112 HashNet[ 2/150] bit:64, lr:0.002000000, scale:1.000, train loss:1.083
2022-04-17 18:46:04,652 Epoch[003/150], Step[0000/0157], Loss: 0.9946
2022-04-17 18:46:11,435 Epoch[003/150], Step[0050/0157], Loss: 1.0509
2022-04-17 18:46:18,557 Epoch[003/150], Step[0100/0157], Loss: 1.0538
2022-04-17 18:46:25,505 Epoch[003/150], Step[0150/0157], Loss: 0.9798
2022-04-17 18:46:26,324 HashNet[ 3/150] bit:64, lr:0.002000000, scale:1.000, train loss:1.004
2022-04-17 18:46:30,959 Epoch[004/150], Step[0000/0157], Loss: 0.9591
2022-04-17 18:46:37,740 Epoch[004/150], Step[0050/0157], Loss: 0.9464
2022-04-17 18:46:44,804 Epoch[004/150], Step[0100/0157], Loss: 0.9710
2022-04-17 18:46:51,849 Epoch[004/150], Step[0150/0157], Loss: 0.9502
2022-04-17 18:46:52,668 HashNet[ 4/150] bit:64, lr:0.002000000, scale:1.000, train loss:0.969
2022-04-17 18:46:57,517 Epoch[005/150], Step[0000/0157], Loss: 0.9038
2022-04-17 18:47:04,282 Epoch[005/150], Step[0050/0157], Loss: 0.9313
2022-04-17 18:47:11,324 Epoch[005/150], Step[0100/0157], Loss: 0.9421
2022-04-17 18:47:18,202 Epoch[005/150], Step[0150/0157], Loss: 0.9396
2022-04-17 18:47:19,030 HashNet[ 5/150] bit:64, lr:0.002000000, scale:1.000, train loss:0.942
2022-04-17 18:47:23,589 Epoch[006/150], Step[0000/0157], Loss: 0.9168
2022-04-17 18:47:30,415 Epoch[006/150], Step[0050/0157], Loss: 0.9306
2022-04-17 18:47:37,560 Epoch[006/150], Step[0100/0157], Loss: 0.9373
2022-04-17 18:47:44,357 Epoch[006/150], Step[0150/0157], Loss: 0.9159
2022-04-17 18:47:45,192 HashNet[ 6/150] bit:64, lr:0.002000000, scale:1.000, train loss:0.924
2022-04-17 18:47:49,706 Epoch[007/150], Step[0000/0157], Loss: 0.9268
2022-04-17 18:47:56,601 Epoch[007/150], Step[0050/0157], Loss: 0.9323
2022-04-17 18:48:03,680 Epoch[007/150], Step[0100/0157], Loss: 0.8961
2022-04-17 18:48:10,562 Epoch[007/150], Step[0150/0157], Loss: 0.8527
2022-04-17 18:48:11,399 HashNet[ 7/150] bit:64, lr:0.002000000, scale:1.000, train loss:0.899
2022-04-17 18:48:15,936 Epoch[008/150], Step[0000/0157], Loss: 0.8594
2022-04-17 18:48:23,244 Epoch[008/150], Step[0050/0157], Loss: 0.8612
2022-04-17 18:48:30,410 Epoch[008/150], Step[0100/0157], Loss: 0.8719
2022-04-17 18:48:37,455 Epoch[008/150], Step[0150/0157], Loss: 0.8550
2022-04-17 18:48:38,287 HashNet[ 8/150] bit:64, lr:0.002000000, scale:1.000, train loss:0.886
2022-04-17 18:48:42,802 Epoch[009/150], Step[0000/0157], Loss: 0.8748
2022-04-17 18:48:49,710 Epoch[009/150], Step[0050/0157], Loss: 0.8777
2022-04-17 18:48:56,986 Epoch[009/150], Step[0100/0157], Loss: 0.8832
2022-04-17 18:49:04,121 Epoch[009/150], Step[0150/0157], Loss: 0.8688
2022-04-17 18:49:04,896 HashNet[ 9/150] bit:64, lr:0.002000000, scale:1.000, train loss:0.871
2022-04-17 18:49:09,583 Epoch[010/150], Step[0000/0157], Loss: 0.9095
2022-04-17 18:49:16,368 Epoch[010/150], Step[0050/0157], Loss: 0.8148
2022-04-17 18:49:23,334 Epoch[010/150], Step[0100/0157], Loss: 0.8763
2022-04-17 18:49:30,292 Epoch[010/150], Step[0150/0157], Loss: 0.8686
2022-04-17 18:49:31,139 HashNet[10/150] bit:64, lr:0.002000000, scale:1.000, train loss:0.855
2022-04-17 18:49:31,139 ----- Validation after Epoch: 10
2022-04-17 19:08:20,967 save in checkpoints_new/bit_64
2022-04-17 19:08:21,733 Max mAP so far: 0.7062 at epoch_10
2022-04-17 19:08:21,733 ----- Save BEST model: checkpoints_new/bit_64/coco.pdparams
2022-04-17 19:08:21,734 ----- Save BEST optim: checkpoints_new/bit_64/coco.pdopt
2022-04-17 19:08:21,734 HashNet epoch:10, bit:64, dataset:coco, MAP:0.706, Best MAP(e10): 0.706
2022-04-17 19:08:26,919 Epoch[011/150], Step[0000/0157], Loss: 0.8779
2022-04-17 19:08:33,799 Epoch[011/150], Step[0050/0157], Loss: 0.8378
2022-04-17 19:08:40,906 Epoch[011/150], Step[0100/0157], Loss: 0.8602
2022-04-17 19:08:48,123 Epoch[011/150], Step[0150/0157], Loss: 0.7905
2022-04-17 19:08:48,939 HashNet[11/150] bit:64, lr:0.002000000, scale:1.000, train loss:0.840
2022-04-17 19:08:53,619 Epoch[012/150], Step[0000/0157], Loss: 0.8619
2022-04-17 19:09:00,492 Epoch[012/150], Step[0050/0157], Loss: 0.8330
2022-04-17 19:09:07,720 Epoch[012/150], Step[0100/0157], Loss: 0.8033
2022-04-17 19:09:14,619 Epoch[012/150], Step[0150/0157], Loss: 0.8485
2022-04-17 19:09:15,445 HashNet[12/150] bit:64, lr:0.002000000, scale:1.000, train loss:0.827
2022-04-17 19:09:20,013 Epoch[013/150], Step[0000/0157], Loss: 0.7440
2022-04-17 19:09:26,898 Epoch[013/150], Step[0050/0157], Loss: 0.8080
2022-04-17 19:09:34,085 Epoch[013/150], Step[0100/0157], Loss: 0.8588
2022-04-17 19:09:40,912 Epoch[013/150], Step[0150/0157], Loss: 0.8090
2022-04-17 19:09:41,826 HashNet[13/150] bit:64, lr:0.002000000, scale:1.000, train loss:0.813
2022-04-17 19:09:46,527 Epoch[014/150], Step[0000/0157], Loss: 0.7794
2022-04-17 19:09:53,485 Epoch[014/150], Step[0050/0157], Loss: 0.8314
2022-04-17 19:10:00,521 Epoch[014/150], Step[0100/0157], Loss: 0.8185
2022-04-17 19:10:07,551 Epoch[014/150], Step[0150/0157], Loss: 0.7509
2022-04-17 19:10:08,370 HashNet[14/150] bit:64, lr:0.002000000, scale:1.000, train loss:0.804
2022-04-17 19:10:13,262 Epoch[015/150], Step[0000/0157], Loss: 0.8070
2022-04-17 19:10:20,237 Epoch[015/150], Step[0050/0157], Loss: 0.7786
2022-04-17 19:10:27,375 Epoch[015/150], Step[0100/0157], Loss: 0.7838
2022-04-17 19:10:34,309 Epoch[015/150], Step[0150/0157], Loss: 0.7539
2022-04-17 19:10:35,137 HashNet[15/150] bit:64, lr:0.002000000, scale:1.000, train loss:0.797
2022-04-17 19:10:39,753 Epoch[016/150], Step[0000/0157], Loss: 0.8240
2022-04-17 19:10:46,587 Epoch[016/150], Step[0050/0157], Loss: 0.7678
2022-04-17 19:10:53,809 Epoch[016/150], Step[0100/0157], Loss: 0.7407
2022-04-17 19:11:00,630 Epoch[016/150], Step[0150/0157], Loss: 0.7807
2022-04-17 19:11:01,482 HashNet[16/150] bit:64, lr:0.002000000, scale:1.000, train loss:0.784
2022-04-17 19:11:06,076 Epoch[017/150], Step[0000/0157], Loss: 0.7635
2022-04-17 19:11:13,086 Epoch[017/150], Step[0050/0157], Loss: 0.8162
2022-04-17 19:11:20,176 Epoch[017/150], Step[0100/0157], Loss: 0.7735
2022-04-17 19:11:27,141 Epoch[017/150], Step[0150/0157], Loss: 0.7571
2022-04-17 19:11:28,070 HashNet[17/150] bit:64, lr:0.002000000, scale:1.000, train loss:0.770
2022-04-17 19:11:32,635 Epoch[018/150], Step[0000/0157], Loss: 0.7577
2022-04-17 19:11:39,737 Epoch[018/150], Step[0050/0157], Loss: 0.7547
2022-04-17 19:11:46,755 Epoch[018/150], Step[0100/0157], Loss: 0.7663
2022-04-17 19:11:53,833 Epoch[018/150], Step[0150/0157], Loss: 0.7506
2022-04-17 19:11:54,648 HashNet[18/150] bit:64, lr:0.002000000, scale:1.000, train loss:0.763
2022-04-17 19:11:59,492 Epoch[019/150], Step[0000/0157], Loss: 0.7879
2022-04-17 19:12:06,248 Epoch[019/150], Step[0050/0157], Loss: 0.7576
2022-04-17 19:12:13,473 Epoch[019/150], Step[0100/0157], Loss: 0.7689
2022-04-17 19:12:20,453 Epoch[019/150], Step[0150/0157], Loss: 0.7563
2022-04-17 19:12:21,532 HashNet[19/150] bit:64, lr:0.002000000, scale:1.000, train loss:0.757
2022-04-17 19:12:26,103 Epoch[020/150], Step[0000/0157], Loss: 0.7258
2022-04-17 19:12:33,037 Epoch[020/150], Step[0050/0157], Loss: 0.7225
2022-04-17 19:12:40,458 Epoch[020/150], Step[0100/0157], Loss: 0.7639
2022-04-17 19:12:47,442 Epoch[020/150], Step[0150/0157], Loss: 0.7337
2022-04-17 19:12:48,370 HashNet[20/150] bit:64, lr:0.002000000, scale:1.000, train loss:0.746
2022-04-17 19:12:48,371 ----- Validation after Epoch: 20
2022-04-17 19:18:32,858 save in checkpoints_new/bit_64
2022-04-17 19:18:35,278 Max mAP so far: 0.7223 at epoch_20
2022-04-17 19:18:35,663 ----- Save BEST model: checkpoints_new/bit_64/coco.pdparams
2022-04-17 19:18:35,663 ----- Save BEST optim: checkpoints_new/bit_64/coco.pdopt
2022-04-17 19:18:35,663 HashNet epoch:20, bit:64, dataset:coco, MAP:0.722, Best MAP(e20): 0.722
2022-04-17 19:18:40,533 Epoch[021/150], Step[0000/0157], Loss: 0.7560
2022-04-17 19:18:48,566 Epoch[021/150], Step[0050/0157], Loss: 0.7224
2022-04-17 19:18:57,354 Epoch[021/150], Step[0100/0157], Loss: 0.7629
2022-04-17 19:19:06,383 Epoch[021/150], Step[0150/0157], Loss: 0.7013
2022-04-17 19:19:07,525 HashNet[21/150] bit:64, lr:0.002000000, scale:1.414, train loss:0.731
2022-04-17 19:19:13,351 Epoch[022/150], Step[0000/0157], Loss: 0.6754
2022-04-17 19:19:22,588 Epoch[022/150], Step[0050/0157], Loss: 0.6906
2022-04-17 19:19:31,533 Epoch[022/150], Step[0100/0157], Loss: 0.7012
2022-04-17 19:19:39,945 Epoch[022/150], Step[0150/0157], Loss: 0.7476
2022-04-17 19:19:40,774 HashNet[22/150] bit:64, lr:0.002000000, scale:1.414, train loss:0.721
2022-04-17 19:19:46,473 Epoch[023/150], Step[0000/0157], Loss: 0.7307
2022-04-17 19:19:55,340 Epoch[023/150], Step[0050/0157], Loss: 0.7133
2022-04-17 19:20:02,649 Epoch[023/150], Step[0100/0157], Loss: 0.7136
2022-04-17 19:20:09,592 Epoch[023/150], Step[0150/0157], Loss: 0.7647
2022-04-17 19:20:10,410 HashNet[23/150] bit:64, lr:0.002000000, scale:1.414, train loss:0.726
2022-04-17 19:20:14,984 Epoch[024/150], Step[0000/0157], Loss: 0.7057
2022-04-17 19:20:21,858 Epoch[024/150], Step[0050/0157], Loss: 0.7314
2022-04-17 19:20:29,128 Epoch[024/150], Step[0100/0157], Loss: 0.7508
2022-04-17 19:20:35,971 Epoch[024/150], Step[0150/0157], Loss: 0.6874
2022-04-17 19:20:36,807 HashNet[24/150] bit:64, lr:0.002000000, scale:1.414, train loss:0.712
2022-04-17 19:20:41,432 Epoch[025/150], Step[0000/0157], Loss: 0.7138
2022-04-17 19:20:48,315 Epoch[025/150], Step[0050/0157], Loss: 0.7340
2022-04-17 19:20:55,437 Epoch[025/150], Step[0100/0157], Loss: 0.7189
2022-04-17 19:21:02,394 Epoch[025/150], Step[0150/0157], Loss: 0.7094
2022-04-17 19:21:03,205 HashNet[25/150] bit:64, lr:0.002000000, scale:1.414, train loss:0.706
2022-04-17 19:21:07,735 Epoch[026/150], Step[0000/0157], Loss: 0.7128
2022-04-17 19:21:14,632 Epoch[026/150], Step[0050/0157], Loss: 0.6944
2022-04-17 19:21:21,685 Epoch[026/150], Step[0100/0157], Loss: 0.7040
2022-04-17 19:21:28,612 Epoch[026/150], Step[0150/0157], Loss: 0.7322
2022-04-17 19:21:29,505 HashNet[26/150] bit:64, lr:0.002000000, scale:1.414, train loss:0.705
2022-04-17 19:21:34,191 Epoch[027/150], Step[0000/0157], Loss: 0.7160
2022-04-17 19:21:41,112 Epoch[027/150], Step[0050/0157], Loss: 0.6686
2022-04-17 19:21:48,352 Epoch[027/150], Step[0100/0157], Loss: 0.7359
2022-04-17 19:21:55,251 Epoch[027/150], Step[0150/0157], Loss: 0.7113
2022-04-17 19:21:56,080 HashNet[27/150] bit:64, lr:0.002000000, scale:1.414, train loss:0.692
2022-04-17 19:22:00,724 Epoch[028/150], Step[0000/0157], Loss: 0.6980
2022-04-17 19:22:07,995 Epoch[028/150], Step[0050/0157], Loss: 0.6280
2022-04-17 19:22:15,063 Epoch[028/150], Step[0100/0157], Loss: 0.6773
2022-04-17 19:22:22,204 Epoch[028/150], Step[0150/0157], Loss: 0.7188
2022-04-17 19:22:23,011 HashNet[28/150] bit:64, lr:0.002000000, scale:1.414, train loss:0.688
2022-04-17 19:22:27,779 Epoch[029/150], Step[0000/0157], Loss: 0.6892
2022-04-17 19:22:34,750 Epoch[029/150], Step[0050/0157], Loss: 0.6864
2022-04-17 19:22:41,934 Epoch[029/150], Step[0100/0157], Loss: 0.6401
2022-04-17 19:22:48,967 Epoch[029/150], Step[0150/0157], Loss: 0.6955
2022-04-17 19:22:49,794 HashNet[29/150] bit:64, lr:0.002000000, scale:1.414, train loss:0.682
2022-04-17 19:22:54,461 Epoch[030/150], Step[0000/0157], Loss: 0.6814
2022-04-17 19:23:01,384 Epoch[030/150], Step[0050/0157], Loss: 0.6576
2022-04-17 19:23:08,467 Epoch[030/150], Step[0100/0157], Loss: 0.6842
2022-04-17 19:23:15,517 Epoch[030/150], Step[0150/0157], Loss: 0.6587
2022-04-17 19:23:16,377 HashNet[30/150] bit:64, lr:0.002000000, scale:1.414, train loss:0.672
2022-04-17 19:23:16,377 ----- Validation after Epoch: 30
2022-04-17 19:28:50,947 HashNet epoch:30, bit:64, dataset:coco, MAP:0.721, Best MAP(e20): 0.722
2022-04-17 19:28:55,570 Epoch[031/150], Step[0000/0157], Loss: 0.6677
2022-04-17 19:29:02,392 Epoch[031/150], Step[0050/0157], Loss: 0.6768
2022-04-17 19:29:09,526 Epoch[031/150], Step[0100/0157], Loss: 0.6686
2022-04-17 19:29:16,452 Epoch[031/150], Step[0150/0157], Loss: 0.6600
2022-04-17 19:29:17,272 HashNet[31/150] bit:64, lr:0.002000000, scale:1.414, train loss:0.671
2022-04-17 19:29:21,881 Epoch[032/150], Step[0000/0157], Loss: 0.6434
2022-04-17 19:29:28,733 Epoch[032/150], Step[0050/0157], Loss: 0.6415
2022-04-17 19:29:35,903 Epoch[032/150], Step[0100/0157], Loss: 0.7161
2022-04-17 19:29:42,790 Epoch[032/150], Step[0150/0157], Loss: 0.6931
2022-04-17 19:29:43,609 HashNet[32/150] bit:64, lr:0.002000000, scale:1.414, train loss:0.669
2022-04-17 19:29:48,189 Epoch[033/150], Step[0000/0157], Loss: 0.6508
2022-04-17 19:29:55,067 Epoch[033/150], Step[0050/0157], Loss: 0.6555
2022-04-17 19:30:02,321 Epoch[033/150], Step[0100/0157], Loss: 0.6489
2022-04-17 19:30:09,383 Epoch[033/150], Step[0150/0157], Loss: 0.6719
2022-04-17 19:30:10,226 HashNet[33/150] bit:64, lr:0.002000000, scale:1.414, train loss:0.664
2022-04-17 19:30:14,803 Epoch[034/150], Step[0000/0157], Loss: 0.6884
2022-04-17 19:30:21,662 Epoch[034/150], Step[0050/0157], Loss: 0.6641
2022-04-17 19:30:28,834 Epoch[034/150], Step[0100/0157], Loss: 0.6961
2022-04-17 19:30:35,633 Epoch[034/150], Step[0150/0157], Loss: 0.6500
2022-04-17 19:30:36,532 HashNet[34/150] bit:64, lr:0.002000000, scale:1.414, train loss:0.665
2022-04-17 19:30:41,120 Epoch[035/150], Step[0000/0157], Loss: 0.6601
2022-04-17 19:30:48,072 Epoch[035/150], Step[0050/0157], Loss: 0.6777
2022-04-17 19:30:55,076 Epoch[035/150], Step[0100/0157], Loss: 0.6235
2022-04-17 19:31:02,028 Epoch[035/150], Step[0150/0157], Loss: 0.6405
2022-04-17 19:31:02,859 HashNet[35/150] bit:64, lr:0.002000000, scale:1.414, train loss:0.661
2022-04-17 19:31:07,410 Epoch[036/150], Step[0000/0157], Loss: 0.6647
2022-04-17 19:31:14,302 Epoch[036/150], Step[0050/0157], Loss: 0.6416
2022-04-17 19:31:21,583 Epoch[036/150], Step[0100/0157], Loss: 0.6508
2022-04-17 19:31:28,362 Epoch[036/150], Step[0150/0157], Loss: 0.6517
2022-04-17 19:31:29,195 HashNet[36/150] bit:64, lr:0.002000000, scale:1.414, train loss:0.657
2022-04-17 19:31:33,772 Epoch[037/150], Step[0000/0157], Loss: 0.6121
2022-04-17 19:31:41,140 Epoch[037/150], Step[0050/0157], Loss: 0.6540
2022-04-17 19:31:48,001 Epoch[037/150], Step[0100/0157], Loss: 0.6378
2022-04-17 19:31:55,125 Epoch[037/150], Step[0150/0157], Loss: 0.6519
2022-04-17 19:31:55,951 HashNet[37/150] bit:64, lr:0.002000000, scale:1.414, train loss:0.649
2022-04-17 19:32:00,543 Epoch[038/150], Step[0000/0157], Loss: 0.6524
2022-04-17 19:32:07,378 Epoch[038/150], Step[0050/0157], Loss: 0.7333
2022-04-17 19:32:14,474 Epoch[038/150], Step[0100/0157], Loss: 0.6542
2022-04-17 19:32:21,402 Epoch[038/150], Step[0150/0157], Loss: 0.6450
2022-04-17 19:32:22,238 HashNet[38/150] bit:64, lr:0.002000000, scale:1.414, train loss:0.644
2022-04-17 19:32:26,794 Epoch[039/150], Step[0000/0157], Loss: 0.6479
2022-04-17 19:32:33,697 Epoch[039/150], Step[0050/0157], Loss: 0.6448
2022-04-17 19:32:40,764 Epoch[039/150], Step[0100/0157], Loss: 0.6360
2022-04-17 19:32:47,715 Epoch[039/150], Step[0150/0157], Loss: 0.6140
2022-04-17 19:32:48,532 HashNet[39/150] bit:64, lr:0.002000000, scale:1.414, train loss:0.645
2022-04-17 19:32:53,172 Epoch[040/150], Step[0000/0157], Loss: 0.6365
2022-04-17 19:32:59,959 Epoch[040/150], Step[0050/0157], Loss: 0.6465
2022-04-17 19:33:07,110 Epoch[040/150], Step[0100/0157], Loss: 0.6592
2022-04-17 19:33:13,971 Epoch[040/150], Step[0150/0157], Loss: 0.6374
2022-04-17 19:33:14,805 HashNet[40/150] bit:64, lr:0.002000000, scale:1.414, train loss:0.642
2022-04-17 19:33:14,805 ----- Validation after Epoch: 40
2022-04-17 19:38:48,938 save in checkpoints_new/bit_64
2022-04-17 19:38:51,704 Max mAP so far: 0.7245 at epoch_40
2022-04-17 19:38:51,704 ----- Save BEST model: checkpoints_new/bit_64/coco.pdparams
2022-04-17 19:38:51,704 ----- Save BEST optim: checkpoints_new/bit_64/coco.pdopt
2022-04-17 19:38:51,704 HashNet epoch:40, bit:64, dataset:coco, MAP:0.724, Best MAP(e40): 0.724
2022-04-17 19:38:56,436 Epoch[041/150], Step[0000/0157], Loss: 0.6291
2022-04-17 19:39:03,275 Epoch[041/150], Step[0050/0157], Loss: 0.6506
2022-04-17 19:39:10,523 Epoch[041/150], Step[0100/0157], Loss: 0.6822
2022-04-17 19:39:17,562 Epoch[041/150], Step[0150/0157], Loss: 0.6464
2022-04-17 19:39:18,372 HashNet[41/150] bit:64, lr:0.002000000, scale:1.732, train loss:0.641
2022-04-17 19:39:22,946 Epoch[042/150], Step[0000/0157], Loss: 0.6297
2022-04-17 19:39:29,870 Epoch[042/150], Step[0050/0157], Loss: 0.6112
2022-04-17 19:39:36,964 Epoch[042/150], Step[0100/0157], Loss: 0.6561
2022-04-17 19:39:43,889 Epoch[042/150], Step[0150/0157], Loss: 0.6101
2022-04-17 19:39:44,733 HashNet[42/150] bit:64, lr:0.002000000, scale:1.732, train loss:0.635
2022-04-17 19:39:49,325 Epoch[043/150], Step[0000/0157], Loss: 0.6285
2022-04-17 19:39:56,193 Epoch[043/150], Step[0050/0157], Loss: 0.6160
2022-04-17 19:40:03,282 Epoch[043/150], Step[0100/0157], Loss: 0.6435
2022-04-17 19:40:10,238 Epoch[043/150], Step[0150/0157], Loss: 0.6370
2022-04-17 19:40:11,062 HashNet[43/150] bit:64, lr:0.002000000, scale:1.732, train loss:0.631
2022-04-17 19:40:15,756 Epoch[044/150], Step[0000/0157], Loss: 0.6574
2022-04-17 19:40:22,550 Epoch[044/150], Step[0050/0157], Loss: 0.6573
2022-04-17 19:40:29,675 Epoch[044/150], Step[0100/0157], Loss: 0.6081
2022-04-17 19:40:36,576 Epoch[044/150], Step[0150/0157], Loss: 0.6207
2022-04-17 19:40:37,418 HashNet[44/150] bit:64, lr:0.002000000, scale:1.732, train loss:0.633
2022-04-17 19:40:42,247 Epoch[045/150], Step[0000/0157], Loss: 0.5900
2022-04-17 19:40:49,071 Epoch[045/150], Step[0050/0157], Loss: 0.6057
2022-04-17 19:40:56,141 Epoch[045/150], Step[0100/0157], Loss: 0.6307
2022-04-17 19:41:02,942 Epoch[045/150], Step[0150/0157], Loss: 0.6632
2022-04-17 19:41:03,759 HashNet[45/150] bit:64, lr:0.002000000, scale:1.732, train loss:0.624
2022-04-17 19:41:08,458 Epoch[046/150], Step[0000/0157], Loss: 0.6207
2022-04-17 19:41:15,393 Epoch[046/150], Step[0050/0157], Loss: 0.6387
2022-04-17 19:41:22,316 Epoch[046/150], Step[0100/0157], Loss: 0.6158
2022-04-17 19:41:29,392 Epoch[046/150], Step[0150/0157], Loss: 0.6236
2022-04-17 19:41:30,216 HashNet[46/150] bit:64, lr:0.002000000, scale:1.732, train loss:0.626
2022-04-17 19:41:34,789 Epoch[047/150], Step[0000/0157], Loss: 0.6210
2022-04-17 19:41:41,638 Epoch[047/150], Step[0050/0157], Loss: 0.6291
2022-04-17 19:41:48,780 Epoch[047/150], Step[0100/0157], Loss: 0.6380
2022-04-17 19:41:55,586 Epoch[047/150], Step[0150/0157], Loss: 0.6634
2022-04-17 19:41:56,429 HashNet[47/150] bit:64, lr:0.002000000, scale:1.732, train loss:0.630
2022-04-17 19:42:01,035 Epoch[048/150], Step[0000/0157], Loss: 0.6809
2022-04-17 19:42:07,860 Epoch[048/150], Step[0050/0157], Loss: 0.6633
2022-04-17 19:42:14,996 Epoch[048/150], Step[0100/0157], Loss: 0.6225
2022-04-17 19:42:21,859 Epoch[048/150], Step[0150/0157], Loss: 0.6080
2022-04-17 19:42:22,715 HashNet[48/150] bit:64, lr:0.002000000, scale:1.732, train loss:0.627
2022-04-17 19:42:27,303 Epoch[049/150], Step[0000/0157], Loss: 0.6271
2022-04-17 19:42:34,126 Epoch[049/150], Step[0050/0157], Loss: 0.6116
2022-04-17 19:42:41,160 Epoch[049/150], Step[0100/0157], Loss: 0.6306
2022-04-17 19:42:48,132 Epoch[049/150], Step[0150/0157], Loss: 0.6495
2022-04-17 19:42:48,981 HashNet[49/150] bit:64, lr:0.002000000, scale:1.732, train loss:0.620
2022-04-17 19:42:53,605 Epoch[050/150], Step[0000/0157], Loss: 0.6420
2022-04-17 19:43:00,474 Epoch[050/150], Step[0050/0157], Loss: 0.6107
2022-04-17 19:43:07,629 Epoch[050/150], Step[0100/0157], Loss: 0.6397
2022-04-17 19:43:14,836 Epoch[050/150], Step[0150/0157], Loss: 0.6307
2022-04-17 19:43:15,658 HashNet[50/150] bit:64, lr:0.002000000, scale:1.732, train loss:0.613
2022-04-17 19:43:15,658 ----- Validation after Epoch: 50
2022-04-17 19:48:50,460 save in checkpoints_new/bit_64
2022-04-17 19:48:52,921 Max mAP so far: 0.7275 at epoch_50
2022-04-17 19:48:52,921 ----- Save BEST model: checkpoints_new/bit_64/coco.pdparams
2022-04-17 19:48:52,921 ----- Save BEST optim: checkpoints_new/bit_64/coco.pdopt
2022-04-17 19:48:52,921 HashNet epoch:50, bit:64, dataset:coco, MAP:0.727, Best MAP(e50): 0.727
2022-04-17 19:48:57,610 Epoch[051/150], Step[0000/0157], Loss: 0.6466
2022-04-17 19:49:04,427 Epoch[051/150], Step[0050/0157], Loss: 0.6104
2022-04-17 19:49:11,636 Epoch[051/150], Step[0100/0157], Loss: 0.6157
2022-04-17 19:49:18,553 Epoch[051/150], Step[0150/0157], Loss: 0.6298
2022-04-17 19:49:19,397 HashNet[51/150] bit:64, lr:0.001000000, scale:1.732, train loss:0.608
2022-04-17 19:49:24,148 Epoch[052/150], Step[0000/0157], Loss: 0.5796
2022-04-17 19:49:31,004 Epoch[052/150], Step[0050/0157], Loss: 0.6020
2022-04-17 19:49:38,117 Epoch[052/150], Step[0100/0157], Loss: 0.6190
2022-04-17 19:49:45,036 Epoch[052/150], Step[0150/0157], Loss: 0.5935
2022-04-17 19:49:45,870 HashNet[52/150] bit:64, lr:0.001000000, scale:1.732, train loss:0.609
2022-04-17 19:49:50,450 Epoch[053/150], Step[0000/0157], Loss: 0.5925
2022-04-17 19:49:57,275 Epoch[053/150], Step[0050/0157], Loss: 0.5838
2022-04-17 19:50:04,342 Epoch[053/150], Step[0100/0157], Loss: 0.6153
2022-04-17 19:50:11,293 Epoch[053/150], Step[0150/0157], Loss: 0.5637
2022-04-17 19:50:12,160 HashNet[53/150] bit:64, lr:0.001000000, scale:1.732, train loss:0.608
2022-04-17 19:50:16,731 Epoch[054/150], Step[0000/0157], Loss: 0.6220
2022-04-17 19:50:23,541 Epoch[054/150], Step[0050/0157], Loss: 0.6401
2022-04-17 19:50:30,653 Epoch[054/150], Step[0100/0157], Loss: 0.6084
2022-04-17 19:50:37,575 Epoch[054/150], Step[0150/0157], Loss: 0.6217
2022-04-17 19:50:38,436 HashNet[54/150] bit:64, lr:0.001000000, scale:1.732, train loss:0.605
2022-04-17 19:50:43,042 Epoch[055/150], Step[0000/0157], Loss: 0.6107
2022-04-17 19:50:49,984 Epoch[055/150], Step[0050/0157], Loss: 0.6348
2022-04-17 19:50:57,106 Epoch[055/150], Step[0100/0157], Loss: 0.5591
2022-04-17 19:51:03,951 Epoch[055/150], Step[0150/0157], Loss: 0.5984
2022-04-17 19:51:04,772 HashNet[55/150] bit:64, lr:0.001000000, scale:1.732, train loss:0.604
2022-04-17 19:51:09,336 Epoch[056/150], Step[0000/0157], Loss: 0.6028
2022-04-17 19:51:16,196 Epoch[056/150], Step[0050/0157], Loss: 0.5397
2022-04-17 19:51:23,323 Epoch[056/150], Step[0100/0157], Loss: 0.5764
2022-04-17 19:51:30,239 Epoch[056/150], Step[0150/0157], Loss: 0.5889
2022-04-17 19:51:31,077 HashNet[56/150] bit:64, lr:0.001000000, scale:1.732, train loss:0.602
2022-04-17 19:51:35,833 Epoch[057/150], Step[0000/0157], Loss: 0.6254
2022-04-17 19:51:42,839 Epoch[057/150], Step[0050/0157], Loss: 0.6253
2022-04-17 19:51:49,810 Epoch[057/150], Step[0100/0157], Loss: 0.6005
2022-04-17 19:51:56,970 Epoch[057/150], Step[0150/0157], Loss: 0.6104
2022-04-17 19:51:57,785 HashNet[57/150] bit:64, lr:0.001000000, scale:1.732, train loss:0.600
2022-04-17 19:52:02,459 Epoch[058/150], Step[0000/0157], Loss: 0.6036
2022-04-17 19:52:09,240 Epoch[058/150], Step[0050/0157], Loss: 0.5955
2022-04-17 19:52:16,314 Epoch[058/150], Step[0100/0157], Loss: 0.6113
2022-04-17 19:52:23,089 Epoch[058/150], Step[0150/0157], Loss: 0.5763
2022-04-17 19:52:23,949 HashNet[58/150] bit:64, lr:0.001000000, scale:1.732, train loss:0.600
2022-04-17 19:52:28,506 Epoch[059/150], Step[0000/0157], Loss: 0.6574
2022-04-17 19:52:35,394 Epoch[059/150], Step[0050/0157], Loss: 0.5506
2022-04-17 19:52:42,497 Epoch[059/150], Step[0100/0157], Loss: 0.6011
2022-04-17 19:52:49,302 Epoch[059/150], Step[0150/0157], Loss: 0.5709
2022-04-17 19:52:50,198 HashNet[59/150] bit:64, lr:0.001000000, scale:1.732, train loss:0.598
2022-04-17 19:52:54,893 Epoch[060/150], Step[0000/0157], Loss: 0.5865
2022-04-17 19:53:01,652 Epoch[060/150], Step[0050/0157], Loss: 0.5825
2022-04-17 19:53:08,755 Epoch[060/150], Step[0100/0157], Loss: 0.6161
2022-04-17 19:53:15,519 Epoch[060/150], Step[0150/0157], Loss: 0.5996
2022-04-17 19:53:16,402 HashNet[60/150] bit:64, lr:0.001000000, scale:1.732, train loss:0.597
2022-04-17 19:53:16,402 ----- Validation after Epoch: 60
2022-04-17 19:58:51,305 save in checkpoints_new/bit_64
2022-04-17 19:58:53,799 Max mAP so far: 0.7301 at epoch_60
2022-04-17 19:58:53,799 ----- Save BEST model: checkpoints_new/bit_64/coco.pdparams
2022-04-17 19:58:53,799 ----- Save BEST optim: checkpoints_new/bit_64/coco.pdopt
2022-04-17 19:58:53,800 HashNet epoch:60, bit:64, dataset:coco, MAP:0.730, Best MAP(e60): 0.730
2022-04-17 19:58:58,380 Epoch[061/150], Step[0000/0157], Loss: 0.6076
2022-04-17 19:59:05,319 Epoch[061/150], Step[0050/0157], Loss: 0.6114
2022-04-17 19:59:12,398 Epoch[061/150], Step[0100/0157], Loss: 0.6385
2022-04-17 19:59:19,400 Epoch[061/150], Step[0150/0157], Loss: 0.5857
2022-04-17 19:59:20,209 HashNet[61/150] bit:64, lr:0.001000000, scale:2.000, train loss:0.598
2022-04-17 19:59:24,896 Epoch[062/150], Step[0000/0157], Loss: 0.5758
2022-04-17 19:59:31,673 Epoch[062/150], Step[0050/0157], Loss: 0.6062
2022-04-17 19:59:38,769 Epoch[062/150], Step[0100/0157], Loss: 0.5618
2022-04-17 19:59:45,994 Epoch[062/150], Step[0150/0157], Loss: 0.6078
2022-04-17 19:59:46,856 HashNet[62/150] bit:64, lr:0.001000000, scale:2.000, train loss:0.594
2022-04-17 19:59:51,435 Epoch[063/150], Step[0000/0157], Loss: 0.5922
2022-04-17 19:59:58,300 Epoch[063/150], Step[0050/0157], Loss: 0.6271
2022-04-17 20:00:05,465 Epoch[063/150], Step[0100/0157], Loss: 0.5803
2022-04-17 20:00:12,327 Epoch[063/150], Step[0150/0157], Loss: 0.5665
2022-04-17 20:00:13,158 HashNet[63/150] bit:64, lr:0.001000000, scale:2.000, train loss:0.594
2022-04-17 20:00:17,728 Epoch[064/150], Step[0000/0157], Loss: 0.6000
2022-04-17 20:00:24,940 Epoch[064/150], Step[0050/0157], Loss: 0.5797
2022-04-17 20:00:32,087 Epoch[064/150], Step[0100/0157], Loss: 0.6074
2022-04-17 20:00:39,117 Epoch[064/150], Step[0150/0157], Loss: 0.5856
2022-04-17 20:00:39,913 HashNet[64/150] bit:64, lr:0.001000000, scale:2.000, train loss:0.593
2022-04-17 20:00:44,507 Epoch[065/150], Step[0000/0157], Loss: 0.5564
2022-04-17 20:00:51,375 Epoch[065/150], Step[0050/0157], Loss: 0.6379
2022-04-17 20:00:58,519 Epoch[065/150], Step[0100/0157], Loss: 0.6094
2022-04-17 20:01:05,393 Epoch[065/150], Step[0150/0157], Loss: 0.5917
2022-04-17 20:01:06,217 HashNet[65/150] bit:64, lr:0.001000000, scale:2.000, train loss:0.592
2022-04-17 20:01:10,788 Epoch[066/150], Step[0000/0157], Loss: 0.5942
2022-04-17 20:01:17,670 Epoch[066/150], Step[0050/0157], Loss: 0.5794
2022-04-17 20:01:24,888 Epoch[066/150], Step[0100/0157], Loss: 0.5943
2022-04-17 20:01:31,752 Epoch[066/150], Step[0150/0157], Loss: 0.6165
2022-04-17 20:01:32,802 HashNet[66/150] bit:64, lr:0.001000000, scale:2.000, train loss:0.591
2022-04-17 20:01:37,357 Epoch[067/150], Step[0000/0157], Loss: 0.6379
2022-04-17 20:01:44,224 Epoch[067/150], Step[0050/0157], Loss: 0.6023
2022-04-17 20:01:51,378 Epoch[067/150], Step[0100/0157], Loss: 0.5516
2022-04-17 20:01:58,732 Epoch[067/150], Step[0150/0157], Loss: 0.5825
2022-04-17 20:01:59,576 HashNet[67/150] bit:64, lr:0.001000000, scale:2.000, train loss:0.590
2022-04-17 20:02:04,133 Epoch[068/150], Step[0000/0157], Loss: 0.6484
2022-04-17 20:02:11,096 Epoch[068/150], Step[0050/0157], Loss: 0.5681
2022-04-17 20:02:18,140 Epoch[068/150], Step[0100/0157], Loss: 0.5915
2022-04-17 20:02:25,071 Epoch[068/150], Step[0150/0157], Loss: 0.5947
2022-04-17 20:02:25,919 HashNet[68/150] bit:64, lr:0.001000000, scale:2.000, train loss:0.589
2022-04-17 20:02:30,526 Epoch[069/150], Step[0000/0157], Loss: 0.5809
2022-04-17 20:02:37,451 Epoch[069/150], Step[0050/0157], Loss: 0.6202
2022-04-17 20:02:44,595 Epoch[069/150], Step[0100/0157], Loss: 0.6426
2022-04-17 20:02:51,398 Epoch[069/150], Step[0150/0157], Loss: 0.5582
2022-04-17 20:02:52,216 HashNet[69/150] bit:64, lr:0.001000000, scale:2.000, train loss:0.591
2022-04-17 20:02:56,804 Epoch[070/150], Step[0000/0157], Loss: 0.5445
2022-04-17 20:03:03,675 Epoch[070/150], Step[0050/0157], Loss: 0.6056
2022-04-17 20:03:10,653 Epoch[070/150], Step[0100/0157], Loss: 0.5834
2022-04-17 20:03:17,738 Epoch[070/150], Step[0150/0157], Loss: 0.5449
2022-04-17 20:03:18,569 HashNet[70/150] bit:64, lr:0.001000000, scale:2.000, train loss:0.595
2022-04-17 20:03:18,569 ----- Validation after Epoch: 70
2022-04-17 20:08:54,749 save in checkpoints_new/bit_64
2022-04-17 20:08:57,275 Max mAP so far: 0.7304 at epoch_70
2022-04-17 20:08:57,275 ----- Save BEST model: checkpoints_new/bit_64/coco.pdparams
2022-04-17 20:08:57,275 ----- Save BEST optim: checkpoints_new/bit_64/coco.pdopt
2022-04-17 20:08:57,275 HashNet epoch:70, bit:64, dataset:coco, MAP:0.730, Best MAP(e70): 0.730
2022-04-17 20:09:01,956 Epoch[071/150], Step[0000/0157], Loss: 0.5822
2022-04-17 20:09:08,780 Epoch[071/150], Step[0050/0157], Loss: 0.6059
2022-04-17 20:09:16,261 Epoch[071/150], Step[0100/0157], Loss: 0.5733
2022-04-17 20:09:23,245 Epoch[071/150], Step[0150/0157], Loss: 0.6408
2022-04-17 20:09:24,059 HashNet[71/150] bit:64, lr:0.001000000, scale:2.000, train loss:0.588
2022-04-17 20:09:28,684 Epoch[072/150], Step[0000/0157], Loss: 0.5862
2022-04-17 20:09:35,526 Epoch[072/150], Step[0050/0157], Loss: 0.5823
2022-04-17 20:09:42,629 Epoch[072/150], Step[0100/0157], Loss: 0.5534
2022-04-17 20:09:49,428 Epoch[072/150], Step[0150/0157], Loss: 0.6368
2022-04-17 20:09:50,278 HashNet[72/150] bit:64, lr:0.001000000, scale:2.000, train loss:0.588
2022-04-17 20:09:55,002 Epoch[073/150], Step[0000/0157], Loss: 0.5972
2022-04-17 20:10:01,802 Epoch[073/150], Step[0050/0157], Loss: 0.5944
2022-04-17 20:10:08,896 Epoch[073/150], Step[0100/0157], Loss: 0.5696
2022-04-17 20:10:15,755 Epoch[073/150], Step[0150/0157], Loss: 0.5557
2022-04-17 20:10:16,579 HashNet[73/150] bit:64, lr:0.001000000, scale:2.000, train loss:0.588
2022-04-17 20:10:21,179 Epoch[074/150], Step[0000/0157], Loss: 0.5411
2022-04-17 20:10:27,999 Epoch[074/150], Step[0050/0157], Loss: 0.6516
2022-04-17 20:10:35,117 Epoch[074/150], Step[0100/0157], Loss: 0.5704
2022-04-17 20:10:41,952 Epoch[074/150], Step[0150/0157], Loss: 0.5977
2022-04-17 20:10:42,795 HashNet[74/150] bit:64, lr:0.001000000, scale:2.000, train loss:0.586
2022-04-17 20:10:47,347 Epoch[075/150], Step[0000/0157], Loss: 0.5644
2022-04-17 20:10:54,214 Epoch[075/150], Step[0050/0157], Loss: 0.5971
2022-04-17 20:11:01,262 Epoch[075/150], Step[0100/0157], Loss: 0.5871
2022-04-17 20:11:08,239 Epoch[075/150], Step[0150/0157], Loss: 0.6083
2022-04-17 20:11:09,262 HashNet[75/150] bit:64, lr:0.001000000, scale:2.000, train loss:0.585
2022-04-17 20:11:13,818 Epoch[076/150], Step[0000/0157], Loss: 0.5608
2022-04-17 20:11:20,669 Epoch[076/150], Step[0050/0157], Loss: 0.5963
2022-04-17 20:11:27,770 Epoch[076/150], Step[0100/0157], Loss: 0.5685
2022-04-17 20:11:34,757 Epoch[076/150], Step[0150/0157], Loss: 0.6040
2022-04-17 20:11:35,591 HashNet[76/150] bit:64, lr:0.001000000, scale:2.000, train loss:0.588
2022-04-17 20:11:40,184 Epoch[077/150], Step[0000/0157], Loss: 0.5810
2022-04-17 20:11:47,022 Epoch[077/150], Step[0050/0157], Loss: 0.5875
2022-04-17 20:11:54,109 Epoch[077/150], Step[0100/0157], Loss: 0.6130
2022-04-17 20:12:01,066 Epoch[077/150], Step[0150/0157], Loss: 0.6076
2022-04-17 20:12:01,891 HashNet[77/150] bit:64, lr:0.001000000, scale:2.000, train loss:0.589
2022-04-17 20:12:06,486 Epoch[078/150], Step[0000/0157], Loss: 0.6216
2022-04-17 20:12:13,340 Epoch[078/150], Step[0050/0157], Loss: 0.5551
2022-04-17 20:12:20,353 Epoch[078/150], Step[0100/0157], Loss: 0.6232
2022-04-17 20:12:27,318 Epoch[078/150], Step[0150/0157], Loss: 0.5546
2022-04-17 20:12:28,139 HashNet[78/150] bit:64, lr:0.001000000, scale:2.000, train loss:0.588
2022-04-17 20:12:32,748 Epoch[079/150], Step[0000/0157], Loss: 0.5923
2022-04-17 20:12:39,635 Epoch[079/150], Step[0050/0157], Loss: 0.6079
2022-04-17 20:12:46,818 Epoch[079/150], Step[0100/0157], Loss: 0.5484
2022-04-17 20:12:53,714 Epoch[079/150], Step[0150/0157], Loss: 0.6325
2022-04-17 20:12:54,564 HashNet[79/150] bit:64, lr:0.001000000, scale:2.000, train loss:0.588
2022-04-17 20:12:59,138 Epoch[080/150], Step[0000/0157], Loss: 0.6215
2022-04-17 20:13:06,025 Epoch[080/150], Step[0050/0157], Loss: 0.5708
2022-04-17 20:13:13,190 Epoch[080/150], Step[0100/0157], Loss: 0.5666
2022-04-17 20:13:20,000 Epoch[080/150], Step[0150/0157], Loss: 0.5923
2022-04-17 20:13:20,907 HashNet[80/150] bit:64, lr:0.001000000, scale:2.000, train loss:0.588
2022-04-17 20:13:20,907 ----- Validation after Epoch: 80
2022-04-17 20:18:55,081 save in checkpoints_new/bit_64
2022-04-17 20:18:57,568 Max mAP so far: 0.7328 at epoch_80
2022-04-17 20:18:57,569 ----- Save BEST model: checkpoints_new/bit_64/coco.pdparams
2022-04-17 20:18:57,569 ----- Save BEST optim: checkpoints_new/bit_64/coco.pdopt
2022-04-17 20:18:57,569 HashNet epoch:80, bit:64, dataset:coco, MAP:0.733, Best MAP(e80): 0.733
2022-04-17 20:19:02,380 Epoch[081/150], Step[0000/0157], Loss: 0.5739
2022-04-17 20:19:09,122 Epoch[081/150], Step[0050/0157], Loss: 0.5805
2022-04-17 20:19:16,235 Epoch[081/150], Step[0100/0157], Loss: 0.5728
2022-04-17 20:19:23,043 Epoch[081/150], Step[0150/0157], Loss: 0.6042
2022-04-17 20:19:23,866 HashNet[81/150] bit:64, lr:0.001000000, scale:2.236, train loss:0.584
2022-04-17 20:19:28,449 Epoch[082/150], Step[0000/0157], Loss: 0.5756
2022-04-17 20:19:35,259 Epoch[082/150], Step[0050/0157], Loss: 0.5513
2022-04-17 20:19:42,438 Epoch[082/150], Step[0100/0157], Loss: 0.6269
2022-04-17 20:19:49,250 Epoch[082/150], Step[0150/0157], Loss: 0.5767
2022-04-17 20:19:50,076 HashNet[82/150] bit:64, lr:0.001000000, scale:2.236, train loss:0.583
2022-04-17 20:19:54,693 Epoch[083/150], Step[0000/0157], Loss: 0.6190
2022-04-17 20:20:01,537 Epoch[083/150], Step[0050/0157], Loss: 0.5787
2022-04-17 20:20:08,592 Epoch[083/150], Step[0100/0157], Loss: 0.6003
2022-04-17 20:20:15,513 Epoch[083/150], Step[0150/0157], Loss: 0.5712
2022-04-17 20:20:16,349 HashNet[83/150] bit:64, lr:0.001000000, scale:2.236, train loss:0.583
2022-04-17 20:20:20,932 Epoch[084/150], Step[0000/0157], Loss: 0.5850
2022-04-17 20:20:27,759 Epoch[084/150], Step[0050/0157], Loss: 0.5704
2022-04-17 20:20:34,988 Epoch[084/150], Step[0100/0157], Loss: 0.5789
2022-04-17 20:20:42,285 Epoch[084/150], Step[0150/0157], Loss: 0.6036
2022-04-17 20:20:43,144 HashNet[84/150] bit:64, lr:0.001000000, scale:2.236, train loss:0.583
2022-04-17 20:20:47,732 Epoch[085/150], Step[0000/0157], Loss: 0.5942
2022-04-17 20:20:54,572 Epoch[085/150], Step[0050/0157], Loss: 0.5753
2022-04-17 20:21:01,853 Epoch[085/150], Step[0100/0157], Loss: 0.5959
2022-04-17 20:21:09,322 Epoch[085/150], Step[0150/0157], Loss: 0.6181
2022-04-17 20:21:10,145 HashNet[85/150] bit:64, lr:0.001000000, scale:2.236, train loss:0.583
2022-04-17 20:21:14,742 Epoch[086/150], Step[0000/0157], Loss: 0.5552
2022-04-17 20:21:21,627 Epoch[086/150], Step[0050/0157], Loss: 0.5661
2022-04-17 20:21:28,939 Epoch[086/150], Step[0100/0157], Loss: 0.5519
2022-04-17 20:21:35,871 Epoch[086/150], Step[0150/0157], Loss: 0.5685
2022-04-17 20:21:36,919 HashNet[86/150] bit:64, lr:0.001000000, scale:2.236, train loss:0.581
2022-04-17 20:21:41,667 Epoch[087/150], Step[0000/0157], Loss: 0.5637
2022-04-17 20:21:48,481 Epoch[087/150], Step[0050/0157], Loss: 0.5721
2022-04-17 20:21:55,614 Epoch[087/150], Step[0100/0157], Loss: 0.5781
2022-04-17 20:22:02,424 Epoch[087/150], Step[0150/0157], Loss: 0.5887
2022-04-17 20:22:03,274 HashNet[87/150] bit:64, lr:0.001000000, scale:2.236, train loss:0.583
2022-04-17 20:22:07,855 Epoch[088/150], Step[0000/0157], Loss: 0.5937
2022-04-17 20:22:14,715 Epoch[088/150], Step[0050/0157], Loss: 0.5842
2022-04-17 20:22:21,703 Epoch[088/150], Step[0100/0157], Loss: 0.5838
2022-04-17 20:22:28,617 Epoch[088/150], Step[0150/0157], Loss: 0.5617
2022-04-17 20:22:29,452 HashNet[88/150] bit:64, lr:0.001000000, scale:2.236, train loss:0.582
2022-04-17 20:22:34,008 Epoch[089/150], Step[0000/0157], Loss: 0.5691
2022-04-17 20:22:41,157 Epoch[089/150], Step[0050/0157], Loss: 0.6138
2022-04-17 20:22:48,304 Epoch[089/150], Step[0100/0157], Loss: 0.6197
2022-04-17 20:22:55,156 Epoch[089/150], Step[0150/0157], Loss: 0.5768
2022-04-17 20:22:55,979 HashNet[89/150] bit:64, lr:0.001000000, scale:2.236, train loss:0.583
2022-04-17 20:23:00,746 Epoch[090/150], Step[0000/0157], Loss: 0.5815
2022-04-17 20:23:07,755 Epoch[090/150], Step[0050/0157], Loss: 0.5712
2022-04-17 20:23:14,662 Epoch[090/150], Step[0100/0157], Loss: 0.5856
2022-04-17 20:23:21,584 Epoch[090/150], Step[0150/0157], Loss: 0.5810
2022-04-17 20:23:22,411 HashNet[90/150] bit:64, lr:0.001000000, scale:2.236, train loss:0.580
2022-04-17 20:23:22,411 ----- Validation after Epoch: 90
2022-04-17 20:28:58,800 HashNet epoch:90, bit:64, dataset:coco, MAP:0.732, Best MAP(e80): 0.733
2022-04-17 20:29:03,405 Epoch[091/150], Step[0000/0157], Loss: 0.5877
2022-04-17 20:29:10,316 Epoch[091/150], Step[0050/0157], Loss: 0.6113
2022-04-17 20:29:17,458 Epoch[091/150], Step[0100/0157], Loss: 0.5561
2022-04-17 20:29:24,395 Epoch[091/150], Step[0150/0157], Loss: 0.6035
2022-04-17 20:29:25,206 HashNet[91/150] bit:64, lr:0.001000000, scale:2.236, train loss:0.581
2022-04-17 20:29:29,768 Epoch[092/150], Step[0000/0157], Loss: 0.5428
2022-04-17 20:29:36,624 Epoch[092/150], Step[0050/0157], Loss: 0.6006
2022-04-17 20:29:43,917 Epoch[092/150], Step[0100/0157], Loss: 0.5916
2022-04-17 20:29:51,129 Epoch[092/150], Step[0150/0157], Loss: 0.5714
2022-04-17 20:29:52,183 HashNet[92/150] bit:64, lr:0.001000000, scale:2.236, train loss:0.581
2022-04-17 20:29:56,696 Epoch[093/150], Step[0000/0157], Loss: 0.5366
2022-04-17 20:30:03,587 Epoch[093/150], Step[0050/0157], Loss: 0.5882
2022-04-17 20:30:10,770 Epoch[093/150], Step[0100/0157], Loss: 0.5843
2022-04-17 20:30:17,627 Epoch[093/150], Step[0150/0157], Loss: 0.5833
2022-04-17 20:30:18,467 HashNet[93/150] bit:64, lr:0.001000000, scale:2.236, train loss:0.581
2022-04-17 20:30:23,003 Epoch[094/150], Step[0000/0157], Loss: 0.5616
2022-04-17 20:30:29,882 Epoch[094/150], Step[0050/0157], Loss: 0.5294
2022-04-17 20:30:37,357 Epoch[094/150], Step[0100/0157], Loss: 0.6106
2022-04-17 20:30:44,342 Epoch[094/150], Step[0150/0157], Loss: 0.5889
2022-04-17 20:30:45,165 HashNet[94/150] bit:64, lr:0.001000000, scale:2.236, train loss:0.580
2022-04-17 20:30:49,730 Epoch[095/150], Step[0000/0157], Loss: 0.5961
2022-04-17 20:30:56,696 Epoch[095/150], Step[0050/0157], Loss: 0.6165
2022-04-17 20:31:03,880 Epoch[095/150], Step[0100/0157], Loss: 0.6155
2022-04-17 20:31:10,770 Epoch[095/150], Step[0150/0157], Loss: 0.5651
2022-04-17 20:31:11,624 HashNet[95/150] bit:64, lr:0.001000000, scale:2.236, train loss:0.581
2022-04-17 20:31:16,202 Epoch[096/150], Step[0000/0157], Loss: 0.6176
2022-04-17 20:31:23,039 Epoch[096/150], Step[0050/0157], Loss: 0.5868
2022-04-17 20:31:30,178 Epoch[096/150], Step[0100/0157], Loss: 0.5691
2022-04-17 20:31:37,033 Epoch[096/150], Step[0150/0157], Loss: 0.5659
2022-04-17 20:31:37,842 HashNet[96/150] bit:64, lr:0.001000000, scale:2.236, train loss:0.577
2022-04-17 20:31:42,420 Epoch[097/150], Step[0000/0157], Loss: 0.5776
2022-04-17 20:31:49,319 Epoch[097/150], Step[0050/0157], Loss: 0.5650
2022-04-17 20:31:56,316 Epoch[097/150], Step[0100/0157], Loss: 0.5797
2022-04-17 20:32:03,454 Epoch[097/150], Step[0150/0157], Loss: 0.5858
2022-04-17 20:32:04,315 HashNet[97/150] bit:64, lr:0.001000000, scale:2.236, train loss:0.580
2022-04-17 20:32:08,865 Epoch[098/150], Step[0000/0157], Loss: 0.6061
2022-04-17 20:32:15,744 Epoch[098/150], Step[0050/0157], Loss: 0.5639
2022-04-17 20:32:22,762 Epoch[098/150], Step[0100/0157], Loss: 0.5667
2022-04-17 20:32:29,720 Epoch[098/150], Step[0150/0157], Loss: 0.6143
2022-04-17 20:32:30,547 HashNet[98/150] bit:64, lr:0.001000000, scale:2.236, train loss:0.578
2022-04-17 20:32:35,248 Epoch[099/150], Step[0000/0157], Loss: 0.5634
2022-04-17 20:32:42,038 Epoch[099/150], Step[0050/0157], Loss: 0.5790
2022-04-17 20:32:49,135 Epoch[099/150], Step[0100/0157], Loss: 0.5587
2022-04-17 20:32:56,034 Epoch[099/150], Step[0150/0157], Loss: 0.5846
2022-04-17 20:32:56,845 HashNet[99/150] bit:64, lr:0.001000000, scale:2.236, train loss:0.579
2022-04-17 20:33:01,441 Epoch[100/150], Step[0000/0157], Loss: 0.5484
2022-04-17 20:33:08,326 Epoch[100/150], Step[0050/0157], Loss: 0.6187
2022-04-17 20:33:15,493 Epoch[100/150], Step[0100/0157], Loss: 0.6081
2022-04-17 20:33:22,326 Epoch[100/150], Step[0150/0157], Loss: 0.5655
2022-04-17 20:33:23,171 HashNet[100/150] bit:64, lr:0.001000000, scale:2.236, train loss:0.578
2022-04-17 20:33:23,172 ----- Validation after Epoch: 100
2022-04-17 20:38:56,808 save in checkpoints_new/bit_64
2022-04-17 20:38:59,327 Max mAP so far: 0.7330 at epoch_100
2022-04-17 20:38:59,728 ----- Save BEST model: checkpoints_new/bit_64/coco.pdparams
2022-04-17 20:38:59,729 ----- Save BEST optim: checkpoints_new/bit_64/coco.pdopt
2022-04-17 20:38:59,729 HashNet epoch:100, bit:64, dataset:coco, MAP:0.733, Best MAP(e100): 0.733
2022-04-17 20:39:04,400 Epoch[101/150], Step[0000/0157], Loss: 0.5453
2022-04-17 20:39:11,243 Epoch[101/150], Step[0050/0157], Loss: 0.5741
2022-04-17 20:39:18,391 Epoch[101/150], Step[0100/0157], Loss: 0.5745
2022-04-17 20:39:25,523 Epoch[101/150], Step[0150/0157], Loss: 0.5555
2022-04-17 20:39:26,320 HashNet[101/150] bit:64, lr:0.000500000, scale:2.449, train loss:0.574
2022-04-17 20:39:30,887 Epoch[102/150], Step[0000/0157], Loss: 0.5319
2022-04-17 20:39:37,759 Epoch[102/150], Step[0050/0157], Loss: 0.5990
2022-04-17 20:39:44,887 Epoch[102/150], Step[0100/0157], Loss: 0.5649
2022-04-17 20:39:51,846 Epoch[102/150], Step[0150/0157], Loss: 0.5936
2022-04-17 20:39:52,687 HashNet[102/150] bit:64, lr:0.000500000, scale:2.449, train loss:0.572
2022-04-17 20:39:57,285 Epoch[103/150], Step[0000/0157], Loss: 0.5652
2022-04-17 20:40:04,133 Epoch[103/150], Step[0050/0157], Loss: 0.5466
2022-04-17 20:40:11,263 Epoch[103/150], Step[0100/0157], Loss: 0.5297
2022-04-17 20:40:18,120 Epoch[103/150], Step[0150/0157], Loss: 0.5953
2022-04-17 20:40:18,951 HashNet[103/150] bit:64, lr:0.000500000, scale:2.449, train loss:0.571
2022-04-17 20:40:23,499 Epoch[104/150], Step[0000/0157], Loss: 0.5694
2022-04-17 20:40:30,393 Epoch[104/150], Step[0050/0157], Loss: 0.5839
2022-04-17 20:40:37,539 Epoch[104/150], Step[0100/0157], Loss: 0.5577
2022-04-17 20:40:44,435 Epoch[104/150], Step[0150/0157], Loss: 0.5798
2022-04-17 20:40:45,384 HashNet[104/150] bit:64, lr:0.000500000, scale:2.449, train loss:0.572
2022-04-17 20:40:50,018 Epoch[105/150], Step[0000/0157], Loss: 0.5585
2022-04-17 20:40:56,834 Epoch[105/150], Step[0050/0157], Loss: 0.5504
2022-04-17 20:41:03,971 Epoch[105/150], Step[0100/0157], Loss: 0.5995
2022-04-17 20:41:10,846 Epoch[105/150], Step[0150/0157], Loss: 0.5475
2022-04-17 20:41:11,666 HashNet[105/150] bit:64, lr:0.000500000, scale:2.449, train loss:0.572
2022-04-17 20:41:16,239 Epoch[106/150], Step[0000/0157], Loss: 0.5942
2022-04-17 20:41:23,045 Epoch[106/150], Step[0050/0157], Loss: 0.5625
2022-04-17 20:41:30,182 Epoch[106/150], Step[0100/0157], Loss: 0.6056
2022-04-17 20:41:37,089 Epoch[106/150], Step[0150/0157], Loss: 0.5702
2022-04-17 20:41:37,930 HashNet[106/150] bit:64, lr:0.000500000, scale:2.449, train loss:0.573
2022-04-17 20:41:42,662 Epoch[107/150], Step[0000/0157], Loss: 0.5898
2022-04-17 20:41:49,440 Epoch[107/150], Step[0050/0157], Loss: 0.5700
2022-04-17 20:41:56,538 Epoch[107/150], Step[0100/0157], Loss: 0.5899
2022-04-17 20:42:03,328 Epoch[107/150], Step[0150/0157], Loss: 0.5767
2022-04-17 20:42:04,232 HashNet[107/150] bit:64, lr:0.000500000, scale:2.449, train loss:0.569
2022-04-17 20:42:08,949 Epoch[108/150], Step[0000/0157], Loss: 0.5330
2022-04-17 20:42:15,701 Epoch[108/150], Step[0050/0157], Loss: 0.5895
2022-04-17 20:42:22,749 Epoch[108/150], Step[0100/0157], Loss: 0.5951
2022-04-17 20:42:29,580 Epoch[108/150], Step[0150/0157], Loss: 0.5997
2022-04-17 20:42:30,467 HashNet[108/150] bit:64, lr:0.000500000, scale:2.449, train loss:0.570
2022-04-17 20:42:35,088 Epoch[109/150], Step[0000/0157], Loss: 0.5925
2022-04-17 20:42:41,954 Epoch[109/150], Step[0050/0157], Loss: 0.5706
2022-04-17 20:42:49,094 Epoch[109/150], Step[0100/0157], Loss: 0.5696
2022-04-17 20:42:55,895 Epoch[109/150], Step[0150/0157], Loss: 0.5675
2022-04-17 20:42:56,722 HashNet[109/150] bit:64, lr:0.000500000, scale:2.449, train loss:0.570
2022-04-17 20:43:01,299 Epoch[110/150], Step[0000/0157], Loss: 0.5773
2022-04-17 20:43:08,125 Epoch[110/150], Step[0050/0157], Loss: 0.5770
2022-04-17 20:43:15,201 Epoch[110/150], Step[0100/0157], Loss: 0.5433
2022-04-17 20:43:22,114 Epoch[110/150], Step[0150/0157], Loss: 0.5834
2022-04-17 20:43:22,952 HashNet[110/150] bit:64, lr:0.000500000, scale:2.449, train loss:0.570
2022-04-17 20:43:22,952 ----- Validation after Epoch: 110
2022-04-17 20:48:55,384 save in checkpoints_new/bit_64
2022-04-17 20:48:58,271 Max mAP so far: 0.7337 at epoch_110
2022-04-17 20:48:58,271 ----- Save BEST model: checkpoints_new/bit_64/coco.pdparams
2022-04-17 20:48:58,271 ----- Save BEST optim: checkpoints_new/bit_64/coco.pdopt
2022-04-17 20:48:58,271 HashNet epoch:110, bit:64, dataset:coco, MAP:0.734, Best MAP(e110): 0.734
2022-04-17 20:49:02,904 Epoch[111/150], Step[0000/0157], Loss: 0.6009
2022-04-17 20:49:10,155 Epoch[111/150], Step[0050/0157], Loss: 0.5595
2022-04-17 20:49:17,048 Epoch[111/150], Step[0100/0157], Loss: 0.5516
2022-04-17 20:49:24,223 Epoch[111/150], Step[0150/0157], Loss: 0.5623
2022-04-17 20:49:25,038 HashNet[111/150] bit:64, lr:0.000500000, scale:2.449, train loss:0.570
2022-04-17 20:49:29,702 Epoch[112/150], Step[0000/0157], Loss: 0.5424
2022-04-17 20:49:36,615 Epoch[112/150], Step[0050/0157], Loss: 0.5711
2022-04-17 20:49:43,622 Epoch[112/150], Step[0100/0157], Loss: 0.5386
2022-04-17 20:49:50,543 Epoch[112/150], Step[0150/0157], Loss: 0.5816
2022-04-17 20:49:51,358 HashNet[112/150] bit:64, lr:0.000500000, scale:2.449, train loss:0.569
2022-04-17 20:49:55,969 Epoch[113/150], Step[0000/0157], Loss: 0.5320
2022-04-17 20:50:02,861 Epoch[113/150], Step[0050/0157], Loss: 0.5644
2022-04-17 20:50:10,545 Epoch[113/150], Step[0100/0157], Loss: 0.6014
2022-04-17 20:50:17,497 Epoch[113/150], Step[0150/0157], Loss: 0.5607
2022-04-17 20:50:18,320 HashNet[113/150] bit:64, lr:0.000500000, scale:2.449, train loss:0.570
2022-04-17 20:50:22,974 Epoch[114/150], Step[0000/0157], Loss: 0.5592
2022-04-17 20:50:29,999 Epoch[114/150], Step[0050/0157], Loss: 0.5913
2022-04-17 20:50:36,928 Epoch[114/150], Step[0100/0157], Loss: 0.5798
2022-04-17 20:50:43,956 Epoch[114/150], Step[0150/0157], Loss: 0.5611
2022-04-17 20:50:44,776 HashNet[114/150] bit:64, lr:0.000500000, scale:2.449, train loss:0.571
2022-04-17 20:50:49,353 Epoch[115/150], Step[0000/0157], Loss: 0.5643
2022-04-17 20:50:56,235 Epoch[115/150], Step[0050/0157], Loss: 0.5441
2022-04-17 20:51:03,275 Epoch[115/150], Step[0100/0157], Loss: 0.5814
2022-04-17 20:51:10,141 Epoch[115/150], Step[0150/0157], Loss: 0.5954
2022-04-17 20:51:11,050 HashNet[115/150] bit:64, lr:0.000500000, scale:2.449, train loss:0.570
2022-04-17 20:51:15,931 Epoch[116/150], Step[0000/0157], Loss: 0.5505
2022-04-17 20:51:22,918 Epoch[116/150], Step[0050/0157], Loss: 0.5620
2022-04-17 20:51:30,135 Epoch[116/150], Step[0100/0157], Loss: 0.5421
2022-04-17 20:51:37,132 Epoch[116/150], Step[0150/0157], Loss: 0.5673
2022-04-17 20:51:38,218 HashNet[116/150] bit:64, lr:0.000500000, scale:2.449, train loss:0.567
2022-04-17 20:51:42,786 Epoch[117/150], Step[0000/0157], Loss: 0.5465
2022-04-17 20:51:49,632 Epoch[117/150], Step[0050/0157], Loss: 0.5733
2022-04-17 20:51:56,741 Epoch[117/150], Step[0100/0157], Loss: 0.5821
2022-04-17 20:52:03,600 Epoch[117/150], Step[0150/0157], Loss: 0.5659
2022-04-17 20:52:04,437 HashNet[117/150] bit:64, lr:0.000500000, scale:2.449, train loss:0.567
2022-04-17 20:52:09,017 Epoch[118/150], Step[0000/0157], Loss: 0.5824
2022-04-17 20:52:15,856 Epoch[118/150], Step[0050/0157], Loss: 0.5786
2022-04-17 20:52:22,932 Epoch[118/150], Step[0100/0157], Loss: 0.5765
2022-04-17 20:52:29,833 Epoch[118/150], Step[0150/0157], Loss: 0.5801
2022-04-17 20:52:30,673 HashNet[118/150] bit:64, lr:0.000500000, scale:2.449, train loss:0.570
2022-04-17 20:52:35,255 Epoch[119/150], Step[0000/0157], Loss: 0.5770
2022-04-17 20:52:42,196 Epoch[119/150], Step[0050/0157], Loss: 0.5894
2022-04-17 20:52:49,095 Epoch[119/150], Step[0100/0157], Loss: 0.5504
2022-04-17 20:52:56,208 Epoch[119/150], Step[0150/0157], Loss: 0.5400
2022-04-17 20:52:57,042 HashNet[119/150] bit:64, lr:0.000500000, scale:2.449, train loss:0.570
2022-04-17 20:53:01,636 Epoch[120/150], Step[0000/0157], Loss: 0.5597
2022-04-17 20:53:08,477 Epoch[120/150], Step[0050/0157], Loss: 0.5701
2022-04-17 20:53:15,612 Epoch[120/150], Step[0100/0157], Loss: 0.5988
2022-04-17 20:53:22,475 Epoch[120/150], Step[0150/0157], Loss: 0.5683
2022-04-17 20:53:23,321 HashNet[120/150] bit:64, lr:0.000500000, scale:2.449, train loss:0.569
2022-04-17 20:53:23,321 ----- Validation after Epoch: 120
2022-04-17 20:58:57,067 save in checkpoints_new/bit_64
2022-04-17 20:58:59,503 Max mAP so far: 0.7343 at epoch_120
2022-04-17 20:58:59,503 ----- Save BEST model: checkpoints_new/bit_64/coco.pdparams
2022-04-17 20:58:59,503 ----- Save BEST optim: checkpoints_new/bit_64/coco.pdopt
2022-04-17 20:58:59,503 HashNet epoch:120, bit:64, dataset:coco, MAP:0.734, Best MAP(e120): 0.734
2022-04-17 20:59:04,164 Epoch[121/150], Step[0000/0157], Loss: 0.5472
2022-04-17 20:59:11,083 Epoch[121/150], Step[0050/0157], Loss: 0.5743
2022-04-17 20:59:18,162 Epoch[121/150], Step[0100/0157], Loss: 0.5844
2022-04-17 20:59:25,102 Epoch[121/150], Step[0150/0157], Loss: 0.5446
2022-04-17 20:59:25,903 HashNet[121/150] bit:64, lr:0.000500000, scale:2.646, train loss:0.568
2022-04-17 20:59:30,461 Epoch[122/150], Step[0000/0157], Loss: 0.5747
2022-04-17 20:59:37,263 Epoch[122/150], Step[0050/0157], Loss: 0.5784
2022-04-17 20:59:44,411 Epoch[122/150], Step[0100/0157], Loss: 0.5801
2022-04-17 20:59:51,202 Epoch[122/150], Step[0150/0157], Loss: 0.5739
2022-04-17 20:59:52,095 HashNet[122/150] bit:64, lr:0.000500000, scale:2.646, train loss:0.567
2022-04-17 20:59:56,717 Epoch[123/150], Step[0000/0157], Loss: 0.5730
2022-04-17 21:00:03,748 Epoch[123/150], Step[0050/0157], Loss: 0.5810
2022-04-17 21:00:10,736 Epoch[123/150], Step[0100/0157], Loss: 0.6067
2022-04-17 21:00:17,799 Epoch[123/150], Step[0150/0157], Loss: 0.6320
2022-04-17 21:00:18,619 HashNet[123/150] bit:64, lr:0.000500000, scale:2.646, train loss:0.568
2022-04-17 21:00:23,212 Epoch[124/150], Step[0000/0157], Loss: 0.5722
2022-04-17 21:00:30,419 Epoch[124/150], Step[0050/0157], Loss: 0.5937
2022-04-17 21:00:37,554 Epoch[124/150], Step[0100/0157], Loss: 0.5545
2022-04-17 21:00:44,341 Epoch[124/150], Step[0150/0157], Loss: 0.5828
2022-04-17 21:00:45,166 HashNet[124/150] bit:64, lr:0.000500000, scale:2.646, train loss:0.566
2022-04-17 21:00:49,723 Epoch[125/150], Step[0000/0157], Loss: 0.5993
2022-04-17 21:00:56,563 Epoch[125/150], Step[0050/0157], Loss: 0.5744
2022-04-17 21:01:03,763 Epoch[125/150], Step[0100/0157], Loss: 0.5653
2022-04-17 21:01:10,677 Epoch[125/150], Step[0150/0157], Loss: 0.5692
2022-04-17 21:01:11,668 HashNet[125/150] bit:64, lr:0.000500000, scale:2.646, train loss:0.566
2022-04-17 21:01:16,268 Epoch[126/150], Step[0000/0157], Loss: 0.6135
2022-04-17 21:01:23,048 Epoch[126/150], Step[0050/0157], Loss: 0.5680
2022-04-17 21:01:30,142 Epoch[126/150], Step[0100/0157], Loss: 0.5500
2022-04-17 21:01:36,967 Epoch[126/150], Step[0150/0157], Loss: 0.5371
2022-04-17 21:01:37,808 HashNet[126/150] bit:64, lr:0.000500000, scale:2.646, train loss:0.568
2022-04-17 21:01:42,375 Epoch[127/150], Step[0000/0157], Loss: 0.5901
2022-04-17 21:01:49,195 Epoch[127/150], Step[0050/0157], Loss: 0.5846
2022-04-17 21:01:56,325 Epoch[127/150], Step[0100/0157], Loss: 0.6181
2022-04-17 21:02:03,159 Epoch[127/150], Step[0150/0157], Loss: 0.5500
2022-04-17 21:02:04,043 HashNet[127/150] bit:64, lr:0.000500000, scale:2.646, train loss:0.567
2022-04-17 21:02:08,742 Epoch[128/150], Step[0000/0157], Loss: 0.6149
2022-04-17 21:02:15,566 Epoch[128/150], Step[0050/0157], Loss: 0.5554
2022-04-17 21:02:22,625 Epoch[128/150], Step[0100/0157], Loss: 0.5949
2022-04-17 21:02:29,359 Epoch[128/150], Step[0150/0157], Loss: 0.5613
2022-04-17 21:02:30,180 HashNet[128/150] bit:64, lr:0.000500000, scale:2.646, train loss:0.569
2022-04-17 21:02:34,768 Epoch[129/150], Step[0000/0157], Loss: 0.5418
2022-04-17 21:02:41,540 Epoch[129/150], Step[0050/0157], Loss: 0.5564
2022-04-17 21:02:48,636 Epoch[129/150], Step[0100/0157], Loss: 0.5956
2022-04-17 21:02:55,515 Epoch[129/150], Step[0150/0157], Loss: 0.5674
2022-04-17 21:02:56,342 HashNet[129/150] bit:64, lr:0.000500000, scale:2.646, train loss:0.568
2022-04-17 21:03:00,919 Epoch[130/150], Step[0000/0157], Loss: 0.5443
2022-04-17 21:03:08,133 Epoch[130/150], Step[0050/0157], Loss: 0.5500
2022-04-17 21:03:15,281 Epoch[130/150], Step[0100/0157], Loss: 0.5497
2022-04-17 21:03:22,456 Epoch[130/150], Step[0150/0157], Loss: 0.5936
2022-04-17 21:03:23,244 HashNet[130/150] bit:64, lr:0.000500000, scale:2.646, train loss:0.568
2022-04-17 21:03:23,244 ----- Validation after Epoch: 130
2022-04-17 21:08:57,783 HashNet epoch:130, bit:64, dataset:coco, MAP:0.734, Best MAP(e120): 0.734
2022-04-17 21:09:02,441 Epoch[131/150], Step[0000/0157], Loss: 0.5631
2022-04-17 21:09:09,283 Epoch[131/150], Step[0050/0157], Loss: 0.5802
2022-04-17 21:09:16,589 Epoch[131/150], Step[0100/0157], Loss: 0.5601
2022-04-17 21:09:23,856 Epoch[131/150], Step[0150/0157], Loss: 0.5532
2022-04-17 21:09:24,925 HashNet[131/150] bit:64, lr:0.000500000, scale:2.646, train loss:0.569
2022-04-17 21:09:29,493 Epoch[132/150], Step[0000/0157], Loss: 0.5658
2022-04-17 21:09:36,398 Epoch[132/150], Step[0050/0157], Loss: 0.5748
2022-04-17 21:09:43,339 Epoch[132/150], Step[0100/0157], Loss: 0.5306
2022-04-17 21:09:50,362 Epoch[132/150], Step[0150/0157], Loss: 0.5380
2022-04-17 21:09:51,189 HashNet[132/150] bit:64, lr:0.000500000, scale:2.646, train loss:0.566
2022-04-17 21:09:55,763 Epoch[133/150], Step[0000/0157], Loss: 0.5470
2022-04-17 21:10:02,599 Epoch[133/150], Step[0050/0157], Loss: 0.6071
2022-04-17 21:10:09,734 Epoch[133/150], Step[0100/0157], Loss: 0.6064
2022-04-17 21:10:16,588 Epoch[133/150], Step[0150/0157], Loss: 0.5730
2022-04-17 21:10:17,423 HashNet[133/150] bit:64, lr:0.000500000, scale:2.646, train loss:0.563
2022-04-17 21:10:22,013 Epoch[134/150], Step[0000/0157], Loss: 0.5629
2022-04-17 21:10:28,825 Epoch[134/150], Step[0050/0157], Loss: 0.5397
2022-04-17 21:10:35,914 Epoch[134/150], Step[0100/0157], Loss: 0.5017
2022-04-17 21:10:42,735 Epoch[134/150], Step[0150/0157], Loss: 0.5936
2022-04-17 21:10:43,571 HashNet[134/150] bit:64, lr:0.000500000, scale:2.646, train loss:0.564
2022-04-17 21:10:48,129 Epoch[135/150], Step[0000/0157], Loss: 0.6145
2022-04-17 21:10:54,931 Epoch[135/150], Step[0050/0157], Loss: 0.5491
2022-04-17 21:11:02,020 Epoch[135/150], Step[0100/0157], Loss: 0.5496
2022-04-17 21:11:08,857 Epoch[135/150], Step[0150/0157], Loss: 0.5523
2022-04-17 21:11:09,674 HashNet[135/150] bit:64, lr:0.000500000, scale:2.646, train loss:0.567
2022-04-17 21:11:14,315 Epoch[136/150], Step[0000/0157], Loss: 0.5358
2022-04-17 21:11:21,124 Epoch[136/150], Step[0050/0157], Loss: 0.5461
2022-04-17 21:11:28,192 Epoch[136/150], Step[0100/0157], Loss: 0.6011
2022-04-17 21:11:35,122 Epoch[136/150], Step[0150/0157], Loss: 0.5810
2022-04-17 21:11:35,934 HashNet[136/150] bit:64, lr:0.000500000, scale:2.646, train loss:0.564
2022-04-17 21:11:40,494 Epoch[137/150], Step[0000/0157], Loss: 0.5495
2022-04-17 21:11:47,774 Epoch[137/150], Step[0050/0157], Loss: 0.5557
2022-04-17 21:11:54,844 Epoch[137/150], Step[0100/0157], Loss: 0.5667
2022-04-17 21:12:01,816 Epoch[137/150], Step[0150/0157], Loss: 0.5790
2022-04-17 21:12:02,649 HashNet[137/150] bit:64, lr:0.000500000, scale:2.646, train loss:0.568
2022-04-17 21:12:07,264 Epoch[138/150], Step[0000/0157], Loss: 0.5788
2022-04-17 21:12:14,050 Epoch[138/150], Step[0050/0157], Loss: 0.5808
2022-04-17 21:12:21,153 Epoch[138/150], Step[0100/0157], Loss: 0.5359
2022-04-17 21:12:27,990 Epoch[138/150], Step[0150/0157], Loss: 0.5885
2022-04-17 21:12:28,841 HashNet[138/150] bit:64, lr:0.000500000, scale:2.646, train loss:0.566
2022-04-17 21:12:33,391 Epoch[139/150], Step[0000/0157], Loss: 0.5845
2022-04-17 21:12:40,248 Epoch[139/150], Step[0050/0157], Loss: 0.5681
2022-04-17 21:12:47,443 Epoch[139/150], Step[0100/0157], Loss: 0.5439
2022-04-17 21:12:54,331 Epoch[139/150], Step[0150/0157], Loss: 0.5347
2022-04-17 21:12:55,162 HashNet[139/150] bit:64, lr:0.000500000, scale:2.646, train loss:0.566
2022-04-17 21:12:59,739 Epoch[140/150], Step[0000/0157], Loss: 0.5929
2022-04-17 21:13:06,613 Epoch[140/150], Step[0050/0157], Loss: 0.5462
2022-04-17 21:13:13,601 Epoch[140/150], Step[0100/0157], Loss: 0.5654
2022-04-17 21:13:20,527 Epoch[140/150], Step[0150/0157], Loss: 0.5407
2022-04-17 21:13:21,353 HashNet[140/150] bit:64, lr:0.000500000, scale:2.646, train loss:0.566
2022-04-17 21:13:21,353 ----- Validation after Epoch: 140
2022-04-17 21:18:54,301 HashNet epoch:140, bit:64, dataset:coco, MAP:0.734, Best MAP(e120): 0.734
2022-04-17 21:18:59,434 Epoch[141/150], Step[0000/0157], Loss: 0.5572
2022-04-17 21:19:06,174 Epoch[141/150], Step[0050/0157], Loss: 0.5823
2022-04-17 21:19:13,313 Epoch[141/150], Step[0100/0157], Loss: 0.5297
2022-04-17 21:19:20,171 Epoch[141/150], Step[0150/0157], Loss: 0.5839
2022-04-17 21:19:21,086 HashNet[141/150] bit:64, lr:0.000500000, scale:2.828, train loss:0.563
2022-04-17 21:19:25,661 Epoch[142/150], Step[0000/0157], Loss: 0.5583
2022-04-17 21:19:32,746 Epoch[142/150], Step[0050/0157], Loss: 0.5765
2022-04-17 21:19:39,844 Epoch[142/150], Step[0100/0157], Loss: 0.5520
2022-04-17 21:19:46,631 Epoch[142/150], Step[0150/0157], Loss: 0.5832
2022-04-17 21:19:47,450 HashNet[142/150] bit:64, lr:0.000500000, scale:2.828, train loss:0.564
2022-04-17 21:19:52,022 Epoch[143/150], Step[0000/0157], Loss: 0.5505
2022-04-17 21:19:58,892 Epoch[143/150], Step[0050/0157], Loss: 0.5834
2022-04-17 21:20:05,903 Epoch[143/150], Step[0100/0157], Loss: 0.5446
2022-04-17 21:20:12,842 Epoch[143/150], Step[0150/0157], Loss: 0.5634
2022-04-17 21:20:13,668 HashNet[143/150] bit:64, lr:0.000500000, scale:2.828, train loss:0.563
2022-04-17 21:20:18,243 Epoch[144/150], Step[0000/0157], Loss: 0.6028
2022-04-17 21:20:25,130 Epoch[144/150], Step[0050/0157], Loss: 0.5481
2022-04-17 21:20:32,193 Epoch[144/150], Step[0100/0157], Loss: 0.5395
2022-04-17 21:20:38,959 Epoch[144/150], Step[0150/0157], Loss: 0.5348
2022-04-17 21:20:39,788 HashNet[144/150] bit:64, lr:0.000500000, scale:2.828, train loss:0.563
2022-04-17 21:20:44,327 Epoch[145/150], Step[0000/0157], Loss: 0.5653
2022-04-17 21:20:51,144 Epoch[145/150], Step[0050/0157], Loss: 0.5455
2022-04-17 21:20:58,289 Epoch[145/150], Step[0100/0157], Loss: 0.5634
2022-04-17 21:21:05,129 Epoch[145/150], Step[0150/0157], Loss: 0.5508
2022-04-17 21:21:05,963 HashNet[145/150] bit:64, lr:0.000500000, scale:2.828, train loss:0.564
2022-04-17 21:21:10,617 Epoch[146/150], Step[0000/0157], Loss: 0.5589
2022-04-17 21:21:17,425 Epoch[146/150], Step[0050/0157], Loss: 0.5871
2022-04-17 21:21:24,433 Epoch[146/150], Step[0100/0157], Loss: 0.5732
2022-04-17 21:21:31,214 Epoch[146/150], Step[0150/0157], Loss: 0.5559
2022-04-17 21:21:32,049 HashNet[146/150] bit:64, lr:0.000500000, scale:2.828, train loss:0.565
2022-04-17 21:21:36,917 Epoch[147/150], Step[0000/0157], Loss: 0.5879
2022-04-17 21:21:43,814 Epoch[147/150], Step[0050/0157], Loss: 0.5699
2022-04-17 21:21:50,820 Epoch[147/150], Step[0100/0157], Loss: 0.5914
2022-04-17 21:21:57,567 Epoch[147/150], Step[0150/0157], Loss: 0.5612
2022-04-17 21:21:58,404 HashNet[147/150] bit:64, lr:0.000500000, scale:2.828, train loss:0.565
2022-04-17 21:22:03,004 Epoch[148/150], Step[0000/0157], Loss: 0.5431
2022-04-17 21:22:09,810 Epoch[148/150], Step[0050/0157], Loss: 0.5553
2022-04-17 21:22:16,886 Epoch[148/150], Step[0100/0157], Loss: 0.6006
2022-04-17 21:22:23,797 Epoch[148/150], Step[0150/0157], Loss: 0.5339
2022-04-17 21:22:24,608 HashNet[148/150] bit:64, lr:0.000500000, scale:2.828, train loss:0.563
2022-04-17 21:22:29,156 Epoch[149/150], Step[0000/0157], Loss: 0.5398
2022-04-17 21:22:36,488 Epoch[149/150], Step[0050/0157], Loss: 0.5553
2022-04-17 21:22:43,663 Epoch[149/150], Step[0100/0157], Loss: 0.5843
2022-04-17 21:22:50,564 Epoch[149/150], Step[0150/0157], Loss: 0.5539
2022-04-17 21:22:51,387 HashNet[149/150] bit:64, lr:0.000500000, scale:2.828, train loss:0.565
2022-04-17 21:22:55,988 Epoch[150/150], Step[0000/0157], Loss: 0.5398
2022-04-17 21:23:03,210 Epoch[150/150], Step[0050/0157], Loss: 0.5768
2022-04-17 21:23:10,365 Epoch[150/150], Step[0100/0157], Loss: 0.5348
2022-04-17 21:23:17,322 Epoch[150/150], Step[0150/0157], Loss: 0.5661
2022-04-17 21:23:18,122 HashNet[150/150] bit:64, lr:0.000500000, scale:2.828, train loss:0.562
2022-04-17 21:23:18,122 ----- Validation after Epoch: 150
2022-04-17 21:28:52,114 HashNet epoch:150, bit:64, dataset:coco, MAP:0.734, Best MAP(e120): 0.734
2022-04-17 21:28:52,114 Training completed for HashNet(64).
2022-04-17 21:28:52,114 Best MAP(e120): 0.734
