2022-04-18 22:42:51,368 
Namespace(alpha=0.1, batch_size=64, bit=48, ckp=None, crop_size=224, data={'train_set': {'list_path': 'data/coco/train.txt', 'batch_size': 64}, 'database': {'list_path': 'data/coco/database.txt', 'batch_size': 64}, 'test': {'list_path': 'data/coco/test.txt', 'batch_size': 64}}, data_path='/home/super/public/wht/datasets/COCO2014/', dataset='coco', de_step=50, debug_steps=50, epoch=150, eval=False, eval_epoch=10, last_epoch=0, learning_rate=0.001, log_path='logs_new/', model='HashNet', momentum=0.9, num_class=80, num_train=10000, optimizer='SGD', resize_size=256, resume=None, save_path='checkpoints_new/', seed=200, step_continuation=20, topK=5000, weight_decay=0.0005)
2022-04-18 22:42:51,369 ----- world_size = 2, local_rank = 0
2022-04-18 22:42:52,583 ----- Total # of train batch (single gpu): 157
2022-04-18 22:42:52,583 ----- Total # of test batch (single gpu): 79
2022-04-18 22:42:52,583 ----- Total # of base batch (single gpu): 1754
2022-04-18 22:42:52,583 Start training from epoch 1.
2022-04-18 22:42:59,754 Epoch[001/150], Step[0000/0157], Loss: 1.3091
2022-04-18 22:43:08,989 Epoch[001/150], Step[0050/0157], Loss: 1.1613
2022-04-18 22:43:18,979 Epoch[001/150], Step[0100/0157], Loss: 1.2445
2022-04-18 22:43:28,977 Epoch[001/150], Step[0150/0157], Loss: 1.1471
2022-04-18 22:43:30,058 HashNet[ 1/150] bit:48, lr:0.002000000, scale:1.000, train loss:1.150
2022-04-18 22:43:35,976 Epoch[002/150], Step[0000/0157], Loss: 1.1749
2022-04-18 22:43:47,020 Epoch[002/150], Step[0050/0157], Loss: 1.1490
2022-04-18 22:43:58,332 Epoch[002/150], Step[0100/0157], Loss: 1.0519
2022-04-18 22:44:09,690 Epoch[002/150], Step[0150/0157], Loss: 1.0562
2022-04-18 22:44:10,927 HashNet[ 2/150] bit:48, lr:0.002000000, scale:1.000, train loss:1.101
2022-04-18 22:44:17,172 Epoch[003/150], Step[0000/0157], Loss: 0.9937
2022-04-18 22:44:27,967 Epoch[003/150], Step[0050/0157], Loss: 0.9700
2022-04-18 22:44:39,245 Epoch[003/150], Step[0100/0157], Loss: 0.9909
2022-04-18 22:44:49,802 Epoch[003/150], Step[0150/0157], Loss: 1.0267
2022-04-18 22:44:51,563 HashNet[ 3/150] bit:48, lr:0.002000000, scale:1.000, train loss:1.005
2022-04-18 22:44:57,752 Epoch[004/150], Step[0000/0157], Loss: 0.9575
2022-04-18 22:45:08,630 Epoch[004/150], Step[0050/0157], Loss: 0.9853
2022-04-18 22:45:19,622 Epoch[004/150], Step[0100/0157], Loss: 0.9701
2022-04-18 22:45:30,553 Epoch[004/150], Step[0150/0157], Loss: 0.9498
2022-04-18 22:45:31,699 HashNet[ 4/150] bit:48, lr:0.002000000, scale:1.000, train loss:0.974
2022-04-18 22:45:37,449 Epoch[005/150], Step[0000/0157], Loss: 0.9557
2022-04-18 22:45:47,393 Epoch[005/150], Step[0050/0157], Loss: 0.9041
2022-04-18 22:45:58,033 Epoch[005/150], Step[0100/0157], Loss: 0.9341
2022-04-18 22:46:08,022 Epoch[005/150], Step[0150/0157], Loss: 0.9012
2022-04-18 22:46:09,193 HashNet[ 5/150] bit:48, lr:0.002000000, scale:1.000, train loss:0.950
2022-04-18 22:46:14,867 Epoch[006/150], Step[0000/0157], Loss: 0.9318
2022-04-18 22:46:24,956 Epoch[006/150], Step[0050/0157], Loss: 0.9482
2022-04-18 22:46:35,206 Epoch[006/150], Step[0100/0157], Loss: 0.9065
2022-04-18 22:46:45,893 Epoch[006/150], Step[0150/0157], Loss: 0.9133
2022-04-18 22:46:46,950 HashNet[ 6/150] bit:48, lr:0.002000000, scale:1.000, train loss:0.925
2022-04-18 22:46:52,780 Epoch[007/150], Step[0000/0157], Loss: 0.9140
2022-04-18 22:47:02,786 Epoch[007/150], Step[0050/0157], Loss: 0.9406
2022-04-18 22:47:13,224 Epoch[007/150], Step[0100/0157], Loss: 0.9081
2022-04-18 22:47:22,790 Epoch[007/150], Step[0150/0157], Loss: 0.9486
2022-04-18 22:47:23,983 HashNet[ 7/150] bit:48, lr:0.002000000, scale:1.000, train loss:0.910
2022-04-18 22:47:29,606 Epoch[008/150], Step[0000/0157], Loss: 0.9217
2022-04-18 22:47:40,064 Epoch[008/150], Step[0050/0157], Loss: 0.8898
2022-04-18 22:47:49,852 Epoch[008/150], Step[0100/0157], Loss: 0.9177
2022-04-18 22:48:00,039 Epoch[008/150], Step[0150/0157], Loss: 0.9320
2022-04-18 22:48:01,304 HashNet[ 8/150] bit:48, lr:0.002000000, scale:1.000, train loss:0.894
2022-04-18 22:48:06,948 Epoch[009/150], Step[0000/0157], Loss: 0.8894
2022-04-18 22:48:17,260 Epoch[009/150], Step[0050/0157], Loss: 0.8509
2022-04-18 22:48:27,032 Epoch[009/150], Step[0100/0157], Loss: 0.8555
2022-04-18 22:48:37,601 Epoch[009/150], Step[0150/0157], Loss: 0.8537
2022-04-18 22:48:38,880 HashNet[ 9/150] bit:48, lr:0.002000000, scale:1.000, train loss:0.872
2022-04-18 22:48:44,691 Epoch[010/150], Step[0000/0157], Loss: 0.8451
2022-04-18 22:48:55,285 Epoch[010/150], Step[0050/0157], Loss: 0.8691
2022-04-18 22:49:05,411 Epoch[010/150], Step[0100/0157], Loss: 0.9113
2022-04-18 22:49:15,673 Epoch[010/150], Step[0150/0157], Loss: 0.8608
2022-04-18 22:49:16,821 HashNet[10/150] bit:48, lr:0.002000000, scale:1.000, train loss:0.864
2022-04-18 22:49:16,821 ----- Validation after Epoch: 10
2022-04-18 22:56:42,412 save in checkpoints_new//train-48-20220418-22-42-51/bit_48
2022-04-18 22:56:42,999 Max mAP so far: 0.6971 at epoch_10
2022-04-18 22:56:42,999 ----- Save BEST model: checkpoints_new//train-48-20220418-22-42-51/bit_48/coco.pdparams
2022-04-18 22:56:42,999 ----- Save BEST optim: checkpoints_new//train-48-20220418-22-42-51/bit_48/coco.pdopt
2022-04-18 22:56:42,999 HashNet epoch:10, bit:48, dataset:coco, MAP:0.697, Best MAP(e10): 0.697
2022-04-18 22:56:49,369 Epoch[011/150], Step[0000/0157], Loss: 0.8357
2022-04-18 22:57:00,110 Epoch[011/150], Step[0050/0157], Loss: 0.8676
2022-04-18 22:57:11,055 Epoch[011/150], Step[0100/0157], Loss: 0.8274
2022-04-18 22:57:22,613 Epoch[011/150], Step[0150/0157], Loss: 0.8488
2022-04-18 22:57:23,786 HashNet[11/150] bit:48, lr:0.002000000, scale:1.000, train loss:0.850
2022-04-18 22:57:30,393 Epoch[012/150], Step[0000/0157], Loss: 0.8328
2022-04-18 22:57:41,418 Epoch[012/150], Step[0050/0157], Loss: 0.8444
2022-04-18 22:57:52,034 Epoch[012/150], Step[0100/0157], Loss: 0.8207
2022-04-18 22:58:03,536 Epoch[012/150], Step[0150/0157], Loss: 0.8718
2022-04-18 22:58:04,892 HashNet[12/150] bit:48, lr:0.002000000, scale:1.000, train loss:0.837
2022-04-18 22:58:11,499 Epoch[013/150], Step[0000/0157], Loss: 0.8106
2022-04-18 22:58:21,654 Epoch[013/150], Step[0050/0157], Loss: 0.8215
2022-04-18 22:58:31,828 Epoch[013/150], Step[0100/0157], Loss: 0.8502
2022-04-18 22:58:42,597 Epoch[013/150], Step[0150/0157], Loss: 0.8469
2022-04-18 22:58:43,618 HashNet[13/150] bit:48, lr:0.002000000, scale:1.000, train loss:0.824
2022-04-18 22:58:49,171 Epoch[014/150], Step[0000/0157], Loss: 0.7926
2022-04-18 22:58:59,283 Epoch[014/150], Step[0050/0157], Loss: 0.8793
2022-04-18 22:59:09,655 Epoch[014/150], Step[0100/0157], Loss: 0.8623
2022-04-18 22:59:19,668 Epoch[014/150], Step[0150/0157], Loss: 0.8116
2022-04-18 22:59:20,741 HashNet[14/150] bit:48, lr:0.002000000, scale:1.000, train loss:0.814
2022-04-18 22:59:26,671 Epoch[015/150], Step[0000/0157], Loss: 0.7933
2022-04-18 22:59:36,381 Epoch[015/150], Step[0050/0157], Loss: 0.8352
2022-04-18 22:59:46,637 Epoch[015/150], Step[0100/0157], Loss: 0.7517
2022-04-18 22:59:57,348 Epoch[015/150], Step[0150/0157], Loss: 0.7977
2022-04-18 22:59:58,315 HashNet[15/150] bit:48, lr:0.002000000, scale:1.000, train loss:0.803
2022-04-18 23:00:04,101 Epoch[016/150], Step[0000/0157], Loss: 0.7724
2022-04-18 23:00:14,762 Epoch[016/150], Step[0050/0157], Loss: 0.7742
2022-04-18 23:00:25,443 Epoch[016/150], Step[0100/0157], Loss: 0.8093
2022-04-18 23:00:35,114 Epoch[016/150], Step[0150/0157], Loss: 0.8031
2022-04-18 23:00:36,414 HashNet[16/150] bit:48, lr:0.002000000, scale:1.000, train loss:0.797
2022-04-18 23:00:42,224 Epoch[017/150], Step[0000/0157], Loss: 0.7971
2022-04-18 23:00:51,931 Epoch[017/150], Step[0050/0157], Loss: 0.7728
2022-04-18 23:01:02,194 Epoch[017/150], Step[0100/0157], Loss: 0.8467
2022-04-18 23:01:12,430 Epoch[017/150], Step[0150/0157], Loss: 0.7589
2022-04-18 23:01:13,500 HashNet[17/150] bit:48, lr:0.002000000, scale:1.000, train loss:0.790
2022-04-18 23:01:19,413 Epoch[018/150], Step[0000/0157], Loss: 0.7714
2022-04-18 23:01:29,338 Epoch[018/150], Step[0050/0157], Loss: 0.7745
2022-04-18 23:01:39,685 Epoch[018/150], Step[0100/0157], Loss: 0.7818
2022-04-18 23:01:49,992 Epoch[018/150], Step[0150/0157], Loss: 0.8024
2022-04-18 23:01:51,081 HashNet[18/150] bit:48, lr:0.002000000, scale:1.000, train loss:0.779
2022-04-18 23:01:56,952 Epoch[019/150], Step[0000/0157], Loss: 0.8099
2022-04-18 23:02:07,158 Epoch[019/150], Step[0050/0157], Loss: 0.7315
2022-04-18 23:02:16,849 Epoch[019/150], Step[0100/0157], Loss: 0.7592
2022-04-18 23:02:26,542 Epoch[019/150], Step[0150/0157], Loss: 0.7631
2022-04-18 23:02:27,779 HashNet[19/150] bit:48, lr:0.002000000, scale:1.000, train loss:0.771
2022-04-18 23:02:33,480 Epoch[020/150], Step[0000/0157], Loss: 0.7536
2022-04-18 23:02:43,404 Epoch[020/150], Step[0050/0157], Loss: 0.7682
2022-04-18 23:02:53,690 Epoch[020/150], Step[0100/0157], Loss: 0.7369
2022-04-18 23:03:03,746 Epoch[020/150], Step[0150/0157], Loss: 0.7619
2022-04-18 23:03:05,122 HashNet[20/150] bit:48, lr:0.002000000, scale:1.000, train loss:0.759
2022-04-18 23:03:05,122 ----- Validation after Epoch: 20
2022-04-18 23:10:36,948 save in checkpoints_new//train-48-20220418-22-42-51/bit_48
2022-04-18 23:10:39,108 Max mAP so far: 0.7047 at epoch_20
2022-04-18 23:10:39,108 ----- Save BEST model: checkpoints_new//train-48-20220418-22-42-51/bit_48/coco.pdparams
2022-04-18 23:10:39,108 ----- Save BEST optim: checkpoints_new//train-48-20220418-22-42-51/bit_48/coco.pdopt
2022-04-18 23:10:39,108 HashNet epoch:20, bit:48, dataset:coco, MAP:0.705, Best MAP(e20): 0.705
2022-04-18 23:10:45,654 Epoch[021/150], Step[0000/0157], Loss: 0.7361
2022-04-18 23:10:56,573 Epoch[021/150], Step[0050/0157], Loss: 0.7767
2022-04-18 23:11:07,756 Epoch[021/150], Step[0100/0157], Loss: 0.7341
2022-04-18 23:11:18,588 Epoch[021/150], Step[0150/0157], Loss: 0.7986
2022-04-18 23:11:19,746 HashNet[21/150] bit:48, lr:0.002000000, scale:1.414, train loss:0.754
2022-04-18 23:11:25,350 Epoch[022/150], Step[0000/0157], Loss: 0.7409
2022-04-18 23:11:35,471 Epoch[022/150], Step[0050/0157], Loss: 0.7418
2022-04-18 23:11:45,404 Epoch[022/150], Step[0100/0157], Loss: 0.7668
2022-04-18 23:11:55,786 Epoch[022/150], Step[0150/0157], Loss: 0.7618
2022-04-18 23:11:56,881 HashNet[22/150] bit:48, lr:0.002000000, scale:1.414, train loss:0.740
2022-04-18 23:12:02,631 Epoch[023/150], Step[0000/0157], Loss: 0.7618
2022-04-18 23:12:12,903 Epoch[023/150], Step[0050/0157], Loss: 0.7653
2022-04-18 23:12:23,092 Epoch[023/150], Step[0100/0157], Loss: 0.7566
2022-04-18 23:12:32,822 Epoch[023/150], Step[0150/0157], Loss: 0.7478
2022-04-18 23:12:33,933 HashNet[23/150] bit:48, lr:0.002000000, scale:1.414, train loss:0.743
2022-04-18 23:12:39,677 Epoch[024/150], Step[0000/0157], Loss: 0.7724
2022-04-18 23:12:49,695 Epoch[024/150], Step[0050/0157], Loss: 0.7247
2022-04-18 23:12:59,410 Epoch[024/150], Step[0100/0157], Loss: 0.7467
2022-04-18 23:13:09,236 Epoch[024/150], Step[0150/0157], Loss: 0.7432
2022-04-18 23:13:10,623 HashNet[24/150] bit:48, lr:0.002000000, scale:1.414, train loss:0.738
2022-04-18 23:13:16,329 Epoch[025/150], Step[0000/0157], Loss: 0.7840
2022-04-18 23:13:26,311 Epoch[025/150], Step[0050/0157], Loss: 0.7469
2022-04-18 23:13:36,036 Epoch[025/150], Step[0100/0157], Loss: 0.7513
2022-04-18 23:13:46,374 Epoch[025/150], Step[0150/0157], Loss: 0.7003
2022-04-18 23:13:47,750 HashNet[25/150] bit:48, lr:0.002000000, scale:1.414, train loss:0.726
2022-04-18 23:13:53,551 Epoch[026/150], Step[0000/0157], Loss: 0.6957
2022-04-18 23:14:03,976 Epoch[026/150], Step[0050/0157], Loss: 0.6798
2022-04-18 23:14:13,289 Epoch[026/150], Step[0100/0157], Loss: 0.7538
2022-04-18 23:14:23,485 Epoch[026/150], Step[0150/0157], Loss: 0.7198
2022-04-18 23:14:24,835 HashNet[26/150] bit:48, lr:0.002000000, scale:1.414, train loss:0.723
2022-04-18 23:14:30,507 Epoch[027/150], Step[0000/0157], Loss: 0.6930
2022-04-18 23:14:40,715 Epoch[027/150], Step[0050/0157], Loss: 0.7110
2022-04-18 23:14:50,934 Epoch[027/150], Step[0100/0157], Loss: 0.6475
2022-04-18 23:15:01,261 Epoch[027/150], Step[0150/0157], Loss: 0.6983
2022-04-18 23:15:02,317 HashNet[27/150] bit:48, lr:0.002000000, scale:1.414, train loss:0.715
2022-04-18 23:15:08,039 Epoch[028/150], Step[0000/0157], Loss: 0.7112
2022-04-18 23:15:18,105 Epoch[028/150], Step[0050/0157], Loss: 0.6884
2022-04-18 23:15:28,576 Epoch[028/150], Step[0100/0157], Loss: 0.7421
2022-04-18 23:15:38,857 Epoch[028/150], Step[0150/0157], Loss: 0.6769
2022-04-18 23:15:39,891 HashNet[28/150] bit:48, lr:0.002000000, scale:1.414, train loss:0.714
2022-04-18 23:15:45,367 Epoch[029/150], Step[0000/0157], Loss: 0.7215
2022-04-18 23:15:55,319 Epoch[029/150], Step[0050/0157], Loss: 0.7037
2022-04-18 23:16:05,563 Epoch[029/150], Step[0100/0157], Loss: 0.6678
2022-04-18 23:16:15,013 Epoch[029/150], Step[0150/0157], Loss: 0.7143
2022-04-18 23:16:16,165 HashNet[29/150] bit:48, lr:0.002000000, scale:1.414, train loss:0.709
2022-04-18 23:16:21,764 Epoch[030/150], Step[0000/0157], Loss: 0.6961
2022-04-18 23:16:32,243 Epoch[030/150], Step[0050/0157], Loss: 0.7134
2022-04-18 23:16:42,470 Epoch[030/150], Step[0100/0157], Loss: 0.6733
2022-04-18 23:16:52,127 Epoch[030/150], Step[0150/0157], Loss: 0.7120
2022-04-18 23:16:53,512 HashNet[30/150] bit:48, lr:0.002000000, scale:1.414, train loss:0.702
2022-04-18 23:16:53,512 ----- Validation after Epoch: 30
2022-04-18 23:24:29,988 save in checkpoints_new//train-48-20220418-22-42-51/bit_48
2022-04-18 23:24:32,310 Max mAP so far: 0.7074 at epoch_30
2022-04-18 23:24:32,310 ----- Save BEST model: checkpoints_new//train-48-20220418-22-42-51/bit_48/coco.pdparams
2022-04-18 23:24:32,310 ----- Save BEST optim: checkpoints_new//train-48-20220418-22-42-51/bit_48/coco.pdopt
2022-04-18 23:24:32,310 HashNet epoch:30, bit:48, dataset:coco, MAP:0.707, Best MAP(e30): 0.707
2022-04-18 23:24:37,937 Epoch[031/150], Step[0000/0157], Loss: 0.7181
2022-04-18 23:24:47,908 Epoch[031/150], Step[0050/0157], Loss: 0.6924
2022-04-18 23:24:58,167 Epoch[031/150], Step[0100/0157], Loss: 0.7877
2022-04-18 23:25:08,135 Epoch[031/150], Step[0150/0157], Loss: 0.6798
2022-04-18 23:25:09,222 HashNet[31/150] bit:48, lr:0.002000000, scale:1.414, train loss:0.701
2022-04-18 23:25:14,957 Epoch[032/150], Step[0000/0157], Loss: 0.6833
2022-04-18 23:25:24,962 Epoch[032/150], Step[0050/0157], Loss: 0.6972
2022-04-18 23:25:35,332 Epoch[032/150], Step[0100/0157], Loss: 0.6582
2022-04-18 23:25:44,806 Epoch[032/150], Step[0150/0157], Loss: 0.6662
2022-04-18 23:25:46,055 HashNet[32/150] bit:48, lr:0.002000000, scale:1.414, train loss:0.694
2022-04-18 23:25:51,805 Epoch[033/150], Step[0000/0157], Loss: 0.6483
2022-04-18 23:26:01,949 Epoch[033/150], Step[0050/0157], Loss: 0.6939
2022-04-18 23:26:11,648 Epoch[033/150], Step[0100/0157], Loss: 0.6653
2022-04-18 23:26:22,687 Epoch[033/150], Step[0150/0157], Loss: 0.7029
2022-04-18 23:26:23,755 HashNet[33/150] bit:48, lr:0.002000000, scale:1.414, train loss:0.688
2022-04-18 23:26:29,369 Epoch[034/150], Step[0000/0157], Loss: 0.6624
2022-04-18 23:26:39,597 Epoch[034/150], Step[0050/0157], Loss: 0.6438
2022-04-18 23:26:50,278 Epoch[034/150], Step[0100/0157], Loss: 0.6878
2022-04-18 23:27:00,240 Epoch[034/150], Step[0150/0157], Loss: 0.6450
2022-04-18 23:27:01,314 HashNet[34/150] bit:48, lr:0.002000000, scale:1.414, train loss:0.683
2022-04-18 23:27:07,184 Epoch[035/150], Step[0000/0157], Loss: 0.6634
2022-04-18 23:27:17,588 Epoch[035/150], Step[0050/0157], Loss: 0.6815
2022-04-18 23:27:27,448 Epoch[035/150], Step[0100/0157], Loss: 0.7548
2022-04-18 23:27:36,946 Epoch[035/150], Step[0150/0157], Loss: 0.7340
2022-04-18 23:27:38,110 HashNet[35/150] bit:48, lr:0.002000000, scale:1.414, train loss:0.682
2022-04-18 23:27:43,775 Epoch[036/150], Step[0000/0157], Loss: 0.6614
2022-04-18 23:27:53,838 Epoch[036/150], Step[0050/0157], Loss: 0.6624
2022-04-18 23:28:03,771 Epoch[036/150], Step[0100/0157], Loss: 0.6801
2022-04-18 23:28:13,225 Epoch[036/150], Step[0150/0157], Loss: 0.6455
2022-04-18 23:28:14,749 HashNet[36/150] bit:48, lr:0.002000000, scale:1.414, train loss:0.677
2022-04-18 23:28:20,422 Epoch[037/150], Step[0000/0157], Loss: 0.6880
2022-04-18 23:28:31,375 Epoch[037/150], Step[0050/0157], Loss: 0.6670
2022-04-18 23:28:42,262 Epoch[037/150], Step[0100/0157], Loss: 0.6821
2022-04-18 23:28:51,995 Epoch[037/150], Step[0150/0157], Loss: 0.7057
2022-04-18 23:28:53,131 HashNet[37/150] bit:48, lr:0.002000000, scale:1.414, train loss:0.678
2022-04-18 23:28:58,869 Epoch[038/150], Step[0000/0157], Loss: 0.6648
2022-04-18 23:29:09,257 Epoch[038/150], Step[0050/0157], Loss: 0.6840
2022-04-18 23:29:19,046 Epoch[038/150], Step[0100/0157], Loss: 0.6484
2022-04-18 23:29:29,847 Epoch[038/150], Step[0150/0157], Loss: 0.6432
2022-04-18 23:29:31,190 HashNet[38/150] bit:48, lr:0.002000000, scale:1.414, train loss:0.674
2022-04-18 23:29:36,808 Epoch[039/150], Step[0000/0157], Loss: 0.6516
2022-04-18 23:29:47,008 Epoch[039/150], Step[0050/0157], Loss: 0.6533
2022-04-18 23:29:57,019 Epoch[039/150], Step[0100/0157], Loss: 0.6951
2022-04-18 23:30:07,339 Epoch[039/150], Step[0150/0157], Loss: 0.6554
2022-04-18 23:30:08,500 HashNet[39/150] bit:48, lr:0.002000000, scale:1.414, train loss:0.672
2022-04-18 23:30:14,290 Epoch[040/150], Step[0000/0157], Loss: 0.6788
2022-04-18 23:30:24,104 Epoch[040/150], Step[0050/0157], Loss: 0.7290
2022-04-18 23:30:34,479 Epoch[040/150], Step[0100/0157], Loss: 0.6213
2022-04-18 23:30:44,078 Epoch[040/150], Step[0150/0157], Loss: 0.6642
2022-04-18 23:30:45,257 HashNet[40/150] bit:48, lr:0.002000000, scale:1.414, train loss:0.667
2022-04-18 23:30:45,257 ----- Validation after Epoch: 40
2022-04-18 23:38:26,759 save in checkpoints_new//train-48-20220418-22-42-51/bit_48
2022-04-18 23:38:29,348 Max mAP so far: 0.7091 at epoch_40
2022-04-18 23:38:29,348 ----- Save BEST model: checkpoints_new//train-48-20220418-22-42-51/bit_48/coco.pdparams
2022-04-18 23:38:29,348 ----- Save BEST optim: checkpoints_new//train-48-20220418-22-42-51/bit_48/coco.pdopt
2022-04-18 23:38:29,349 HashNet epoch:40, bit:48, dataset:coco, MAP:0.709, Best MAP(e40): 0.709
2022-04-18 23:38:35,134 Epoch[041/150], Step[0000/0157], Loss: 0.6635
2022-04-18 23:38:44,576 Epoch[041/150], Step[0050/0157], Loss: 0.7179
2022-04-18 23:38:54,431 Epoch[041/150], Step[0100/0157], Loss: 0.6465
2022-04-18 23:39:04,815 Epoch[041/150], Step[0150/0157], Loss: 0.6084
2022-04-18 23:39:05,852 HashNet[41/150] bit:48, lr:0.002000000, scale:1.732, train loss:0.665
2022-04-18 23:39:11,633 Epoch[042/150], Step[0000/0157], Loss: 0.7025
2022-04-18 23:39:22,010 Epoch[042/150], Step[0050/0157], Loss: 0.6594
2022-04-18 23:39:32,821 Epoch[042/150], Step[0100/0157], Loss: 0.6803
2022-04-18 23:39:42,860 Epoch[042/150], Step[0150/0157], Loss: 0.6820
2022-04-18 23:39:43,944 HashNet[42/150] bit:48, lr:0.002000000, scale:1.732, train loss:0.659
2022-04-18 23:39:49,532 Epoch[043/150], Step[0000/0157], Loss: 0.6601
2022-04-18 23:40:00,572 Epoch[043/150], Step[0050/0157], Loss: 0.6829
2022-04-18 23:40:10,225 Epoch[043/150], Step[0100/0157], Loss: 0.5967
2022-04-18 23:40:20,232 Epoch[043/150], Step[0150/0157], Loss: 0.6449
2022-04-18 23:40:21,325 HashNet[43/150] bit:48, lr:0.002000000, scale:1.732, train loss:0.661
2022-04-18 23:40:27,036 Epoch[044/150], Step[0000/0157], Loss: 0.6746
2022-04-18 23:40:37,437 Epoch[044/150], Step[0050/0157], Loss: 0.6747
2022-04-18 23:40:47,701 Epoch[044/150], Step[0100/0157], Loss: 0.6376
2022-04-18 23:40:57,883 Epoch[044/150], Step[0150/0157], Loss: 0.6356
2022-04-18 23:40:59,054 HashNet[44/150] bit:48, lr:0.002000000, scale:1.732, train loss:0.659
2022-04-18 23:41:04,860 Epoch[045/150], Step[0000/0157], Loss: 0.6113
2022-04-18 23:41:15,381 Epoch[045/150], Step[0050/0157], Loss: 0.6445
2022-04-18 23:41:25,300 Epoch[045/150], Step[0100/0157], Loss: 0.6620
2022-04-18 23:41:35,713 Epoch[045/150], Step[0150/0157], Loss: 0.6380
2022-04-18 23:41:36,756 HashNet[45/150] bit:48, lr:0.002000000, scale:1.732, train loss:0.657
2022-04-18 23:41:42,296 Epoch[046/150], Step[0000/0157], Loss: 0.6206
2022-04-18 23:41:51,966 Epoch[046/150], Step[0050/0157], Loss: 0.6505
2022-04-18 23:42:01,752 Epoch[046/150], Step[0100/0157], Loss: 0.6315
2022-04-18 23:42:11,893 Epoch[046/150], Step[0150/0157], Loss: 0.6740
2022-04-18 23:42:13,173 HashNet[46/150] bit:48, lr:0.002000000, scale:1.732, train loss:0.653
2022-04-18 23:42:18,712 Epoch[047/150], Step[0000/0157], Loss: 0.6428
2022-04-18 23:42:28,878 Epoch[047/150], Step[0050/0157], Loss: 0.6147
2022-04-18 23:42:38,894 Epoch[047/150], Step[0100/0157], Loss: 0.6870
2022-04-18 23:42:48,505 Epoch[047/150], Step[0150/0157], Loss: 0.6349
2022-04-18 23:42:49,668 HashNet[47/150] bit:48, lr:0.002000000, scale:1.732, train loss:0.651
2022-04-18 23:42:55,698 Epoch[048/150], Step[0000/0157], Loss: 0.6055
2022-04-18 23:43:05,331 Epoch[048/150], Step[0050/0157], Loss: 0.6888
2022-04-18 23:43:15,244 Epoch[048/150], Step[0100/0157], Loss: 0.6573
2022-04-18 23:43:24,587 Epoch[048/150], Step[0150/0157], Loss: 0.6271
2022-04-18 23:43:25,825 HashNet[48/150] bit:48, lr:0.002000000, scale:1.732, train loss:0.647
2022-04-18 23:43:31,556 Epoch[049/150], Step[0000/0157], Loss: 0.6650
2022-04-18 23:43:42,109 Epoch[049/150], Step[0050/0157], Loss: 0.6378
2022-04-18 23:43:51,945 Epoch[049/150], Step[0100/0157], Loss: 0.6562
2022-04-18 23:44:01,928 Epoch[049/150], Step[0150/0157], Loss: 0.6343
2022-04-18 23:44:03,073 HashNet[49/150] bit:48, lr:0.002000000, scale:1.732, train loss:0.650
2022-04-18 23:44:08,776 Epoch[050/150], Step[0000/0157], Loss: 0.6555
2022-04-18 23:44:18,767 Epoch[050/150], Step[0050/0157], Loss: 0.6716
2022-04-18 23:44:29,030 Epoch[050/150], Step[0100/0157], Loss: 0.6297
2022-04-18 23:44:38,614 Epoch[050/150], Step[0150/0157], Loss: 0.6606
2022-04-18 23:44:39,560 HashNet[50/150] bit:48, lr:0.002000000, scale:1.732, train loss:0.647
2022-04-18 23:44:39,561 ----- Validation after Epoch: 50
2022-04-18 23:52:23,038 HashNet epoch:50, bit:48, dataset:coco, MAP:0.704, Best MAP(e40): 0.709
2022-04-18 23:52:29,785 Epoch[051/150], Step[0000/0157], Loss: 0.6121
2022-04-18 23:52:40,081 Epoch[051/150], Step[0050/0157], Loss: 0.6532
2022-04-18 23:52:49,755 Epoch[051/150], Step[0100/0157], Loss: 0.6620
2022-04-18 23:52:59,892 Epoch[051/150], Step[0150/0157], Loss: 0.6434
2022-04-18 23:53:01,173 HashNet[51/150] bit:48, lr:0.001000000, scale:1.732, train loss:0.644
2022-04-18 23:53:06,830 Epoch[052/150], Step[0000/0157], Loss: 0.6528
2022-04-18 23:53:16,661 Epoch[052/150], Step[0050/0157], Loss: 0.6786
2022-04-18 23:53:26,572 Epoch[052/150], Step[0100/0157], Loss: 0.6411
2022-04-18 23:53:36,575 Epoch[052/150], Step[0150/0157], Loss: 0.6144
2022-04-18 23:53:37,736 HashNet[52/150] bit:48, lr:0.001000000, scale:1.732, train loss:0.633
2022-04-18 23:53:43,631 Epoch[053/150], Step[0000/0157], Loss: 0.6240
2022-04-18 23:53:54,291 Epoch[053/150], Step[0050/0157], Loss: 0.6452
2022-04-18 23:54:04,597 Epoch[053/150], Step[0100/0157], Loss: 0.6231
2022-04-18 23:54:14,838 Epoch[053/150], Step[0150/0157], Loss: 0.6690
2022-04-18 23:54:15,843 HashNet[53/150] bit:48, lr:0.001000000, scale:1.732, train loss:0.632
2022-04-18 23:54:21,552 Epoch[054/150], Step[0000/0157], Loss: 0.6860
2022-04-18 23:54:31,903 Epoch[054/150], Step[0050/0157], Loss: 0.6146
2022-04-18 23:54:42,175 Epoch[054/150], Step[0100/0157], Loss: 0.6386
2022-04-18 23:54:52,481 Epoch[054/150], Step[0150/0157], Loss: 0.6414
2022-04-18 23:54:53,488 HashNet[54/150] bit:48, lr:0.001000000, scale:1.732, train loss:0.632
2022-04-18 23:54:59,262 Epoch[055/150], Step[0000/0157], Loss: 0.6279
2022-04-18 23:55:09,389 Epoch[055/150], Step[0050/0157], Loss: 0.6959
2022-04-18 23:55:20,508 Epoch[055/150], Step[0100/0157], Loss: 0.6694
2022-04-18 23:55:30,443 Epoch[055/150], Step[0150/0157], Loss: 0.6159
2022-04-18 23:55:31,508 HashNet[55/150] bit:48, lr:0.001000000, scale:1.732, train loss:0.633
2022-04-18 23:55:37,314 Epoch[056/150], Step[0000/0157], Loss: 0.6642
2022-04-18 23:55:46,809 Epoch[056/150], Step[0050/0157], Loss: 0.6120
2022-04-18 23:55:56,640 Epoch[056/150], Step[0100/0157], Loss: 0.6229
2022-04-18 23:56:07,001 Epoch[056/150], Step[0150/0157], Loss: 0.6126
2022-04-18 23:56:08,015 HashNet[56/150] bit:48, lr:0.001000000, scale:1.732, train loss:0.630
2022-04-18 23:56:13,557 Epoch[057/150], Step[0000/0157], Loss: 0.5869
2022-04-18 23:56:23,873 Epoch[057/150], Step[0050/0157], Loss: 0.6524
2022-04-18 23:56:33,658 Epoch[057/150], Step[0100/0157], Loss: 0.6635
2022-04-18 23:56:43,592 Epoch[057/150], Step[0150/0157], Loss: 0.6895
2022-04-18 23:56:45,028 HashNet[57/150] bit:48, lr:0.001000000, scale:1.732, train loss:0.627
2022-04-18 23:56:50,744 Epoch[058/150], Step[0000/0157], Loss: 0.6051
2022-04-18 23:57:00,474 Epoch[058/150], Step[0050/0157], Loss: 0.6520
2022-04-18 23:57:11,172 Epoch[058/150], Step[0100/0157], Loss: 0.6385
2022-04-18 23:57:21,886 Epoch[058/150], Step[0150/0157], Loss: 0.6490
2022-04-18 23:57:23,193 HashNet[58/150] bit:48, lr:0.001000000, scale:1.732, train loss:0.624
2022-04-18 23:57:28,928 Epoch[059/150], Step[0000/0157], Loss: 0.6048
2022-04-18 23:57:38,881 Epoch[059/150], Step[0050/0157], Loss: 0.6591
2022-04-18 23:57:49,015 Epoch[059/150], Step[0100/0157], Loss: 0.6511
2022-04-18 23:57:58,876 Epoch[059/150], Step[0150/0157], Loss: 0.6224
2022-04-18 23:58:00,058 HashNet[59/150] bit:48, lr:0.001000000, scale:1.732, train loss:0.627
2022-04-18 23:58:05,629 Epoch[060/150], Step[0000/0157], Loss: 0.6453
2022-04-18 23:58:15,475 Epoch[060/150], Step[0050/0157], Loss: 0.6086
2022-04-18 23:58:25,349 Epoch[060/150], Step[0100/0157], Loss: 0.5941
2022-04-18 23:58:35,276 Epoch[060/150], Step[0150/0157], Loss: 0.6230
2022-04-18 23:58:36,276 HashNet[60/150] bit:48, lr:0.001000000, scale:1.732, train loss:0.625
2022-04-18 23:58:36,276 ----- Validation after Epoch: 60
2022-04-19 00:06:19,791 HashNet epoch:60, bit:48, dataset:coco, MAP:0.706, Best MAP(e40): 0.709
2022-04-19 00:06:26,606 Epoch[061/150], Step[0000/0157], Loss: 0.6031
2022-04-19 00:06:35,997 Epoch[061/150], Step[0050/0157], Loss: 0.6462
2022-04-19 00:06:46,505 Epoch[061/150], Step[0100/0157], Loss: 0.6188
2022-04-19 00:06:56,615 Epoch[061/150], Step[0150/0157], Loss: 0.6135
2022-04-19 00:06:58,107 HashNet[61/150] bit:48, lr:0.001000000, scale:2.000, train loss:0.626
2022-04-19 00:07:03,909 Epoch[062/150], Step[0000/0157], Loss: 0.6441
2022-04-19 00:07:14,142 Epoch[062/150], Step[0050/0157], Loss: 0.6479
2022-04-19 00:07:24,379 Epoch[062/150], Step[0100/0157], Loss: 0.6598
2022-04-19 00:07:34,851 Epoch[062/150], Step[0150/0157], Loss: 0.6153
2022-04-19 00:07:35,967 HashNet[62/150] bit:48, lr:0.001000000, scale:2.000, train loss:0.622
2022-04-19 00:07:41,691 Epoch[063/150], Step[0000/0157], Loss: 0.6225
2022-04-19 00:07:52,039 Epoch[063/150], Step[0050/0157], Loss: 0.6066
2022-04-19 00:08:01,849 Epoch[063/150], Step[0100/0157], Loss: 0.6204
2022-04-19 00:08:11,762 Epoch[063/150], Step[0150/0157], Loss: 0.6157
2022-04-19 00:08:13,170 HashNet[63/150] bit:48, lr:0.001000000, scale:2.000, train loss:0.622
2022-04-19 00:08:18,881 Epoch[064/150], Step[0000/0157], Loss: 0.6511
2022-04-19 00:08:28,262 Epoch[064/150], Step[0050/0157], Loss: 0.6082
2022-04-19 00:08:39,139 Epoch[064/150], Step[0100/0157], Loss: 0.6058
2022-04-19 00:08:49,025 Epoch[064/150], Step[0150/0157], Loss: 0.6396
2022-04-19 00:08:50,059 HashNet[64/150] bit:48, lr:0.001000000, scale:2.000, train loss:0.625
2022-04-19 00:08:55,913 Epoch[065/150], Step[0000/0157], Loss: 0.6201
2022-04-19 00:09:05,443 Epoch[065/150], Step[0050/0157], Loss: 0.6337
2022-04-19 00:09:15,421 Epoch[065/150], Step[0100/0157], Loss: 0.6241
2022-04-19 00:09:25,402 Epoch[065/150], Step[0150/0157], Loss: 0.6432
2022-04-19 00:09:26,467 HashNet[65/150] bit:48, lr:0.001000000, scale:2.000, train loss:0.623
2022-04-19 00:09:32,097 Epoch[066/150], Step[0000/0157], Loss: 0.6256
2022-04-19 00:09:42,205 Epoch[066/150], Step[0050/0157], Loss: 0.6368
2022-04-19 00:09:52,270 Epoch[066/150], Step[0100/0157], Loss: 0.6156
2022-04-19 00:10:02,916 Epoch[066/150], Step[0150/0157], Loss: 0.6597
2022-04-19 00:10:03,940 HashNet[66/150] bit:48, lr:0.001000000, scale:2.000, train loss:0.621
2022-04-19 00:10:09,604 Epoch[067/150], Step[0000/0157], Loss: 0.6129
2022-04-19 00:10:19,256 Epoch[067/150], Step[0050/0157], Loss: 0.6553
2022-04-19 00:10:29,740 Epoch[067/150], Step[0100/0157], Loss: 0.6385
2022-04-19 00:10:39,855 Epoch[067/150], Step[0150/0157], Loss: 0.6254
2022-04-19 00:10:40,876 HashNet[67/150] bit:48, lr:0.001000000, scale:2.000, train loss:0.620
2022-04-19 00:10:46,489 Epoch[068/150], Step[0000/0157], Loss: 0.6048
2022-04-19 00:10:56,315 Epoch[068/150], Step[0050/0157], Loss: 0.6100
2022-04-19 00:11:06,315 Epoch[068/150], Step[0100/0157], Loss: 0.6408
2022-04-19 00:11:16,059 Epoch[068/150], Step[0150/0157], Loss: 0.6599
2022-04-19 00:11:17,181 HashNet[68/150] bit:48, lr:0.001000000, scale:2.000, train loss:0.622
2022-04-19 00:11:22,949 Epoch[069/150], Step[0000/0157], Loss: 0.6582
2022-04-19 00:11:32,969 Epoch[069/150], Step[0050/0157], Loss: 0.6304
2022-04-19 00:11:42,787 Epoch[069/150], Step[0100/0157], Loss: 0.6188
2022-04-19 00:11:53,086 Epoch[069/150], Step[0150/0157], Loss: 0.5879
2022-04-19 00:11:54,144 HashNet[69/150] bit:48, lr:0.001000000, scale:2.000, train loss:0.619
2022-04-19 00:12:00,019 Epoch[070/150], Step[0000/0157], Loss: 0.6389
2022-04-19 00:12:10,210 Epoch[070/150], Step[0050/0157], Loss: 0.6023
2022-04-19 00:12:20,487 Epoch[070/150], Step[0100/0157], Loss: 0.5868
2022-04-19 00:12:31,926 Epoch[070/150], Step[0150/0157], Loss: 0.5917
2022-04-19 00:12:33,020 HashNet[70/150] bit:48, lr:0.001000000, scale:2.000, train loss:0.618
2022-04-19 00:12:33,021 ----- Validation after Epoch: 70
2022-04-19 00:20:14,747 save in checkpoints_new//train-48-20220418-22-42-51/bit_48
2022-04-19 00:20:16,824 Max mAP so far: 0.7097 at epoch_70
2022-04-19 00:20:16,824 ----- Save BEST model: checkpoints_new//train-48-20220418-22-42-51/bit_48/coco.pdparams
2022-04-19 00:20:16,824 ----- Save BEST optim: checkpoints_new//train-48-20220418-22-42-51/bit_48/coco.pdopt
2022-04-19 00:20:16,824 HashNet epoch:70, bit:48, dataset:coco, MAP:0.710, Best MAP(e70): 0.710
2022-04-19 00:20:22,414 Epoch[071/150], Step[0000/0157], Loss: 0.6118
2022-04-19 00:20:32,888 Epoch[071/150], Step[0050/0157], Loss: 0.6121
2022-04-19 00:20:42,271 Epoch[071/150], Step[0100/0157], Loss: 0.6322
2022-04-19 00:20:52,177 Epoch[071/150], Step[0150/0157], Loss: 0.5990
2022-04-19 00:20:53,227 HashNet[71/150] bit:48, lr:0.001000000, scale:2.000, train loss:0.619
2022-04-19 00:20:59,013 Epoch[072/150], Step[0000/0157], Loss: 0.6581
2022-04-19 00:21:08,669 Epoch[072/150], Step[0050/0157], Loss: 0.6151
2022-04-19 00:21:18,581 Epoch[072/150], Step[0100/0157], Loss: 0.6002
2022-04-19 00:21:28,753 Epoch[072/150], Step[0150/0157], Loss: 0.6083
2022-04-19 00:21:29,822 HashNet[72/150] bit:48, lr:0.001000000, scale:2.000, train loss:0.618
2022-04-19 00:21:35,720 Epoch[073/150], Step[0000/0157], Loss: 0.5741
2022-04-19 00:21:46,256 Epoch[073/150], Step[0050/0157], Loss: 0.6326
2022-04-19 00:21:56,379 Epoch[073/150], Step[0100/0157], Loss: 0.5802
2022-04-19 00:22:06,650 Epoch[073/150], Step[0150/0157], Loss: 0.6166
2022-04-19 00:22:07,669 HashNet[73/150] bit:48, lr:0.001000000, scale:2.000, train loss:0.621
2022-04-19 00:22:13,385 Epoch[074/150], Step[0000/0157], Loss: 0.6577
2022-04-19 00:22:23,748 Epoch[074/150], Step[0050/0157], Loss: 0.5997
2022-04-19 00:22:33,690 Epoch[074/150], Step[0100/0157], Loss: 0.6035
2022-04-19 00:22:43,429 Epoch[074/150], Step[0150/0157], Loss: 0.5845
2022-04-19 00:22:44,442 HashNet[74/150] bit:48, lr:0.001000000, scale:2.000, train loss:0.619
2022-04-19 00:22:50,329 Epoch[075/150], Step[0000/0157], Loss: 0.6091
2022-04-19 00:23:00,466 Epoch[075/150], Step[0050/0157], Loss: 0.6345
2022-04-19 00:23:10,619 Epoch[075/150], Step[0100/0157], Loss: 0.6004
2022-04-19 00:23:20,270 Epoch[075/150], Step[0150/0157], Loss: 0.6078
2022-04-19 00:23:21,284 HashNet[75/150] bit:48, lr:0.001000000, scale:2.000, train loss:0.618
2022-04-19 00:23:27,031 Epoch[076/150], Step[0000/0157], Loss: 0.6107
2022-04-19 00:23:37,082 Epoch[076/150], Step[0050/0157], Loss: 0.6062
2022-04-19 00:23:46,773 Epoch[076/150], Step[0100/0157], Loss: 0.6138
2022-04-19 00:23:56,290 Epoch[076/150], Step[0150/0157], Loss: 0.6315
2022-04-19 00:23:57,439 HashNet[76/150] bit:48, lr:0.001000000, scale:2.000, train loss:0.615
2022-04-19 00:24:03,102 Epoch[077/150], Step[0000/0157], Loss: 0.6078
2022-04-19 00:24:12,992 Epoch[077/150], Step[0050/0157], Loss: 0.6129
2022-04-19 00:24:23,174 Epoch[077/150], Step[0100/0157], Loss: 0.6284
2022-04-19 00:24:33,795 Epoch[077/150], Step[0150/0157], Loss: 0.6204
2022-04-19 00:24:34,791 HashNet[77/150] bit:48, lr:0.001000000, scale:2.000, train loss:0.614
2022-04-19 00:24:40,418 Epoch[078/150], Step[0000/0157], Loss: 0.6221
2022-04-19 00:24:50,148 Epoch[078/150], Step[0050/0157], Loss: 0.5987
2022-04-19 00:24:59,616 Epoch[078/150], Step[0100/0157], Loss: 0.6537
2022-04-19 00:25:11,262 Epoch[078/150], Step[0150/0157], Loss: 0.6558
2022-04-19 00:25:13,408 HashNet[78/150] bit:48, lr:0.001000000, scale:2.000, train loss:0.613
2022-04-19 00:25:20,184 Epoch[079/150], Step[0000/0157], Loss: 0.6545
2022-04-19 00:25:31,029 Epoch[079/150], Step[0050/0157], Loss: 0.5773
2022-04-19 00:25:42,175 Epoch[079/150], Step[0100/0157], Loss: 0.6417
2022-04-19 00:25:54,146 Epoch[079/150], Step[0150/0157], Loss: 0.6175
2022-04-19 00:25:55,201 HashNet[79/150] bit:48, lr:0.001000000, scale:2.000, train loss:0.618
2022-04-19 00:26:01,520 Epoch[080/150], Step[0000/0157], Loss: 0.6201
2022-04-19 00:26:12,503 Epoch[080/150], Step[0050/0157], Loss: 0.6303
2022-04-19 00:26:26,281 Epoch[080/150], Step[0100/0157], Loss: 0.5998
2022-04-19 00:26:41,411 Epoch[080/150], Step[0150/0157], Loss: 0.5782
2022-04-19 00:26:42,765 HashNet[80/150] bit:48, lr:0.001000000, scale:2.000, train loss:0.613
2022-04-19 00:26:42,765 ----- Validation after Epoch: 80
2022-04-19 00:34:05,247 save in checkpoints_new//train-48-20220418-22-42-51/bit_48
2022-04-19 00:34:07,571 Max mAP so far: 0.7120 at epoch_80
2022-04-19 00:34:07,572 ----- Save BEST model: checkpoints_new//train-48-20220418-22-42-51/bit_48/coco.pdparams
2022-04-19 00:34:07,572 ----- Save BEST optim: checkpoints_new//train-48-20220418-22-42-51/bit_48/coco.pdopt
2022-04-19 00:34:07,572 HashNet epoch:80, bit:48, dataset:coco, MAP:0.712, Best MAP(e80): 0.712
2022-04-19 00:34:14,243 Epoch[081/150], Step[0000/0157], Loss: 0.6178
2022-04-19 00:34:24,592 Epoch[081/150], Step[0050/0157], Loss: 0.6300
2022-04-19 00:34:34,498 Epoch[081/150], Step[0100/0157], Loss: 0.6459
2022-04-19 00:34:45,148 Epoch[081/150], Step[0150/0157], Loss: 0.6113
2022-04-19 00:34:46,312 HashNet[81/150] bit:48, lr:0.001000000, scale:2.236, train loss:0.613
2022-04-19 00:34:52,040 Epoch[082/150], Step[0000/0157], Loss: 0.6186
2022-04-19 00:35:01,459 Epoch[082/150], Step[0050/0157], Loss: 0.5949
2022-04-19 00:35:11,862 Epoch[082/150], Step[0100/0157], Loss: 0.6023
2022-04-19 00:35:22,148 Epoch[082/150], Step[0150/0157], Loss: 0.5603
2022-04-19 00:35:23,173 HashNet[82/150] bit:48, lr:0.001000000, scale:2.236, train loss:0.615
2022-04-19 00:35:28,848 Epoch[083/150], Step[0000/0157], Loss: 0.6284
2022-04-19 00:35:38,828 Epoch[083/150], Step[0050/0157], Loss: 0.5942
2022-04-19 00:35:48,962 Epoch[083/150], Step[0100/0157], Loss: 0.6337
2022-04-19 00:35:59,156 Epoch[083/150], Step[0150/0157], Loss: 0.5947
2022-04-19 00:36:00,222 HashNet[83/150] bit:48, lr:0.001000000, scale:2.236, train loss:0.613
2022-04-19 00:36:05,901 Epoch[084/150], Step[0000/0157], Loss: 0.5984
2022-04-19 00:36:16,264 Epoch[084/150], Step[0050/0157], Loss: 0.5674
2022-04-19 00:36:26,083 Epoch[084/150], Step[0100/0157], Loss: 0.5921
2022-04-19 00:36:35,891 Epoch[084/150], Step[0150/0157], Loss: 0.5915
2022-04-19 00:36:37,167 HashNet[84/150] bit:48, lr:0.001000000, scale:2.236, train loss:0.611
2022-04-19 00:36:42,834 Epoch[085/150], Step[0000/0157], Loss: 0.6186
2022-04-19 00:36:53,196 Epoch[085/150], Step[0050/0157], Loss: 0.6133
2022-04-19 00:37:03,433 Epoch[085/150], Step[0100/0157], Loss: 0.6574
2022-04-19 00:37:13,087 Epoch[085/150], Step[0150/0157], Loss: 0.6512
2022-04-19 00:37:14,216 HashNet[85/150] bit:48, lr:0.001000000, scale:2.236, train loss:0.609
2022-04-19 00:37:19,851 Epoch[086/150], Step[0000/0157], Loss: 0.5721
2022-04-19 00:37:29,797 Epoch[086/150], Step[0050/0157], Loss: 0.6362
2022-04-19 00:37:39,505 Epoch[086/150], Step[0100/0157], Loss: 0.6819
2022-04-19 00:37:49,719 Epoch[086/150], Step[0150/0157], Loss: 0.6283
2022-04-19 00:37:50,821 HashNet[86/150] bit:48, lr:0.001000000, scale:2.236, train loss:0.610
2022-04-19 00:37:57,102 Epoch[087/150], Step[0000/0157], Loss: 0.6336
2022-04-19 00:38:07,648 Epoch[087/150], Step[0050/0157], Loss: 0.6398
2022-04-19 00:38:19,271 Epoch[087/150], Step[0100/0157], Loss: 0.6059
2022-04-19 00:38:30,005 Epoch[087/150], Step[0150/0157], Loss: 0.6035
2022-04-19 00:38:31,250 HashNet[87/150] bit:48, lr:0.001000000, scale:2.236, train loss:0.608
2022-04-19 00:38:37,679 Epoch[088/150], Step[0000/0157], Loss: 0.6385
2022-04-19 00:38:49,049 Epoch[088/150], Step[0050/0157], Loss: 0.5626
2022-04-19 00:39:03,234 Epoch[088/150], Step[0100/0157], Loss: 0.6007
2022-04-19 00:39:16,692 Epoch[088/150], Step[0150/0157], Loss: 0.6245
2022-04-19 00:39:18,188 HashNet[88/150] bit:48, lr:0.001000000, scale:2.236, train loss:0.610
2022-04-19 00:39:25,801 Epoch[089/150], Step[0000/0157], Loss: 0.6180
2022-04-19 00:39:38,702 Epoch[089/150], Step[0050/0157], Loss: 0.6649
2022-04-19 00:39:53,033 Epoch[089/150], Step[0100/0157], Loss: 0.6342
2022-04-19 00:40:05,196 Epoch[089/150], Step[0150/0157], Loss: 0.5935
2022-04-19 00:40:06,394 HashNet[89/150] bit:48, lr:0.001000000, scale:2.236, train loss:0.610
2022-04-19 00:40:12,770 Epoch[090/150], Step[0000/0157], Loss: 0.5800
2022-04-19 00:40:23,792 Epoch[090/150], Step[0050/0157], Loss: 0.5955
2022-04-19 00:40:35,061 Epoch[090/150], Step[0100/0157], Loss: 0.5743
2022-04-19 00:40:45,731 Epoch[090/150], Step[0150/0157], Loss: 0.6349
2022-04-19 00:40:46,773 HashNet[90/150] bit:48, lr:0.001000000, scale:2.236, train loss:0.610
2022-04-19 00:40:46,773 ----- Validation after Epoch: 90
2022-04-19 00:47:59,839 save in checkpoints_new//train-48-20220418-22-42-51/bit_48
2022-04-19 00:48:02,069 Max mAP so far: 0.7139 at epoch_90
2022-04-19 00:48:02,070 ----- Save BEST model: checkpoints_new//train-48-20220418-22-42-51/bit_48/coco.pdparams
2022-04-19 00:48:02,070 ----- Save BEST optim: checkpoints_new//train-48-20220418-22-42-51/bit_48/coco.pdopt
2022-04-19 00:48:02,070 HashNet epoch:90, bit:48, dataset:coco, MAP:0.714, Best MAP(e90): 0.714
2022-04-19 00:48:07,883 Epoch[091/150], Step[0000/0157], Loss: 0.6728
2022-04-19 00:48:17,630 Epoch[091/150], Step[0050/0157], Loss: 0.6291
2022-04-19 00:48:27,374 Epoch[091/150], Step[0100/0157], Loss: 0.6404
2022-04-19 00:48:37,228 Epoch[091/150], Step[0150/0157], Loss: 0.6428
2022-04-19 00:48:38,280 HashNet[91/150] bit:48, lr:0.001000000, scale:2.236, train loss:0.612
2022-04-19 00:48:44,086 Epoch[092/150], Step[0000/0157], Loss: 0.5755
2022-04-19 00:48:53,718 Epoch[092/150], Step[0050/0157], Loss: 0.6127
2022-04-19 00:49:04,223 Epoch[092/150], Step[0100/0157], Loss: 0.5992
2022-04-19 00:49:14,308 Epoch[092/150], Step[0150/0157], Loss: 0.6371
2022-04-19 00:49:15,315 HashNet[92/150] bit:48, lr:0.001000000, scale:2.236, train loss:0.609
2022-04-19 00:49:21,211 Epoch[093/150], Step[0000/0157], Loss: 0.6007
2022-04-19 00:49:31,101 Epoch[093/150], Step[0050/0157], Loss: 0.6187
2022-04-19 00:49:41,292 Epoch[093/150], Step[0100/0157], Loss: 0.6076
2022-04-19 00:49:51,685 Epoch[093/150], Step[0150/0157], Loss: 0.5989
2022-04-19 00:49:52,937 HashNet[93/150] bit:48, lr:0.001000000, scale:2.236, train loss:0.608
2022-04-19 00:49:58,779 Epoch[094/150], Step[0000/0157], Loss: 0.6356
2022-04-19 00:50:08,728 Epoch[094/150], Step[0050/0157], Loss: 0.6197
2022-04-19 00:50:18,660 Epoch[094/150], Step[0100/0157], Loss: 0.5550
2022-04-19 00:50:29,113 Epoch[094/150], Step[0150/0157], Loss: 0.5981
2022-04-19 00:50:30,414 HashNet[94/150] bit:48, lr:0.001000000, scale:2.236, train loss:0.609
2022-04-19 00:50:36,233 Epoch[095/150], Step[0000/0157], Loss: 0.6103
2022-04-19 00:50:45,980 Epoch[095/150], Step[0050/0157], Loss: 0.6089
2022-04-19 00:50:56,385 Epoch[095/150], Step[0100/0157], Loss: 0.6094
2022-04-19 00:51:07,366 Epoch[095/150], Step[0150/0157], Loss: 0.6150
2022-04-19 00:51:08,550 HashNet[95/150] bit:48, lr:0.001000000, scale:2.236, train loss:0.609
2022-04-19 00:51:14,849 Epoch[096/150], Step[0000/0157], Loss: 0.5873
2022-04-19 00:51:25,494 Epoch[096/150], Step[0050/0157], Loss: 0.5544
2022-04-19 00:51:38,142 Epoch[096/150], Step[0100/0157], Loss: 0.6103
2022-04-19 00:51:51,923 Epoch[096/150], Step[0150/0157], Loss: 0.5988
2022-04-19 00:51:53,792 HashNet[96/150] bit:48, lr:0.001000000, scale:2.236, train loss:0.609
2022-04-19 00:52:01,029 Epoch[097/150], Step[0000/0157], Loss: 0.5747
2022-04-19 00:52:14,375 Epoch[097/150], Step[0050/0157], Loss: 0.5900
2022-04-19 00:52:28,097 Epoch[097/150], Step[0100/0157], Loss: 0.6016
2022-04-19 00:52:42,150 Epoch[097/150], Step[0150/0157], Loss: 0.5667
2022-04-19 00:52:43,909 HashNet[97/150] bit:48, lr:0.001000000, scale:2.236, train loss:0.609
2022-04-19 00:52:50,826 Epoch[098/150], Step[0000/0157], Loss: 0.5924
2022-04-19 00:53:04,470 Epoch[098/150], Step[0050/0157], Loss: 0.6185
2022-04-19 00:53:15,806 Epoch[098/150], Step[0100/0157], Loss: 0.6291
2022-04-19 00:53:26,966 Epoch[098/150], Step[0150/0157], Loss: 0.5572
2022-04-19 00:53:28,201 HashNet[98/150] bit:48, lr:0.001000000, scale:2.236, train loss:0.606
2022-04-19 00:53:34,620 Epoch[099/150], Step[0000/0157], Loss: 0.5971
2022-04-19 00:53:46,023 Epoch[099/150], Step[0050/0157], Loss: 0.5726
2022-04-19 00:53:55,918 Epoch[099/150], Step[0100/0157], Loss: 0.5924
2022-04-19 00:54:06,117 Epoch[099/150], Step[0150/0157], Loss: 0.5929
2022-04-19 00:54:07,252 HashNet[99/150] bit:48, lr:0.001000000, scale:2.236, train loss:0.606
2022-04-19 00:54:12,901 Epoch[100/150], Step[0000/0157], Loss: 0.6139
2022-04-19 00:54:23,230 Epoch[100/150], Step[0050/0157], Loss: 0.5626
2022-04-19 00:54:33,242 Epoch[100/150], Step[0100/0157], Loss: 0.5848
2022-04-19 00:54:42,813 Epoch[100/150], Step[0150/0157], Loss: 0.6303
2022-04-19 00:54:44,175 HashNet[100/150] bit:48, lr:0.001000000, scale:2.236, train loss:0.607
2022-04-19 00:54:44,175 ----- Validation after Epoch: 100
2022-04-19 01:01:58,837 save in checkpoints_new//train-48-20220418-22-42-51/bit_48
2022-04-19 01:02:01,087 Max mAP so far: 0.7145 at epoch_100
2022-04-19 01:02:01,087 ----- Save BEST model: checkpoints_new//train-48-20220418-22-42-51/bit_48/coco.pdparams
2022-04-19 01:02:01,088 ----- Save BEST optim: checkpoints_new//train-48-20220418-22-42-51/bit_48/coco.pdopt
2022-04-19 01:02:01,088 HashNet epoch:100, bit:48, dataset:coco, MAP:0.714, Best MAP(e100): 0.714
2022-04-19 01:02:07,701 Epoch[101/150], Step[0000/0157], Loss: 0.5859
2022-04-19 01:02:17,456 Epoch[101/150], Step[0050/0157], Loss: 0.5874
2022-04-19 01:02:27,348 Epoch[101/150], Step[0100/0157], Loss: 0.6215
2022-04-19 01:02:37,616 Epoch[101/150], Step[0150/0157], Loss: 0.6192
2022-04-19 01:02:38,906 HashNet[101/150] bit:48, lr:0.000500000, scale:2.449, train loss:0.605
2022-04-19 01:02:44,837 Epoch[102/150], Step[0000/0157], Loss: 0.6001
2022-04-19 01:02:54,626 Epoch[102/150], Step[0050/0157], Loss: 0.5852
2022-04-19 01:03:05,227 Epoch[102/150], Step[0100/0157], Loss: 0.5891
2022-04-19 01:03:14,995 Epoch[102/150], Step[0150/0157], Loss: 0.6030
2022-04-19 01:03:16,184 HashNet[102/150] bit:48, lr:0.000500000, scale:2.449, train loss:0.604
2022-04-19 01:03:21,918 Epoch[103/150], Step[0000/0157], Loss: 0.6284
2022-04-19 01:03:31,673 Epoch[103/150], Step[0050/0157], Loss: 0.5691
2022-04-19 01:03:41,787 Epoch[103/150], Step[0100/0157], Loss: 0.5809
2022-04-19 01:03:51,705 Epoch[103/150], Step[0150/0157], Loss: 0.6145
2022-04-19 01:03:52,765 HashNet[103/150] bit:48, lr:0.000500000, scale:2.449, train loss:0.605
2022-04-19 01:03:58,789 Epoch[104/150], Step[0000/0157], Loss: 0.5961
2022-04-19 01:04:09,688 Epoch[104/150], Step[0050/0157], Loss: 0.6126
2022-04-19 01:04:20,947 Epoch[104/150], Step[0100/0157], Loss: 0.5772
2022-04-19 01:04:32,939 Epoch[104/150], Step[0150/0157], Loss: 0.6000
2022-04-19 01:04:34,769 HashNet[104/150] bit:48, lr:0.000500000, scale:2.449, train loss:0.603
2022-04-19 01:04:42,483 Epoch[105/150], Step[0000/0157], Loss: 0.5931
2022-04-19 01:04:56,454 Epoch[105/150], Step[0050/0157], Loss: 0.6280
2022-04-19 01:05:10,020 Epoch[105/150], Step[0100/0157], Loss: 0.6091
2022-04-19 01:05:24,267 Epoch[105/150], Step[0150/0157], Loss: 0.6177
2022-04-19 01:05:25,814 HashNet[105/150] bit:48, lr:0.000500000, scale:2.449, train loss:0.600
2022-04-19 01:05:34,101 Epoch[106/150], Step[0000/0157], Loss: 0.6060
2022-04-19 01:05:47,784 Epoch[106/150], Step[0050/0157], Loss: 0.5873
2022-04-19 01:06:01,139 Epoch[106/150], Step[0100/0157], Loss: 0.6234
2022-04-19 01:06:13,489 Epoch[106/150], Step[0150/0157], Loss: 0.5766
2022-04-19 01:06:15,377 HashNet[106/150] bit:48, lr:0.000500000, scale:2.449, train loss:0.601
2022-04-19 01:06:21,767 Epoch[107/150], Step[0000/0157], Loss: 0.6379
2022-04-19 01:06:32,593 Epoch[107/150], Step[0050/0157], Loss: 0.5730
2022-04-19 01:06:43,192 Epoch[107/150], Step[0100/0157], Loss: 0.6153
2022-04-19 01:06:52,829 Epoch[107/150], Step[0150/0157], Loss: 0.6032
2022-04-19 01:06:53,971 HashNet[107/150] bit:48, lr:0.000500000, scale:2.449, train loss:0.599
2022-04-19 01:06:59,680 Epoch[108/150], Step[0000/0157], Loss: 0.5735
2022-04-19 01:07:09,839 Epoch[108/150], Step[0050/0157], Loss: 0.5780
2022-04-19 01:07:20,321 Epoch[108/150], Step[0100/0157], Loss: 0.6113
2022-04-19 01:07:31,076 Epoch[108/150], Step[0150/0157], Loss: 0.5894
2022-04-19 01:07:32,088 HashNet[108/150] bit:48, lr:0.000500000, scale:2.449, train loss:0.599
2022-04-19 01:07:37,928 Epoch[109/150], Step[0000/0157], Loss: 0.5970
2022-04-19 01:07:48,088 Epoch[109/150], Step[0050/0157], Loss: 0.5735
2022-04-19 01:07:58,472 Epoch[109/150], Step[0100/0157], Loss: 0.5892
2022-04-19 01:08:09,297 Epoch[109/150], Step[0150/0157], Loss: 0.5803
2022-04-19 01:08:10,475 HashNet[109/150] bit:48, lr:0.000500000, scale:2.449, train loss:0.600
2022-04-19 01:08:16,229 Epoch[110/150], Step[0000/0157], Loss: 0.5868
2022-04-19 01:08:25,825 Epoch[110/150], Step[0050/0157], Loss: 0.6008
2022-04-19 01:08:35,647 Epoch[110/150], Step[0100/0157], Loss: 0.6279
2022-04-19 01:08:46,187 Epoch[110/150], Step[0150/0157], Loss: 0.6182
2022-04-19 01:08:47,588 HashNet[110/150] bit:48, lr:0.000500000, scale:2.449, train loss:0.601
2022-04-19 01:08:47,589 ----- Validation after Epoch: 110
2022-04-19 01:15:59,147 save in checkpoints_new//train-48-20220418-22-42-51/bit_48
2022-04-19 01:16:01,605 Max mAP so far: 0.7145 at epoch_110
2022-04-19 01:16:01,605 ----- Save BEST model: checkpoints_new//train-48-20220418-22-42-51/bit_48/coco.pdparams
2022-04-19 01:16:01,605 ----- Save BEST optim: checkpoints_new//train-48-20220418-22-42-51/bit_48/coco.pdopt
2022-04-19 01:16:01,605 HashNet epoch:110, bit:48, dataset:coco, MAP:0.715, Best MAP(e110): 0.715
2022-04-19 01:16:07,148 Epoch[111/150], Step[0000/0157], Loss: 0.6016
2022-04-19 01:16:16,802 Epoch[111/150], Step[0050/0157], Loss: 0.5568
2022-04-19 01:16:26,606 Epoch[111/150], Step[0100/0157], Loss: 0.6111
2022-04-19 01:16:36,215 Epoch[111/150], Step[0150/0157], Loss: 0.5712
2022-04-19 01:16:37,558 HashNet[111/150] bit:48, lr:0.000500000, scale:2.449, train loss:0.602
2022-04-19 01:16:43,505 Epoch[112/150], Step[0000/0157], Loss: 0.5972
2022-04-19 01:16:53,633 Epoch[112/150], Step[0050/0157], Loss: 0.6246
2022-04-19 01:17:04,615 Epoch[112/150], Step[0100/0157], Loss: 0.5651
2022-04-19 01:17:16,737 Epoch[112/150], Step[0150/0157], Loss: 0.6441
2022-04-19 01:17:18,668 HashNet[112/150] bit:48, lr:0.000500000, scale:2.449, train loss:0.599
2022-04-19 01:17:26,875 Epoch[113/150], Step[0000/0157], Loss: 0.6293
2022-04-19 01:17:40,342 Epoch[113/150], Step[0050/0157], Loss: 0.6006
2022-04-19 01:17:52,760 Epoch[113/150], Step[0100/0157], Loss: 0.5774
2022-04-19 01:18:07,432 Epoch[113/150], Step[0150/0157], Loss: 0.5747
2022-04-19 01:18:08,696 HashNet[113/150] bit:48, lr:0.000500000, scale:2.449, train loss:0.599
2022-04-19 01:18:15,967 Epoch[114/150], Step[0000/0157], Loss: 0.5904
2022-04-19 01:18:29,850 Epoch[114/150], Step[0050/0157], Loss: 0.6334
2022-04-19 01:18:44,087 Epoch[114/150], Step[0100/0157], Loss: 0.5688
2022-04-19 01:18:57,151 Epoch[114/150], Step[0150/0157], Loss: 0.5730
2022-04-19 01:18:59,090 HashNet[114/150] bit:48, lr:0.000500000, scale:2.449, train loss:0.600
2022-04-19 01:19:07,364 Epoch[115/150], Step[0000/0157], Loss: 0.6186
2022-04-19 01:19:19,603 Epoch[115/150], Step[0050/0157], Loss: 0.6191
2022-04-19 01:19:29,431 Epoch[115/150], Step[0100/0157], Loss: 0.5683
2022-04-19 01:19:39,109 Epoch[115/150], Step[0150/0157], Loss: 0.5785
2022-04-19 01:19:40,516 HashNet[115/150] bit:48, lr:0.000500000, scale:2.449, train loss:0.600
2022-04-19 01:19:46,041 Epoch[116/150], Step[0000/0157], Loss: 0.6552
2022-04-19 01:19:55,954 Epoch[116/150], Step[0050/0157], Loss: 0.5748
2022-04-19 01:20:06,858 Epoch[116/150], Step[0100/0157], Loss: 0.5699
2022-04-19 01:20:16,267 Epoch[116/150], Step[0150/0157], Loss: 0.6221
2022-04-19 01:20:17,434 HashNet[116/150] bit:48, lr:0.000500000, scale:2.449, train loss:0.600
2022-04-19 01:20:23,194 Epoch[117/150], Step[0000/0157], Loss: 0.5792
2022-04-19 01:20:33,528 Epoch[117/150], Step[0050/0157], Loss: 0.6143
2022-04-19 01:20:43,071 Epoch[117/150], Step[0100/0157], Loss: 0.6118
2022-04-19 01:20:53,802 Epoch[117/150], Step[0150/0157], Loss: 0.5939
2022-04-19 01:20:55,216 HashNet[117/150] bit:48, lr:0.000500000, scale:2.449, train loss:0.601
2022-04-19 01:21:00,946 Epoch[118/150], Step[0000/0157], Loss: 0.5788
2022-04-19 01:21:10,888 Epoch[118/150], Step[0050/0157], Loss: 0.6382
2022-04-19 01:21:21,385 Epoch[118/150], Step[0100/0157], Loss: 0.6128
2022-04-19 01:21:31,016 Epoch[118/150], Step[0150/0157], Loss: 0.5953
2022-04-19 01:21:32,149 HashNet[118/150] bit:48, lr:0.000500000, scale:2.449, train loss:0.599
2022-04-19 01:21:37,751 Epoch[119/150], Step[0000/0157], Loss: 0.6219
2022-04-19 01:21:48,356 Epoch[119/150], Step[0050/0157], Loss: 0.5752
2022-04-19 01:21:58,107 Epoch[119/150], Step[0100/0157], Loss: 0.6411
2022-04-19 01:22:07,763 Epoch[119/150], Step[0150/0157], Loss: 0.6301
2022-04-19 01:22:08,766 HashNet[119/150] bit:48, lr:0.000500000, scale:2.449, train loss:0.600
2022-04-19 01:22:14,469 Epoch[120/150], Step[0000/0157], Loss: 0.6038
2022-04-19 01:22:24,359 Epoch[120/150], Step[0050/0157], Loss: 0.6268
2022-04-19 01:22:34,712 Epoch[120/150], Step[0100/0157], Loss: 0.5766
2022-04-19 01:22:44,861 Epoch[120/150], Step[0150/0157], Loss: 0.5721
2022-04-19 01:22:46,211 HashNet[120/150] bit:48, lr:0.000500000, scale:2.449, train loss:0.598
2022-04-19 01:22:46,211 ----- Validation after Epoch: 120
2022-04-19 01:30:03,063 HashNet epoch:120, bit:48, dataset:coco, MAP:0.714, Best MAP(e110): 0.715
2022-04-19 01:30:11,360 Epoch[121/150], Step[0000/0157], Loss: 0.5615
2022-04-19 01:30:24,028 Epoch[121/150], Step[0050/0157], Loss: 0.6023
2022-04-19 01:30:38,403 Epoch[121/150], Step[0100/0157], Loss: 0.6064
2022-04-19 01:30:51,296 Epoch[121/150], Step[0150/0157], Loss: 0.6112
2022-04-19 01:30:52,689 HashNet[121/150] bit:48, lr:0.000500000, scale:2.646, train loss:0.600
2022-04-19 01:31:01,304 Epoch[122/150], Step[0000/0157], Loss: 0.5578
2022-04-19 01:31:14,785 Epoch[122/150], Step[0050/0157], Loss: 0.6222
2022-04-19 01:31:29,824 Epoch[122/150], Step[0100/0157], Loss: 0.5780
2022-04-19 01:31:44,660 Epoch[122/150], Step[0150/0157], Loss: 0.5427
2022-04-19 01:31:46,266 HashNet[122/150] bit:48, lr:0.000500000, scale:2.646, train loss:0.599
2022-04-19 01:31:53,887 Epoch[123/150], Step[0000/0157], Loss: 0.5884
2022-04-19 01:32:07,861 Epoch[123/150], Step[0050/0157], Loss: 0.5906
2022-04-19 01:32:20,298 Epoch[123/150], Step[0100/0157], Loss: 0.6142
2022-04-19 01:32:30,516 Epoch[123/150], Step[0150/0157], Loss: 0.5982
2022-04-19 01:32:31,681 HashNet[123/150] bit:48, lr:0.000500000, scale:2.646, train loss:0.597
2022-04-19 01:32:37,436 Epoch[124/150], Step[0000/0157], Loss: 0.6207
2022-04-19 01:32:47,128 Epoch[124/150], Step[0050/0157], Loss: 0.6044
2022-04-19 01:32:57,505 Epoch[124/150], Step[0100/0157], Loss: 0.6209
2022-04-19 01:33:07,851 Epoch[124/150], Step[0150/0157], Loss: 0.5920
2022-04-19 01:33:08,916 HashNet[124/150] bit:48, lr:0.000500000, scale:2.646, train loss:0.596
2022-04-19 01:33:14,404 Epoch[125/150], Step[0000/0157], Loss: 0.6246
2022-04-19 01:33:24,418 Epoch[125/150], Step[0050/0157], Loss: 0.6065
2022-04-19 01:33:35,029 Epoch[125/150], Step[0100/0157], Loss: 0.6090
2022-04-19 01:33:44,437 Epoch[125/150], Step[0150/0157], Loss: 0.6047
2022-04-19 01:33:45,708 HashNet[125/150] bit:48, lr:0.000500000, scale:2.646, train loss:0.597
2022-04-19 01:33:51,503 Epoch[126/150], Step[0000/0157], Loss: 0.6014
2022-04-19 01:34:01,690 Epoch[126/150], Step[0050/0157], Loss: 0.5632
2022-04-19 01:34:11,619 Epoch[126/150], Step[0100/0157], Loss: 0.5688
2022-04-19 01:34:22,134 Epoch[126/150], Step[0150/0157], Loss: 0.5807
2022-04-19 01:34:23,238 HashNet[126/150] bit:48, lr:0.000500000, scale:2.646, train loss:0.598
2022-04-19 01:34:29,133 Epoch[127/150], Step[0000/0157], Loss: 0.5694
2022-04-19 01:34:39,083 Epoch[127/150], Step[0050/0157], Loss: 0.6092
2022-04-19 01:34:49,659 Epoch[127/150], Step[0100/0157], Loss: 0.5401
2022-04-19 01:34:59,778 Epoch[127/150], Step[0150/0157], Loss: 0.5735
2022-04-19 01:35:00,809 HashNet[127/150] bit:48, lr:0.000500000, scale:2.646, train loss:0.596
2022-04-19 01:35:06,503 Epoch[128/150], Step[0000/0157], Loss: 0.5939
2022-04-19 01:35:16,740 Epoch[128/150], Step[0050/0157], Loss: 0.6750
2022-04-19 01:35:27,049 Epoch[128/150], Step[0100/0157], Loss: 0.6287
2022-04-19 01:35:37,096 Epoch[128/150], Step[0150/0157], Loss: 0.5658
2022-04-19 01:35:38,149 HashNet[128/150] bit:48, lr:0.000500000, scale:2.646, train loss:0.598
2022-04-19 01:35:44,184 Epoch[129/150], Step[0000/0157], Loss: 0.5757
2022-04-19 01:35:54,356 Epoch[129/150], Step[0050/0157], Loss: 0.5899
2022-04-19 01:36:04,438 Epoch[129/150], Step[0100/0157], Loss: 0.6014
2022-04-19 01:36:14,357 Epoch[129/150], Step[0150/0157], Loss: 0.6187
2022-04-19 01:36:15,480 HashNet[129/150] bit:48, lr:0.000500000, scale:2.646, train loss:0.596
2022-04-19 01:36:21,315 Epoch[130/150], Step[0000/0157], Loss: 0.6108
2022-04-19 01:36:31,397 Epoch[130/150], Step[0050/0157], Loss: 0.5699
2022-04-19 01:36:41,931 Epoch[130/150], Step[0100/0157], Loss: 0.5775
2022-04-19 01:36:51,723 Epoch[130/150], Step[0150/0157], Loss: 0.5874
2022-04-19 01:36:52,950 HashNet[130/150] bit:48, lr:0.000500000, scale:2.646, train loss:0.596
2022-04-19 01:36:52,950 ----- Validation after Epoch: 130
2022-04-19 01:44:16,723 HashNet epoch:130, bit:48, dataset:coco, MAP:0.713, Best MAP(e110): 0.715
2022-04-19 01:44:26,267 Epoch[131/150], Step[0000/0157], Loss: 0.6279
2022-04-19 01:44:39,724 Epoch[131/150], Step[0050/0157], Loss: 0.5933
2022-04-19 01:44:52,422 Epoch[131/150], Step[0100/0157], Loss: 0.5682
2022-04-19 01:45:04,796 Epoch[131/150], Step[0150/0157], Loss: 0.5626
2022-04-19 01:45:06,285 HashNet[131/150] bit:48, lr:0.000500000, scale:2.646, train loss:0.596
2022-04-19 01:45:13,100 Epoch[132/150], Step[0000/0157], Loss: 0.5772
2022-04-19 01:45:24,057 Epoch[132/150], Step[0050/0157], Loss: 0.6118
2022-04-19 01:45:34,833 Epoch[132/150], Step[0100/0157], Loss: 0.6162
2022-04-19 01:45:44,941 Epoch[132/150], Step[0150/0157], Loss: 0.5918
2022-04-19 01:45:46,042 HashNet[132/150] bit:48, lr:0.000500000, scale:2.646, train loss:0.595
2022-04-19 01:45:51,765 Epoch[133/150], Step[0000/0157], Loss: 0.5915
2022-04-19 01:46:01,383 Epoch[133/150], Step[0050/0157], Loss: 0.6019
2022-04-19 01:46:12,241 Epoch[133/150], Step[0100/0157], Loss: 0.6244
2022-04-19 01:46:21,779 Epoch[133/150], Step[0150/0157], Loss: 0.6187
2022-04-19 01:46:22,963 HashNet[133/150] bit:48, lr:0.000500000, scale:2.646, train loss:0.597
2022-04-19 01:46:28,693 Epoch[134/150], Step[0000/0157], Loss: 0.5939
2022-04-19 01:46:39,928 Epoch[134/150], Step[0050/0157], Loss: 0.6395
2022-04-19 01:46:50,559 Epoch[134/150], Step[0100/0157], Loss: 0.5544
2022-04-19 01:47:01,170 Epoch[134/150], Step[0150/0157], Loss: 0.6204
2022-04-19 01:47:02,227 HashNet[134/150] bit:48, lr:0.000500000, scale:2.646, train loss:0.597
2022-04-19 01:47:08,296 Epoch[135/150], Step[0000/0157], Loss: 0.5832
2022-04-19 01:47:18,234 Epoch[135/150], Step[0050/0157], Loss: 0.5881
2022-04-19 01:47:27,747 Epoch[135/150], Step[0100/0157], Loss: 0.6152
2022-04-19 01:47:38,162 Epoch[135/150], Step[0150/0157], Loss: 0.5756
2022-04-19 01:47:39,522 HashNet[135/150] bit:48, lr:0.000500000, scale:2.646, train loss:0.598
2022-04-19 01:47:45,198 Epoch[136/150], Step[0000/0157], Loss: 0.5997
2022-04-19 01:47:55,704 Epoch[136/150], Step[0050/0157], Loss: 0.5881
2022-04-19 01:48:05,642 Epoch[136/150], Step[0100/0157], Loss: 0.6058
2022-04-19 01:48:15,673 Epoch[136/150], Step[0150/0157], Loss: 0.6248
2022-04-19 01:48:16,998 HashNet[136/150] bit:48, lr:0.000500000, scale:2.646, train loss:0.596
2022-04-19 01:48:22,840 Epoch[137/150], Step[0000/0157], Loss: 0.5820
2022-04-19 01:48:32,716 Epoch[137/150], Step[0050/0157], Loss: 0.6318
2022-04-19 01:48:42,506 Epoch[137/150], Step[0100/0157], Loss: 0.6068
2022-04-19 01:48:52,434 Epoch[137/150], Step[0150/0157], Loss: 0.5891
2022-04-19 01:48:53,929 HashNet[137/150] bit:48, lr:0.000500000, scale:2.646, train loss:0.597
2022-04-19 01:48:59,602 Epoch[138/150], Step[0000/0157], Loss: 0.5681
2022-04-19 01:49:10,046 Epoch[138/150], Step[0050/0157], Loss: 0.5487
2022-04-19 01:49:20,165 Epoch[138/150], Step[0100/0157], Loss: 0.5908
2022-04-19 01:49:29,900 Epoch[138/150], Step[0150/0157], Loss: 0.6333
2022-04-19 01:49:30,970 HashNet[138/150] bit:48, lr:0.000500000, scale:2.646, train loss:0.596
2022-04-19 01:49:36,873 Epoch[139/150], Step[0000/0157], Loss: 0.6285
2022-04-19 01:49:47,111 Epoch[139/150], Step[0050/0157], Loss: 0.5846
2022-04-19 01:49:57,096 Epoch[139/150], Step[0100/0157], Loss: 0.6157
2022-04-19 01:50:08,038 Epoch[139/150], Step[0150/0157], Loss: 0.5933
2022-04-19 01:50:09,140 HashNet[139/150] bit:48, lr:0.000500000, scale:2.646, train loss:0.598
2022-04-19 01:50:14,780 Epoch[140/150], Step[0000/0157], Loss: 0.6049
2022-04-19 01:50:24,968 Epoch[140/150], Step[0050/0157], Loss: 0.6334
2022-04-19 01:50:35,285 Epoch[140/150], Step[0100/0157], Loss: 0.6289
2022-04-19 01:50:45,364 Epoch[140/150], Step[0150/0157], Loss: 0.6352
2022-04-19 01:50:46,557 HashNet[140/150] bit:48, lr:0.000500000, scale:2.646, train loss:0.597
2022-04-19 01:50:46,557 ----- Validation after Epoch: 140
2022-04-19 01:58:05,815 HashNet epoch:140, bit:48, dataset:coco, MAP:0.714, Best MAP(e110): 0.715
2022-04-19 01:58:26,948 Epoch[141/150], Step[0000/0157], Loss: 0.6036
2022-04-19 01:58:37,717 Epoch[141/150], Step[0050/0157], Loss: 0.5969
2022-04-19 01:58:48,636 Epoch[141/150], Step[0100/0157], Loss: 0.6153
2022-04-19 01:58:58,693 Epoch[141/150], Step[0150/0157], Loss: 0.6052
2022-04-19 01:58:59,773 HashNet[141/150] bit:48, lr:0.000500000, scale:2.828, train loss:0.595
2022-04-19 01:59:05,532 Epoch[142/150], Step[0000/0157], Loss: 0.5989
2022-04-19 01:59:15,714 Epoch[142/150], Step[0050/0157], Loss: 0.5707
2022-04-19 01:59:26,128 Epoch[142/150], Step[0100/0157], Loss: 0.6394
2022-04-19 01:59:36,273 Epoch[142/150], Step[0150/0157], Loss: 0.5777
2022-04-19 01:59:37,364 HashNet[142/150] bit:48, lr:0.000500000, scale:2.828, train loss:0.595
2022-04-19 01:59:43,219 Epoch[143/150], Step[0000/0157], Loss: 0.5927
2022-04-19 01:59:53,210 Epoch[143/150], Step[0050/0157], Loss: 0.5965
2022-04-19 02:00:02,481 Epoch[143/150], Step[0100/0157], Loss: 0.6378
2022-04-19 02:00:13,013 Epoch[143/150], Step[0150/0157], Loss: 0.5862
2022-04-19 02:00:14,156 HashNet[143/150] bit:48, lr:0.000500000, scale:2.828, train loss:0.594
2022-04-19 02:00:19,989 Epoch[144/150], Step[0000/0157], Loss: 0.6587
2022-04-19 02:00:30,017 Epoch[144/150], Step[0050/0157], Loss: 0.5960
2022-04-19 02:00:40,940 Epoch[144/150], Step[0100/0157], Loss: 0.5817
2022-04-19 02:00:50,838 Epoch[144/150], Step[0150/0157], Loss: 0.6078
2022-04-19 02:00:52,107 HashNet[144/150] bit:48, lr:0.000500000, scale:2.828, train loss:0.595
2022-04-19 02:00:57,800 Epoch[145/150], Step[0000/0157], Loss: 0.5746
2022-04-19 02:01:08,284 Epoch[145/150], Step[0050/0157], Loss: 0.5720
2022-04-19 02:01:18,714 Epoch[145/150], Step[0100/0157], Loss: 0.5365
2022-04-19 02:01:28,545 Epoch[145/150], Step[0150/0157], Loss: 0.5981
2022-04-19 02:01:29,617 HashNet[145/150] bit:48, lr:0.000500000, scale:2.828, train loss:0.595
2022-04-19 02:01:35,321 Epoch[146/150], Step[0000/0157], Loss: 0.6155
2022-04-19 02:01:45,578 Epoch[146/150], Step[0050/0157], Loss: 0.5668
2022-04-19 02:01:55,404 Epoch[146/150], Step[0100/0157], Loss: 0.6024
2022-04-19 02:02:06,275 Epoch[146/150], Step[0150/0157], Loss: 0.6029
2022-04-19 02:02:07,312 HashNet[146/150] bit:48, lr:0.000500000, scale:2.828, train loss:0.594
2022-04-19 02:02:12,966 Epoch[147/150], Step[0000/0157], Loss: 0.6162
2022-04-19 02:02:22,330 Epoch[147/150], Step[0050/0157], Loss: 0.5627
2022-04-19 02:02:33,277 Epoch[147/150], Step[0100/0157], Loss: 0.6176
2022-04-19 02:02:43,548 Epoch[147/150], Step[0150/0157], Loss: 0.6281
2022-04-19 02:02:44,612 HashNet[147/150] bit:48, lr:0.000500000, scale:2.828, train loss:0.595
2022-04-19 02:02:50,495 Epoch[148/150], Step[0000/0157], Loss: 0.6041
2022-04-19 02:03:01,051 Epoch[148/150], Step[0050/0157], Loss: 0.5461
2022-04-19 02:03:10,827 Epoch[148/150], Step[0100/0157], Loss: 0.5742
2022-04-19 02:03:20,754 Epoch[148/150], Step[0150/0157], Loss: 0.5778
2022-04-19 02:03:22,049 HashNet[148/150] bit:48, lr:0.000500000, scale:2.828, train loss:0.593
2022-04-19 02:03:27,819 Epoch[149/150], Step[0000/0157], Loss: 0.5914
2022-04-19 02:03:38,135 Epoch[149/150], Step[0050/0157], Loss: 0.5970
2022-04-19 02:03:49,122 Epoch[149/150], Step[0100/0157], Loss: 0.5678
2022-04-19 02:03:58,547 Epoch[149/150], Step[0150/0157], Loss: 0.5536
2022-04-19 02:03:59,855 HashNet[149/150] bit:48, lr:0.000500000, scale:2.828, train loss:0.595
2022-04-19 02:04:05,425 Epoch[150/150], Step[0000/0157], Loss: 0.6294
2022-04-19 02:04:16,039 Epoch[150/150], Step[0050/0157], Loss: 0.5598
2022-04-19 02:04:26,432 Epoch[150/150], Step[0100/0157], Loss: 0.6410
2022-04-19 02:04:36,331 Epoch[150/150], Step[0150/0157], Loss: 0.5778
2022-04-19 02:04:37,576 HashNet[150/150] bit:48, lr:0.000500000, scale:2.828, train loss:0.594
2022-04-19 02:04:37,576 ----- Validation after Epoch: 150
2022-04-19 02:12:13,787 save in checkpoints_new//train-48-20220418-22-42-51/bit_48
2022-04-19 02:12:16,008 Max mAP so far: 0.7161 at epoch_150
2022-04-19 02:12:16,008 ----- Save BEST model: checkpoints_new//train-48-20220418-22-42-51/bit_48/coco.pdparams
2022-04-19 02:12:16,009 ----- Save BEST optim: checkpoints_new//train-48-20220418-22-42-51/bit_48/coco.pdopt
2022-04-19 02:12:16,009 HashNet epoch:150, bit:48, dataset:coco, MAP:0.716, Best MAP(e150): 0.716
2022-04-19 02:12:16,009 Training completed for HashNet(48).
2022-04-19 02:12:16,009 Best MAP(e150): 0.716
