2022-04-17 23:57:27,365 
Namespace(alpha=0.1, batch_size=64, bit=32, ckp=None, crop_size=224, data={'train_set': {'list_path': 'data/coco/train.txt', 'batch_size': 64}, 'database': {'list_path': 'data/coco/database.txt', 'batch_size': 64}, 'test': {'list_path': 'data/coco/test.txt', 'batch_size': 64}}, data_path='/home/super/public/wht/datasets/COCO2014/', dataset='coco', de_step=50, debug_steps=50, epoch=150, eval=False, eval_epoch=10, last_epoch=0, learning_rate=0.001, log_path='logs_new/', model='HashNet', momentum=0.9, num_class=80, num_train=10000, optimizer='SGD', resize_size=256, resume=None, save_path='checkpoints_new/', seed=2000, step_continuation=20, topK=5000, weight_decay=0.0005)
2022-04-17 23:57:27,365 ----- world_size = 2, local_rank = 0
2022-04-17 23:57:28,858 ----- Total # of train batch (single gpu): 157
2022-04-17 23:57:28,858 ----- Total # of test batch (single gpu): 79
2022-04-17 23:57:28,858 ----- Total # of base batch (single gpu): 1754
2022-04-17 23:57:28,858 Start training from epoch 1.
2022-04-17 23:57:34,103 Epoch[001/150], Step[0000/0157], Loss: 1.3403
2022-04-17 23:57:40,391 Epoch[001/150], Step[0050/0157], Loss: 1.1534
2022-04-17 23:57:47,252 Epoch[001/150], Step[0100/0157], Loss: 1.2294
2022-04-17 23:57:54,250 Epoch[001/150], Step[0150/0157], Loss: 1.2106
2022-04-17 23:57:55,138 HashNet[ 1/150] bit:32, lr:0.002000000, scale:1.000, train loss:1.175
2022-04-17 23:57:59,664 Epoch[002/150], Step[0000/0157], Loss: 1.1460
2022-04-17 23:58:06,539 Epoch[002/150], Step[0050/0157], Loss: 1.1688
2022-04-17 23:58:13,570 Epoch[002/150], Step[0100/0157], Loss: 1.1662
2022-04-17 23:58:20,382 Epoch[002/150], Step[0150/0157], Loss: 1.0168
2022-04-17 23:58:21,207 HashNet[ 2/150] bit:32, lr:0.002000000, scale:1.000, train loss:1.145
2022-04-17 23:58:25,722 Epoch[003/150], Step[0000/0157], Loss: 1.0407
2022-04-17 23:58:32,715 Epoch[003/150], Step[0050/0157], Loss: 1.0613
2022-04-17 23:58:39,587 Epoch[003/150], Step[0100/0157], Loss: 1.0588
2022-04-17 23:58:46,623 Epoch[003/150], Step[0150/0157], Loss: 0.9924
2022-04-17 23:58:47,445 HashNet[ 3/150] bit:32, lr:0.002000000, scale:1.000, train loss:1.042
2022-04-17 23:58:51,970 Epoch[004/150], Step[0000/0157], Loss: 1.0325
2022-04-17 23:58:58,840 Epoch[004/150], Step[0050/0157], Loss: 0.9634
2022-04-17 23:59:05,844 Epoch[004/150], Step[0100/0157], Loss: 1.0081
2022-04-17 23:59:12,701 Epoch[004/150], Step[0150/0157], Loss: 0.9745
2022-04-17 23:59:13,520 HashNet[ 4/150] bit:32, lr:0.002000000, scale:1.000, train loss:0.993
2022-04-17 23:59:18,064 Epoch[005/150], Step[0000/0157], Loss: 0.9516
2022-04-17 23:59:25,144 Epoch[005/150], Step[0050/0157], Loss: 0.9605
2022-04-17 23:59:32,028 Epoch[005/150], Step[0100/0157], Loss: 0.9692
2022-04-17 23:59:38,995 Epoch[005/150], Step[0150/0157], Loss: 0.9407
2022-04-17 23:59:39,811 HashNet[ 5/150] bit:32, lr:0.002000000, scale:1.000, train loss:0.968
2022-04-17 23:59:44,449 Epoch[006/150], Step[0000/0157], Loss: 0.9301
2022-04-17 23:59:51,226 Epoch[006/150], Step[0050/0157], Loss: 0.9521
2022-04-17 23:59:58,349 Epoch[006/150], Step[0100/0157], Loss: 0.9495
2022-04-18 00:00:05,250 Epoch[006/150], Step[0150/0157], Loss: 0.9575
2022-04-18 00:00:06,069 HashNet[ 6/150] bit:32, lr:0.002000000, scale:1.000, train loss:0.949
2022-04-18 00:00:10,575 Epoch[007/150], Step[0000/0157], Loss: 0.9711
2022-04-18 00:00:17,417 Epoch[007/150], Step[0050/0157], Loss: 0.9488
2022-04-18 00:00:24,516 Epoch[007/150], Step[0100/0157], Loss: 0.9064
2022-04-18 00:00:31,316 Epoch[007/150], Step[0150/0157], Loss: 0.9150
2022-04-18 00:00:32,148 HashNet[ 7/150] bit:32, lr:0.002000000, scale:1.000, train loss:0.935
2022-04-18 00:00:36,666 Epoch[008/150], Step[0000/0157], Loss: 0.9359
2022-04-18 00:00:43,469 Epoch[008/150], Step[0050/0157], Loss: 0.9167
2022-04-18 00:00:50,555 Epoch[008/150], Step[0100/0157], Loss: 0.9140
2022-04-18 00:00:57,441 Epoch[008/150], Step[0150/0157], Loss: 0.9225
2022-04-18 00:00:58,263 HashNet[ 8/150] bit:32, lr:0.002000000, scale:1.000, train loss:0.927
2022-04-18 00:01:02,945 Epoch[009/150], Step[0000/0157], Loss: 0.9418
2022-04-18 00:01:09,931 Epoch[009/150], Step[0050/0157], Loss: 0.8917
2022-04-18 00:01:16,844 Epoch[009/150], Step[0100/0157], Loss: 0.9322
2022-04-18 00:01:23,721 Epoch[009/150], Step[0150/0157], Loss: 0.9056
2022-04-18 00:01:24,539 HashNet[ 9/150] bit:32, lr:0.002000000, scale:1.000, train loss:0.907
2022-04-18 00:01:29,117 Epoch[010/150], Step[0000/0157], Loss: 0.9251
2022-04-18 00:01:35,929 Epoch[010/150], Step[0050/0157], Loss: 0.8801
2022-04-18 00:01:43,015 Epoch[010/150], Step[0100/0157], Loss: 0.9038
2022-04-18 00:01:49,830 Epoch[010/150], Step[0150/0157], Loss: 0.9240
2022-04-18 00:01:50,671 HashNet[10/150] bit:32, lr:0.002000000, scale:1.000, train loss:0.896
2022-04-18 00:01:50,672 ----- Validation after Epoch: 10
2022-04-18 00:07:14,593 save in checkpoints_new/bit_32
2022-04-18 00:07:15,367 Max mAP so far: 0.6656 at epoch_10
2022-04-18 00:07:15,368 ----- Save BEST model: checkpoints_new/bit_32/coco.pdparams
2022-04-18 00:07:15,368 ----- Save BEST optim: checkpoints_new/bit_32/coco.pdopt
2022-04-18 00:07:15,368 HashNet epoch:10, bit:32, dataset:coco, MAP:0.666, Best MAP(e10): 0.666
2022-04-18 00:07:20,767 Epoch[011/150], Step[0000/0157], Loss: 0.9074
2022-04-18 00:07:27,574 Epoch[011/150], Step[0050/0157], Loss: 0.8671
2022-04-18 00:07:34,579 Epoch[011/150], Step[0100/0157], Loss: 0.9246
2022-04-18 00:07:41,543 Epoch[011/150], Step[0150/0157], Loss: 0.8458
2022-04-18 00:07:42,344 HashNet[11/150] bit:32, lr:0.002000000, scale:1.000, train loss:0.884
2022-04-18 00:07:46,901 Epoch[012/150], Step[0000/0157], Loss: 0.9019
2022-04-18 00:07:53,685 Epoch[012/150], Step[0050/0157], Loss: 0.8720
2022-04-18 00:08:00,794 Epoch[012/150], Step[0100/0157], Loss: 0.8574
2022-04-18 00:08:07,583 Epoch[012/150], Step[0150/0157], Loss: 0.8685
2022-04-18 00:08:08,405 HashNet[12/150] bit:32, lr:0.002000000, scale:1.000, train loss:0.878
2022-04-18 00:08:13,151 Epoch[013/150], Step[0000/0157], Loss: 0.8309
2022-04-18 00:08:19,992 Epoch[013/150], Step[0050/0157], Loss: 0.8141
2022-04-18 00:08:26,926 Epoch[013/150], Step[0100/0157], Loss: 0.8868
2022-04-18 00:08:33,949 Epoch[013/150], Step[0150/0157], Loss: 0.8617
2022-04-18 00:08:34,776 HashNet[13/150] bit:32, lr:0.002000000, scale:1.000, train loss:0.862
2022-04-18 00:08:39,354 Epoch[014/150], Step[0000/0157], Loss: 0.8267
2022-04-18 00:08:46,148 Epoch[014/150], Step[0050/0157], Loss: 0.8447
2022-04-18 00:08:53,303 Epoch[014/150], Step[0100/0157], Loss: 0.8738
2022-04-18 00:09:00,094 Epoch[014/150], Step[0150/0157], Loss: 0.8304
2022-04-18 00:09:00,912 HashNet[14/150] bit:32, lr:0.002000000, scale:1.000, train loss:0.853
2022-04-18 00:09:05,473 Epoch[015/150], Step[0000/0157], Loss: 0.8680
2022-04-18 00:09:12,285 Epoch[015/150], Step[0050/0157], Loss: 0.8561
2022-04-18 00:09:19,371 Epoch[015/150], Step[0100/0157], Loss: 0.8596
2022-04-18 00:09:26,251 Epoch[015/150], Step[0150/0157], Loss: 0.8242
2022-04-18 00:09:27,080 HashNet[15/150] bit:32, lr:0.002000000, scale:1.000, train loss:0.849
2022-04-18 00:09:31,784 Epoch[016/150], Step[0000/0157], Loss: 0.9021
2022-04-18 00:09:38,554 Epoch[016/150], Step[0050/0157], Loss: 0.8053
2022-04-18 00:09:45,638 Epoch[016/150], Step[0100/0157], Loss: 0.8307
2022-04-18 00:09:52,349 Epoch[016/150], Step[0150/0157], Loss: 0.8201
2022-04-18 00:09:53,164 HashNet[16/150] bit:32, lr:0.002000000, scale:1.000, train loss:0.838
2022-04-18 00:09:57,687 Epoch[017/150], Step[0000/0157], Loss: 0.8300
2022-04-18 00:10:04,484 Epoch[017/150], Step[0050/0157], Loss: 0.8468
2022-04-18 00:10:11,599 Epoch[017/150], Step[0100/0157], Loss: 0.8508
2022-04-18 00:10:18,444 Epoch[017/150], Step[0150/0157], Loss: 0.8138
2022-04-18 00:10:19,258 HashNet[17/150] bit:32, lr:0.002000000, scale:1.000, train loss:0.829
2022-04-18 00:10:23,850 Epoch[018/150], Step[0000/0157], Loss: 0.8644
2022-04-18 00:10:30,608 Epoch[018/150], Step[0050/0157], Loss: 0.8281
2022-04-18 00:10:37,707 Epoch[018/150], Step[0100/0157], Loss: 0.8428
2022-04-18 00:10:44,510 Epoch[018/150], Step[0150/0157], Loss: 0.8216
2022-04-18 00:10:45,341 HashNet[18/150] bit:32, lr:0.002000000, scale:1.000, train loss:0.825
2022-04-18 00:10:49,958 Epoch[019/150], Step[0000/0157], Loss: 0.8159
2022-04-18 00:10:56,707 Epoch[019/150], Step[0050/0157], Loss: 0.8103
2022-04-18 00:11:03,830 Epoch[019/150], Step[0100/0157], Loss: 0.8450
2022-04-18 00:11:10,684 Epoch[019/150], Step[0150/0157], Loss: 0.7938
2022-04-18 00:11:11,510 HashNet[19/150] bit:32, lr:0.002000000, scale:1.000, train loss:0.818
2022-04-18 00:11:16,156 Epoch[020/150], Step[0000/0157], Loss: 0.7686
2022-04-18 00:11:22,915 Epoch[020/150], Step[0050/0157], Loss: 0.7672
2022-04-18 00:11:30,010 Epoch[020/150], Step[0100/0157], Loss: 0.8537
2022-04-18 00:11:36,962 Epoch[020/150], Step[0150/0157], Loss: 0.7894
2022-04-18 00:11:37,822 HashNet[20/150] bit:32, lr:0.002000000, scale:1.000, train loss:0.816
2022-04-18 00:11:37,822 ----- Validation after Epoch: 20
2022-04-18 00:17:00,125 save in checkpoints_new/bit_32
2022-04-18 00:17:02,781 Max mAP so far: 0.6697 at epoch_20
2022-04-18 00:17:02,781 ----- Save BEST model: checkpoints_new/bit_32/coco.pdparams
2022-04-18 00:17:02,781 ----- Save BEST optim: checkpoints_new/bit_32/coco.pdopt
2022-04-18 00:17:02,781 HashNet epoch:20, bit:32, dataset:coco, MAP:0.670, Best MAP(e20): 0.670
2022-04-18 00:17:07,446 Epoch[021/150], Step[0000/0157], Loss: 0.8421
2022-04-18 00:17:14,239 Epoch[021/150], Step[0050/0157], Loss: 0.7886
2022-04-18 00:17:21,257 Epoch[021/150], Step[0100/0157], Loss: 0.8404
2022-04-18 00:17:28,100 Epoch[021/150], Step[0150/0157], Loss: 0.7535
2022-04-18 00:17:29,013 HashNet[21/150] bit:32, lr:0.002000000, scale:1.414, train loss:0.799
2022-04-18 00:17:33,565 Epoch[022/150], Step[0000/0157], Loss: 0.7817
2022-04-18 00:17:40,395 Epoch[022/150], Step[0050/0157], Loss: 0.7791
2022-04-18 00:17:47,459 Epoch[022/150], Step[0100/0157], Loss: 0.7697
2022-04-18 00:17:54,329 Epoch[022/150], Step[0150/0157], Loss: 0.8228
2022-04-18 00:17:55,124 HashNet[22/150] bit:32, lr:0.002000000, scale:1.414, train loss:0.792
2022-04-18 00:17:59,705 Epoch[023/150], Step[0000/0157], Loss: 0.8451
2022-04-18 00:18:06,500 Epoch[023/150], Step[0050/0157], Loss: 0.8048
2022-04-18 00:18:13,613 Epoch[023/150], Step[0100/0157], Loss: 0.8038
2022-04-18 00:18:20,510 Epoch[023/150], Step[0150/0157], Loss: 0.8389
2022-04-18 00:18:21,295 HashNet[23/150] bit:32, lr:0.002000000, scale:1.414, train loss:0.792
2022-04-18 00:18:25,868 Epoch[024/150], Step[0000/0157], Loss: 0.7549
2022-04-18 00:18:32,733 Epoch[024/150], Step[0050/0157], Loss: 0.8053
2022-04-18 00:18:39,877 Epoch[024/150], Step[0100/0157], Loss: 0.8203
2022-04-18 00:18:46,713 Epoch[024/150], Step[0150/0157], Loss: 0.8096
2022-04-18 00:18:47,544 HashNet[24/150] bit:32, lr:0.002000000, scale:1.414, train loss:0.784
2022-04-18 00:18:52,126 Epoch[025/150], Step[0000/0157], Loss: 0.7900
2022-04-18 00:18:59,009 Epoch[025/150], Step[0050/0157], Loss: 0.8127
2022-04-18 00:19:06,171 Epoch[025/150], Step[0100/0157], Loss: 0.8433
2022-04-18 00:19:12,983 Epoch[025/150], Step[0150/0157], Loss: 0.7914
2022-04-18 00:19:13,815 HashNet[25/150] bit:32, lr:0.002000000, scale:1.414, train loss:0.779
2022-04-18 00:19:18,362 Epoch[026/150], Step[0000/0157], Loss: 0.7689
2022-04-18 00:19:25,567 Epoch[026/150], Step[0050/0157], Loss: 0.7758
2022-04-18 00:19:32,452 Epoch[026/150], Step[0100/0157], Loss: 0.7645
2022-04-18 00:19:39,615 Epoch[026/150], Step[0150/0157], Loss: 0.7854
2022-04-18 00:19:40,428 HashNet[26/150] bit:32, lr:0.002000000, scale:1.414, train loss:0.772
2022-04-18 00:19:44,975 Epoch[027/150], Step[0000/0157], Loss: 0.7470
2022-04-18 00:19:51,747 Epoch[027/150], Step[0050/0157], Loss: 0.7429
2022-04-18 00:19:58,835 Epoch[027/150], Step[0100/0157], Loss: 0.8003
2022-04-18 00:20:05,711 Epoch[027/150], Step[0150/0157], Loss: 0.7801
2022-04-18 00:20:06,585 HashNet[27/150] bit:32, lr:0.002000000, scale:1.414, train loss:0.765
2022-04-18 00:20:11,183 Epoch[028/150], Step[0000/0157], Loss: 0.7794
2022-04-18 00:20:18,003 Epoch[028/150], Step[0050/0157], Loss: 0.7078
2022-04-18 00:20:25,133 Epoch[028/150], Step[0100/0157], Loss: 0.7730
2022-04-18 00:20:31,954 Epoch[028/150], Step[0150/0157], Loss: 0.7854
2022-04-18 00:20:32,760 HashNet[28/150] bit:32, lr:0.002000000, scale:1.414, train loss:0.764
2022-04-18 00:20:37,657 Epoch[029/150], Step[0000/0157], Loss: 0.7596
2022-04-18 00:20:44,452 Epoch[029/150], Step[0050/0157], Loss: 0.7591
2022-04-18 00:20:51,571 Epoch[029/150], Step[0100/0157], Loss: 0.7222
2022-04-18 00:20:58,404 Epoch[029/150], Step[0150/0157], Loss: 0.7377
2022-04-18 00:20:59,240 HashNet[29/150] bit:32, lr:0.002000000, scale:1.414, train loss:0.761
2022-04-18 00:21:03,882 Epoch[030/150], Step[0000/0157], Loss: 0.7622
2022-04-18 00:21:10,806 Epoch[030/150], Step[0050/0157], Loss: 0.7614
2022-04-18 00:21:17,764 Epoch[030/150], Step[0100/0157], Loss: 0.7652
2022-04-18 00:21:24,678 Epoch[030/150], Step[0150/0157], Loss: 0.7300
2022-04-18 00:21:25,507 HashNet[30/150] bit:32, lr:0.002000000, scale:1.414, train loss:0.759
2022-04-18 00:21:25,507 ----- Validation after Epoch: 30
2022-04-18 00:26:46,293 save in checkpoints_new/bit_32
2022-04-18 00:26:48,974 Max mAP so far: 0.6728 at epoch_30
2022-04-18 00:26:48,974 ----- Save BEST model: checkpoints_new/bit_32/coco.pdparams
2022-04-18 00:26:48,974 ----- Save BEST optim: checkpoints_new/bit_32/coco.pdopt
2022-04-18 00:26:48,974 HashNet epoch:30, bit:32, dataset:coco, MAP:0.673, Best MAP(e30): 0.673
2022-04-18 00:26:53,655 Epoch[031/150], Step[0000/0157], Loss: 0.7680
2022-04-18 00:27:00,847 Epoch[031/150], Step[0050/0157], Loss: 0.7646
2022-04-18 00:27:08,054 Epoch[031/150], Step[0100/0157], Loss: 0.7442
2022-04-18 00:27:14,895 Epoch[031/150], Step[0150/0157], Loss: 0.7350
2022-04-18 00:27:15,695 HashNet[31/150] bit:32, lr:0.002000000, scale:1.414, train loss:0.751
2022-04-18 00:27:20,435 Epoch[032/150], Step[0000/0157], Loss: 0.7096
2022-04-18 00:27:27,165 Epoch[032/150], Step[0050/0157], Loss: 0.7198
2022-04-18 00:27:34,222 Epoch[032/150], Step[0100/0157], Loss: 0.7799
2022-04-18 00:27:41,080 Epoch[032/150], Step[0150/0157], Loss: 0.7557
2022-04-18 00:27:41,913 HashNet[32/150] bit:32, lr:0.002000000, scale:1.414, train loss:0.747
2022-04-18 00:27:46,479 Epoch[033/150], Step[0000/0157], Loss: 0.7472
2022-04-18 00:27:53,326 Epoch[033/150], Step[0050/0157], Loss: 0.7355
2022-04-18 00:28:00,658 Epoch[033/150], Step[0100/0157], Loss: 0.7385
2022-04-18 00:28:07,450 Epoch[033/150], Step[0150/0157], Loss: 0.7582
2022-04-18 00:28:08,272 HashNet[33/150] bit:32, lr:0.002000000, scale:1.414, train loss:0.742
2022-04-18 00:28:12,778 Epoch[034/150], Step[0000/0157], Loss: 0.7609
2022-04-18 00:28:19,614 Epoch[034/150], Step[0050/0157], Loss: 0.7526
2022-04-18 00:28:26,728 Epoch[034/150], Step[0100/0157], Loss: 0.7868
2022-04-18 00:28:34,106 Epoch[034/150], Step[0150/0157], Loss: 0.7785
2022-04-18 00:28:34,901 HashNet[34/150] bit:32, lr:0.002000000, scale:1.414, train loss:0.741
2022-04-18 00:28:39,520 Epoch[035/150], Step[0000/0157], Loss: 0.7631
2022-04-18 00:28:46,415 Epoch[035/150], Step[0050/0157], Loss: 0.7450
2022-04-18 00:28:53,431 Epoch[035/150], Step[0100/0157], Loss: 0.7418
2022-04-18 00:29:00,370 Epoch[035/150], Step[0150/0157], Loss: 0.7088
2022-04-18 00:29:01,199 HashNet[35/150] bit:32, lr:0.002000000, scale:1.414, train loss:0.737
2022-04-18 00:29:05,852 Epoch[036/150], Step[0000/0157], Loss: 0.7647
2022-04-18 00:29:13,574 Epoch[036/150], Step[0050/0157], Loss: 0.7247
2022-04-18 00:29:21,085 Epoch[036/150], Step[0100/0157], Loss: 0.7180
2022-04-18 00:29:28,089 Epoch[036/150], Step[0150/0157], Loss: 0.7588
2022-04-18 00:29:28,942 HashNet[36/150] bit:32, lr:0.002000000, scale:1.414, train loss:0.731
2022-04-18 00:29:33,496 Epoch[037/150], Step[0000/0157], Loss: 0.7064
2022-04-18 00:29:40,313 Epoch[037/150], Step[0050/0157], Loss: 0.7387
2022-04-18 00:29:47,263 Epoch[037/150], Step[0100/0157], Loss: 0.7201
2022-04-18 00:29:54,253 Epoch[037/150], Step[0150/0157], Loss: 0.7377
2022-04-18 00:29:55,118 HashNet[37/150] bit:32, lr:0.002000000, scale:1.414, train loss:0.729
2022-04-18 00:29:59,734 Epoch[038/150], Step[0000/0157], Loss: 0.7186
2022-04-18 00:30:06,561 Epoch[038/150], Step[0050/0157], Loss: 0.7604
2022-04-18 00:30:13,609 Epoch[038/150], Step[0100/0157], Loss: 0.7469
2022-04-18 00:30:20,413 Epoch[038/150], Step[0150/0157], Loss: 0.7451
2022-04-18 00:30:21,297 HashNet[38/150] bit:32, lr:0.002000000, scale:1.414, train loss:0.728
2022-04-18 00:30:25,886 Epoch[039/150], Step[0000/0157], Loss: 0.7317
2022-04-18 00:30:32,992 Epoch[039/150], Step[0050/0157], Loss: 0.7079
2022-04-18 00:30:40,143 Epoch[039/150], Step[0100/0157], Loss: 0.7162
2022-04-18 00:30:46,962 Epoch[039/150], Step[0150/0157], Loss: 0.7068
2022-04-18 00:30:47,764 HashNet[39/150] bit:32, lr:0.002000000, scale:1.414, train loss:0.723
2022-04-18 00:30:52,349 Epoch[040/150], Step[0000/0157], Loss: 0.7361
2022-04-18 00:30:59,449 Epoch[040/150], Step[0050/0157], Loss: 0.7281
2022-04-18 00:31:06,379 Epoch[040/150], Step[0100/0157], Loss: 0.7547
2022-04-18 00:31:13,454 Epoch[040/150], Step[0150/0157], Loss: 0.7322
2022-04-18 00:31:14,272 HashNet[40/150] bit:32, lr:0.002000000, scale:1.414, train loss:0.719
2022-04-18 00:31:14,272 ----- Validation after Epoch: 40
2022-04-18 00:36:35,517 save in checkpoints_new/bit_32
2022-04-18 00:36:38,202 Max mAP so far: 0.6734 at epoch_40
2022-04-18 00:36:38,202 ----- Save BEST model: checkpoints_new/bit_32/coco.pdparams
2022-04-18 00:36:38,202 ----- Save BEST optim: checkpoints_new/bit_32/coco.pdopt
2022-04-18 00:36:38,202 HashNet epoch:40, bit:32, dataset:coco, MAP:0.673, Best MAP(e40): 0.673
2022-04-18 00:36:42,846 Epoch[041/150], Step[0000/0157], Loss: 0.7128
2022-04-18 00:36:49,660 Epoch[041/150], Step[0050/0157], Loss: 0.7291
2022-04-18 00:36:56,766 Epoch[041/150], Step[0100/0157], Loss: 0.7581
2022-04-18 00:37:03,791 Epoch[041/150], Step[0150/0157], Loss: 0.7501
2022-04-18 00:37:04,621 HashNet[41/150] bit:32, lr:0.002000000, scale:1.732, train loss:0.718
2022-04-18 00:37:09,178 Epoch[042/150], Step[0000/0157], Loss: 0.7066
2022-04-18 00:37:15,973 Epoch[042/150], Step[0050/0157], Loss: 0.6956
2022-04-18 00:37:23,048 Epoch[042/150], Step[0100/0157], Loss: 0.7294
2022-04-18 00:37:29,953 Epoch[042/150], Step[0150/0157], Loss: 0.6978
2022-04-18 00:37:30,830 HashNet[42/150] bit:32, lr:0.002000000, scale:1.732, train loss:0.714
2022-04-18 00:37:35,393 Epoch[043/150], Step[0000/0157], Loss: 0.7037
2022-04-18 00:37:42,186 Epoch[043/150], Step[0050/0157], Loss: 0.6901
2022-04-18 00:37:49,292 Epoch[043/150], Step[0100/0157], Loss: 0.7074
2022-04-18 00:37:56,115 Epoch[043/150], Step[0150/0157], Loss: 0.7370
2022-04-18 00:37:56,960 HashNet[43/150] bit:32, lr:0.002000000, scale:1.732, train loss:0.709
2022-04-18 00:38:01,734 Epoch[044/150], Step[0000/0157], Loss: 0.7061
2022-04-18 00:38:08,764 Epoch[044/150], Step[0050/0157], Loss: 0.7527
2022-04-18 00:38:15,852 Epoch[044/150], Step[0100/0157], Loss: 0.6892
2022-04-18 00:38:22,752 Epoch[044/150], Step[0150/0157], Loss: 0.7049
2022-04-18 00:38:23,754 HashNet[44/150] bit:32, lr:0.002000000, scale:1.732, train loss:0.711
2022-04-18 00:38:28,326 Epoch[045/150], Step[0000/0157], Loss: 0.6759
2022-04-18 00:38:35,135 Epoch[045/150], Step[0050/0157], Loss: 0.6886
2022-04-18 00:38:42,210 Epoch[045/150], Step[0100/0157], Loss: 0.6966
2022-04-18 00:38:49,030 Epoch[045/150], Step[0150/0157], Loss: 0.7527
2022-04-18 00:38:49,865 HashNet[45/150] bit:32, lr:0.002000000, scale:1.732, train loss:0.709
2022-04-18 00:38:54,438 Epoch[046/150], Step[0000/0157], Loss: 0.6983
2022-04-18 00:39:01,792 Epoch[046/150], Step[0050/0157], Loss: 0.7280
2022-04-18 00:39:09,154 Epoch[046/150], Step[0100/0157], Loss: 0.6869
2022-04-18 00:39:16,007 Epoch[046/150], Step[0150/0157], Loss: 0.7130
2022-04-18 00:39:16,817 HashNet[46/150] bit:32, lr:0.002000000, scale:1.732, train loss:0.708
2022-04-18 00:39:21,363 Epoch[047/150], Step[0000/0157], Loss: 0.7152
2022-04-18 00:39:28,182 Epoch[047/150], Step[0050/0157], Loss: 0.6893
2022-04-18 00:39:35,270 Epoch[047/150], Step[0100/0157], Loss: 0.6984
2022-04-18 00:39:42,111 Epoch[047/150], Step[0150/0157], Loss: 0.7332
2022-04-18 00:39:42,923 HashNet[47/150] bit:32, lr:0.002000000, scale:1.732, train loss:0.704
2022-04-18 00:39:47,499 Epoch[048/150], Step[0000/0157], Loss: 0.7586
2022-04-18 00:39:54,303 Epoch[048/150], Step[0050/0157], Loss: 0.7227
2022-04-18 00:40:01,318 Epoch[048/150], Step[0100/0157], Loss: 0.7093
2022-04-18 00:40:08,201 Epoch[048/150], Step[0150/0157], Loss: 0.7106
2022-04-18 00:40:09,264 HashNet[48/150] bit:32, lr:0.002000000, scale:1.732, train loss:0.705
2022-04-18 00:40:13,816 Epoch[049/150], Step[0000/0157], Loss: 0.7101
2022-04-18 00:40:20,614 Epoch[049/150], Step[0050/0157], Loss: 0.6931
2022-04-18 00:40:27,672 Epoch[049/150], Step[0100/0157], Loss: 0.7227
2022-04-18 00:40:34,480 Epoch[049/150], Step[0150/0157], Loss: 0.7253
2022-04-18 00:40:35,361 HashNet[49/150] bit:32, lr:0.002000000, scale:1.732, train loss:0.703
2022-04-18 00:40:39,953 Epoch[050/150], Step[0000/0157], Loss: 0.7014
2022-04-18 00:40:46,756 Epoch[050/150], Step[0050/0157], Loss: 0.7021
2022-04-18 00:40:53,868 Epoch[050/150], Step[0100/0157], Loss: 0.7486
2022-04-18 00:41:00,645 Epoch[050/150], Step[0150/0157], Loss: 0.7101
2022-04-18 00:41:01,471 HashNet[50/150] bit:32, lr:0.002000000, scale:1.732, train loss:0.704
2022-04-18 00:41:01,472 ----- Validation after Epoch: 50
2022-04-18 00:46:24,637 save in checkpoints_new/bit_32
2022-04-18 00:46:27,285 Max mAP so far: 0.6741 at epoch_50
2022-04-18 00:46:27,286 ----- Save BEST model: checkpoints_new/bit_32/coco.pdparams
2022-04-18 00:46:27,286 ----- Save BEST optim: checkpoints_new/bit_32/coco.pdopt
2022-04-18 00:46:27,286 HashNet epoch:50, bit:32, dataset:coco, MAP:0.674, Best MAP(e50): 0.674
2022-04-18 00:46:31,869 Epoch[051/150], Step[0000/0157], Loss: 0.7373
2022-04-18 00:46:38,549 Epoch[051/150], Step[0050/0157], Loss: 0.6961
2022-04-18 00:46:45,636 Epoch[051/150], Step[0100/0157], Loss: 0.7010
2022-04-18 00:46:52,515 Epoch[051/150], Step[0150/0157], Loss: 0.7020
2022-04-18 00:46:53,388 HashNet[51/150] bit:32, lr:0.001000000, scale:1.732, train loss:0.696
2022-04-18 00:46:57,946 Epoch[052/150], Step[0000/0157], Loss: 0.6660
2022-04-18 00:47:04,759 Epoch[052/150], Step[0050/0157], Loss: 0.6785
2022-04-18 00:47:12,384 Epoch[052/150], Step[0100/0157], Loss: 0.7122
2022-04-18 00:47:19,311 Epoch[052/150], Step[0150/0157], Loss: 0.6719
2022-04-18 00:47:20,358 HashNet[52/150] bit:32, lr:0.001000000, scale:1.732, train loss:0.691
2022-04-18 00:47:24,919 Epoch[053/150], Step[0000/0157], Loss: 0.6732
2022-04-18 00:47:31,734 Epoch[053/150], Step[0050/0157], Loss: 0.6651
2022-04-18 00:47:38,844 Epoch[053/150], Step[0100/0157], Loss: 0.6917
2022-04-18 00:47:45,656 Epoch[053/150], Step[0150/0157], Loss: 0.6472
2022-04-18 00:47:46,488 HashNet[53/150] bit:32, lr:0.001000000, scale:1.732, train loss:0.689
2022-04-18 00:47:51,024 Epoch[054/150], Step[0000/0157], Loss: 0.7274
2022-04-18 00:47:57,864 Epoch[054/150], Step[0050/0157], Loss: 0.7064
2022-04-18 00:48:04,822 Epoch[054/150], Step[0100/0157], Loss: 0.7244
2022-04-18 00:48:11,842 Epoch[054/150], Step[0150/0157], Loss: 0.7020
2022-04-18 00:48:12,653 HashNet[54/150] bit:32, lr:0.001000000, scale:1.732, train loss:0.690
2022-04-18 00:48:17,265 Epoch[055/150], Step[0000/0157], Loss: 0.6933
2022-04-18 00:48:24,203 Epoch[055/150], Step[0050/0157], Loss: 0.7067
2022-04-18 00:48:31,118 Epoch[055/150], Step[0100/0157], Loss: 0.6397
2022-04-18 00:48:38,084 Epoch[055/150], Step[0150/0157], Loss: 0.7040
2022-04-18 00:48:38,920 HashNet[55/150] bit:32, lr:0.001000000, scale:1.732, train loss:0.689
2022-04-18 00:48:43,621 Epoch[056/150], Step[0000/0157], Loss: 0.7032
2022-04-18 00:48:50,375 Epoch[056/150], Step[0050/0157], Loss: 0.6220
2022-04-18 00:48:57,447 Epoch[056/150], Step[0100/0157], Loss: 0.6604
2022-04-18 00:49:04,270 Epoch[056/150], Step[0150/0157], Loss: 0.6793
2022-04-18 00:49:05,097 HashNet[56/150] bit:32, lr:0.001000000, scale:1.732, train loss:0.685
2022-04-18 00:49:09,682 Epoch[057/150], Step[0000/0157], Loss: 0.6991
2022-04-18 00:49:16,510 Epoch[057/150], Step[0050/0157], Loss: 0.7106
2022-04-18 00:49:23,530 Epoch[057/150], Step[0100/0157], Loss: 0.7134
2022-04-18 00:49:30,386 Epoch[057/150], Step[0150/0157], Loss: 0.7001
2022-04-18 00:49:31,208 HashNet[57/150] bit:32, lr:0.001000000, scale:1.732, train loss:0.687
2022-04-18 00:49:35,759 Epoch[058/150], Step[0000/0157], Loss: 0.7019
2022-04-18 00:49:42,690 Epoch[058/150], Step[0050/0157], Loss: 0.6951
2022-04-18 00:49:49,609 Epoch[058/150], Step[0100/0157], Loss: 0.6834
2022-04-18 00:49:56,688 Epoch[058/150], Step[0150/0157], Loss: 0.6633
2022-04-18 00:49:57,515 HashNet[58/150] bit:32, lr:0.001000000, scale:1.732, train loss:0.686
2022-04-18 00:50:02,061 Epoch[059/150], Step[0000/0157], Loss: 0.7432
2022-04-18 00:50:08,896 Epoch[059/150], Step[0050/0157], Loss: 0.6324
2022-04-18 00:50:15,940 Epoch[059/150], Step[0100/0157], Loss: 0.6855
2022-04-18 00:50:22,899 Epoch[059/150], Step[0150/0157], Loss: 0.6914
2022-04-18 00:50:23,695 HashNet[59/150] bit:32, lr:0.001000000, scale:1.732, train loss:0.682
2022-04-18 00:50:28,249 Epoch[060/150], Step[0000/0157], Loss: 0.6593
2022-04-18 00:50:35,061 Epoch[060/150], Step[0050/0157], Loss: 0.6536
2022-04-18 00:50:42,132 Epoch[060/150], Step[0100/0157], Loss: 0.6807
2022-04-18 00:50:48,946 Epoch[060/150], Step[0150/0157], Loss: 0.6846
2022-04-18 00:50:49,781 HashNet[60/150] bit:32, lr:0.001000000, scale:1.732, train loss:0.684
2022-04-18 00:50:49,781 ----- Validation after Epoch: 60
2022-04-18 00:56:14,985 save in checkpoints_new/bit_32
2022-04-18 00:56:17,611 Max mAP so far: 0.6755 at epoch_60
2022-04-18 00:56:17,611 ----- Save BEST model: checkpoints_new/bit_32/coco.pdparams
2022-04-18 00:56:17,611 ----- Save BEST optim: checkpoints_new/bit_32/coco.pdopt
2022-04-18 00:56:17,611 HashNet epoch:60, bit:32, dataset:coco, MAP:0.675, Best MAP(e60): 0.675
2022-04-18 00:56:22,192 Epoch[061/150], Step[0000/0157], Loss: 0.6796
2022-04-18 00:56:29,094 Epoch[061/150], Step[0050/0157], Loss: 0.6853
2022-04-18 00:56:36,152 Epoch[061/150], Step[0100/0157], Loss: 0.7379
2022-04-18 00:56:43,062 Epoch[061/150], Step[0150/0157], Loss: 0.6667
2022-04-18 00:56:43,872 HashNet[61/150] bit:32, lr:0.001000000, scale:2.000, train loss:0.681
2022-04-18 00:56:48,445 Epoch[062/150], Step[0000/0157], Loss: 0.6607
2022-04-18 00:56:55,222 Epoch[062/150], Step[0050/0157], Loss: 0.6792
2022-04-18 00:57:02,294 Epoch[062/150], Step[0100/0157], Loss: 0.6485
2022-04-18 00:57:09,177 Epoch[062/150], Step[0150/0157], Loss: 0.7252
2022-04-18 00:57:09,991 HashNet[62/150] bit:32, lr:0.001000000, scale:2.000, train loss:0.681
2022-04-18 00:57:14,925 Epoch[063/150], Step[0000/0157], Loss: 0.6606
2022-04-18 00:57:21,782 Epoch[063/150], Step[0050/0157], Loss: 0.7004
2022-04-18 00:57:28,807 Epoch[063/150], Step[0100/0157], Loss: 0.6589
2022-04-18 00:57:35,595 Epoch[063/150], Step[0150/0157], Loss: 0.6488
2022-04-18 00:57:36,397 HashNet[63/150] bit:32, lr:0.001000000, scale:2.000, train loss:0.679
2022-04-18 00:57:41,142 Epoch[064/150], Step[0000/0157], Loss: 0.6863
2022-04-18 00:57:47,966 Epoch[064/150], Step[0050/0157], Loss: 0.6610
2022-04-18 00:57:54,976 Epoch[064/150], Step[0100/0157], Loss: 0.6977
2022-04-18 00:58:02,052 Epoch[064/150], Step[0150/0157], Loss: 0.6613
2022-04-18 00:58:02,840 HashNet[64/150] bit:32, lr:0.001000000, scale:2.000, train loss:0.679
2022-04-18 00:58:07,400 Epoch[065/150], Step[0000/0157], Loss: 0.6440
2022-04-18 00:58:14,237 Epoch[065/150], Step[0050/0157], Loss: 0.7228
2022-04-18 00:58:21,354 Epoch[065/150], Step[0100/0157], Loss: 0.7021
2022-04-18 00:58:28,200 Epoch[065/150], Step[0150/0157], Loss: 0.6722
2022-04-18 00:58:29,016 HashNet[65/150] bit:32, lr:0.001000000, scale:2.000, train loss:0.680
2022-04-18 00:58:33,737 Epoch[066/150], Step[0000/0157], Loss: 0.6825
2022-04-18 00:58:40,526 Epoch[066/150], Step[0050/0157], Loss: 0.6561
2022-04-18 00:58:47,543 Epoch[066/150], Step[0100/0157], Loss: 0.6760
2022-04-18 00:58:54,360 Epoch[066/150], Step[0150/0157], Loss: 0.7184
2022-04-18 00:58:55,191 HashNet[66/150] bit:32, lr:0.001000000, scale:2.000, train loss:0.679
2022-04-18 00:58:59,737 Epoch[067/150], Step[0000/0157], Loss: 0.7286
2022-04-18 00:59:06,555 Epoch[067/150], Step[0050/0157], Loss: 0.6844
2022-04-18 00:59:13,642 Epoch[067/150], Step[0100/0157], Loss: 0.6387
2022-04-18 00:59:20,393 Epoch[067/150], Step[0150/0157], Loss: 0.6483
2022-04-18 00:59:21,233 HashNet[67/150] bit:32, lr:0.001000000, scale:2.000, train loss:0.679
2022-04-18 00:59:25,809 Epoch[068/150], Step[0000/0157], Loss: 0.6999
2022-04-18 00:59:33,135 Epoch[068/150], Step[0050/0157], Loss: 0.6558
2022-04-18 00:59:40,278 Epoch[068/150], Step[0100/0157], Loss: 0.6869
2022-04-18 00:59:47,450 Epoch[068/150], Step[0150/0157], Loss: 0.6792
2022-04-18 00:59:48,234 HashNet[68/150] bit:32, lr:0.001000000, scale:2.000, train loss:0.678
2022-04-18 00:59:52,789 Epoch[069/150], Step[0000/0157], Loss: 0.6884
2022-04-18 00:59:59,647 Epoch[069/150], Step[0050/0157], Loss: 0.6993
2022-04-18 01:00:06,447 Epoch[069/150], Step[0100/0157], Loss: 0.7104
2022-04-18 01:00:13,375 Epoch[069/150], Step[0150/0157], Loss: 0.6437
2022-04-18 01:00:14,172 HashNet[69/150] bit:32, lr:0.001000000, scale:2.000, train loss:0.680
2022-04-18 01:00:19,050 Epoch[070/150], Step[0000/0157], Loss: 0.6237
2022-04-18 01:00:25,920 Epoch[070/150], Step[0050/0157], Loss: 0.6964
2022-04-18 01:00:33,067 Epoch[070/150], Step[0100/0157], Loss: 0.6602
2022-04-18 01:00:39,877 Epoch[070/150], Step[0150/0157], Loss: 0.6360
2022-04-18 01:00:40,701 HashNet[70/150] bit:32, lr:0.001000000, scale:2.000, train loss:0.678
2022-04-18 01:00:40,701 ----- Validation after Epoch: 70
2022-04-18 01:06:04,689 save in checkpoints_new/bit_32
2022-04-18 01:06:07,325 Max mAP so far: 0.6813 at epoch_70
2022-04-18 01:06:07,325 ----- Save BEST model: checkpoints_new/bit_32/coco.pdparams
2022-04-18 01:06:07,326 ----- Save BEST optim: checkpoints_new/bit_32/coco.pdopt
2022-04-18 01:06:07,326 HashNet epoch:70, bit:32, dataset:coco, MAP:0.681, Best MAP(e70): 0.681
2022-04-18 01:06:11,816 Epoch[071/150], Step[0000/0157], Loss: 0.6768
2022-04-18 01:06:18,768 Epoch[071/150], Step[0050/0157], Loss: 0.7015
2022-04-18 01:06:25,985 Epoch[071/150], Step[0100/0157], Loss: 0.6626
2022-04-18 01:06:33,244 Epoch[071/150], Step[0150/0157], Loss: 0.7253
2022-04-18 01:06:34,025 HashNet[71/150] bit:32, lr:0.001000000, scale:2.000, train loss:0.678
2022-04-18 01:06:38,602 Epoch[072/150], Step[0000/0157], Loss: 0.6729
2022-04-18 01:06:45,688 Epoch[072/150], Step[0050/0157], Loss: 0.6777
2022-04-18 01:06:52,803 Epoch[072/150], Step[0100/0157], Loss: 0.6514
2022-04-18 01:07:00,089 Epoch[072/150], Step[0150/0157], Loss: 0.7202
2022-04-18 01:07:00,916 HashNet[72/150] bit:32, lr:0.001000000, scale:2.000, train loss:0.676
2022-04-18 01:07:05,480 Epoch[073/150], Step[0000/0157], Loss: 0.6918
2022-04-18 01:07:12,331 Epoch[073/150], Step[0050/0157], Loss: 0.6740
2022-04-18 01:07:19,470 Epoch[073/150], Step[0100/0157], Loss: 0.6529
2022-04-18 01:07:26,261 Epoch[073/150], Step[0150/0157], Loss: 0.6371
2022-04-18 01:07:27,080 HashNet[73/150] bit:32, lr:0.001000000, scale:2.000, train loss:0.675
2022-04-18 01:07:31,678 Epoch[074/150], Step[0000/0157], Loss: 0.6308
2022-04-18 01:07:38,465 Epoch[074/150], Step[0050/0157], Loss: 0.7117
2022-04-18 01:07:45,499 Epoch[074/150], Step[0100/0157], Loss: 0.6623
2022-04-18 01:07:52,357 Epoch[074/150], Step[0150/0157], Loss: 0.6699
2022-04-18 01:07:53,190 HashNet[74/150] bit:32, lr:0.001000000, scale:2.000, train loss:0.672
2022-04-18 01:07:57,763 Epoch[075/150], Step[0000/0157], Loss: 0.6760
2022-04-18 01:08:04,909 Epoch[075/150], Step[0050/0157], Loss: 0.6873
2022-04-18 01:08:11,783 Epoch[075/150], Step[0100/0157], Loss: 0.6697
2022-04-18 01:08:18,901 Epoch[075/150], Step[0150/0157], Loss: 0.7222
2022-04-18 01:08:19,709 HashNet[75/150] bit:32, lr:0.001000000, scale:2.000, train loss:0.673
2022-04-18 01:08:24,269 Epoch[076/150], Step[0000/0157], Loss: 0.6628
2022-04-18 01:08:31,084 Epoch[076/150], Step[0050/0157], Loss: 0.6774
2022-04-18 01:08:38,182 Epoch[076/150], Step[0100/0157], Loss: 0.6425
2022-04-18 01:08:45,063 Epoch[076/150], Step[0150/0157], Loss: 0.6926
2022-04-18 01:08:45,891 HashNet[76/150] bit:32, lr:0.001000000, scale:2.000, train loss:0.670
2022-04-18 01:08:50,496 Epoch[077/150], Step[0000/0157], Loss: 0.6834
2022-04-18 01:08:57,322 Epoch[077/150], Step[0050/0157], Loss: 0.6722
2022-04-18 01:09:04,279 Epoch[077/150], Step[0100/0157], Loss: 0.6994
2022-04-18 01:09:11,258 Epoch[077/150], Step[0150/0157], Loss: 0.6812
2022-04-18 01:09:12,072 HashNet[77/150] bit:32, lr:0.001000000, scale:2.000, train loss:0.676
2022-04-18 01:09:16,999 Epoch[078/150], Step[0000/0157], Loss: 0.7006
2022-04-18 01:09:23,935 Epoch[078/150], Step[0050/0157], Loss: 0.6342
2022-04-18 01:09:31,047 Epoch[078/150], Step[0100/0157], Loss: 0.7018
2022-04-18 01:09:37,861 Epoch[078/150], Step[0150/0157], Loss: 0.6762
2022-04-18 01:09:38,657 HashNet[78/150] bit:32, lr:0.001000000, scale:2.000, train loss:0.675
2022-04-18 01:09:43,444 Epoch[079/150], Step[0000/0157], Loss: 0.6818
2022-04-18 01:09:50,316 Epoch[079/150], Step[0050/0157], Loss: 0.6965
2022-04-18 01:09:57,420 Epoch[079/150], Step[0100/0157], Loss: 0.6320
2022-04-18 01:10:04,228 Epoch[079/150], Step[0150/0157], Loss: 0.7045
2022-04-18 01:10:05,042 HashNet[79/150] bit:32, lr:0.001000000, scale:2.000, train loss:0.672
2022-04-18 01:10:09,681 Epoch[080/150], Step[0000/0157], Loss: 0.7164
2022-04-18 01:10:16,462 Epoch[080/150], Step[0050/0157], Loss: 0.6660
2022-04-18 01:10:23,560 Epoch[080/150], Step[0100/0157], Loss: 0.6559
2022-04-18 01:10:30,425 Epoch[080/150], Step[0150/0157], Loss: 0.6748
2022-04-18 01:10:31,297 HashNet[80/150] bit:32, lr:0.001000000, scale:2.000, train loss:0.672
2022-04-18 01:10:31,298 ----- Validation after Epoch: 80
2022-04-18 01:15:52,661 HashNet epoch:80, bit:32, dataset:coco, MAP:0.677, Best MAP(e70): 0.681
2022-04-18 01:15:57,531 Epoch[081/150], Step[0000/0157], Loss: 0.6574
2022-04-18 01:16:04,284 Epoch[081/150], Step[0050/0157], Loss: 0.6495
2022-04-18 01:16:11,313 Epoch[081/150], Step[0100/0157], Loss: 0.6571
2022-04-18 01:16:18,274 Epoch[081/150], Step[0150/0157], Loss: 0.6922
2022-04-18 01:16:19,103 HashNet[81/150] bit:32, lr:0.001000000, scale:2.236, train loss:0.671
2022-04-18 01:16:23,654 Epoch[082/150], Step[0000/0157], Loss: 0.6686
2022-04-18 01:16:30,875 Epoch[082/150], Step[0050/0157], Loss: 0.6561
2022-04-18 01:16:37,681 Epoch[082/150], Step[0100/0157], Loss: 0.7334
2022-04-18 01:16:44,805 Epoch[082/150], Step[0150/0157], Loss: 0.6544
2022-04-18 01:16:45,617 HashNet[82/150] bit:32, lr:0.001000000, scale:2.236, train loss:0.671
2022-04-18 01:16:50,190 Epoch[083/150], Step[0000/0157], Loss: 0.6886
2022-04-18 01:16:57,137 Epoch[083/150], Step[0050/0157], Loss: 0.6668
2022-04-18 01:17:04,377 Epoch[083/150], Step[0100/0157], Loss: 0.6842
2022-04-18 01:17:11,247 Epoch[083/150], Step[0150/0157], Loss: 0.6572
2022-04-18 01:17:12,054 HashNet[83/150] bit:32, lr:0.001000000, scale:2.236, train loss:0.669
2022-04-18 01:17:16,638 Epoch[084/150], Step[0000/0157], Loss: 0.6840
2022-04-18 01:17:23,439 Epoch[084/150], Step[0050/0157], Loss: 0.6588
2022-04-18 01:17:30,539 Epoch[084/150], Step[0100/0157], Loss: 0.6606
2022-04-18 01:17:37,291 Epoch[084/150], Step[0150/0157], Loss: 0.7074
2022-04-18 01:17:38,141 HashNet[84/150] bit:32, lr:0.001000000, scale:2.236, train loss:0.670
2022-04-18 01:17:42,852 Epoch[085/150], Step[0000/0157], Loss: 0.6777
2022-04-18 01:17:50,245 Epoch[085/150], Step[0050/0157], Loss: 0.6773
2022-04-18 01:17:57,127 Epoch[085/150], Step[0100/0157], Loss: 0.6780
2022-04-18 01:18:04,314 Epoch[085/150], Step[0150/0157], Loss: 0.7022
2022-04-18 01:18:05,128 HashNet[85/150] bit:32, lr:0.001000000, scale:2.236, train loss:0.669
2022-04-18 01:18:09,674 Epoch[086/150], Step[0000/0157], Loss: 0.6413
2022-04-18 01:18:16,476 Epoch[086/150], Step[0050/0157], Loss: 0.6605
2022-04-18 01:18:23,459 Epoch[086/150], Step[0100/0157], Loss: 0.6487
2022-04-18 01:18:30,451 Epoch[086/150], Step[0150/0157], Loss: 0.6655
2022-04-18 01:18:31,267 HashNet[86/150] bit:32, lr:0.001000000, scale:2.236, train loss:0.671
2022-04-18 01:18:35,888 Epoch[087/150], Step[0000/0157], Loss: 0.6555
2022-04-18 01:18:42,712 Epoch[087/150], Step[0050/0157], Loss: 0.6553
2022-04-18 01:18:49,842 Epoch[087/150], Step[0100/0157], Loss: 0.6598
2022-04-18 01:18:56,616 Epoch[087/150], Step[0150/0157], Loss: 0.6639
2022-04-18 01:18:57,465 HashNet[87/150] bit:32, lr:0.001000000, scale:2.236, train loss:0.670
2022-04-18 01:19:02,172 Epoch[088/150], Step[0000/0157], Loss: 0.6564
2022-04-18 01:19:08,953 Epoch[088/150], Step[0050/0157], Loss: 0.6735
2022-04-18 01:19:16,025 Epoch[088/150], Step[0100/0157], Loss: 0.6697
2022-04-18 01:19:22,861 Epoch[088/150], Step[0150/0157], Loss: 0.6445
2022-04-18 01:19:23,719 HashNet[88/150] bit:32, lr:0.001000000, scale:2.236, train loss:0.669
2022-04-18 01:19:28,502 Epoch[089/150], Step[0000/0157], Loss: 0.6627
2022-04-18 01:19:35,762 Epoch[089/150], Step[0050/0157], Loss: 0.7073
2022-04-18 01:19:42,960 Epoch[089/150], Step[0100/0157], Loss: 0.6884
2022-04-18 01:19:49,808 Epoch[089/150], Step[0150/0157], Loss: 0.6557
2022-04-18 01:19:50,617 HashNet[89/150] bit:32, lr:0.001000000, scale:2.236, train loss:0.671
2022-04-18 01:19:55,179 Epoch[090/150], Step[0000/0157], Loss: 0.6788
2022-04-18 01:20:02,880 Epoch[090/150], Step[0050/0157], Loss: 0.6592
2022-04-18 01:20:09,992 Epoch[090/150], Step[0100/0157], Loss: 0.6809
2022-04-18 01:20:17,175 Epoch[090/150], Step[0150/0157], Loss: 0.6770
2022-04-18 01:20:17,991 HashNet[90/150] bit:32, lr:0.001000000, scale:2.236, train loss:0.668
2022-04-18 01:20:17,992 ----- Validation after Epoch: 90
2022-04-18 01:25:39,393 HashNet epoch:90, bit:32, dataset:coco, MAP:0.680, Best MAP(e70): 0.681
2022-04-18 01:25:45,354 Epoch[091/150], Step[0000/0157], Loss: 0.6766
2022-04-18 01:25:52,275 Epoch[091/150], Step[0050/0157], Loss: 0.6681
2022-04-18 01:25:59,624 Epoch[091/150], Step[0100/0157], Loss: 0.6545
2022-04-18 01:26:07,194 Epoch[091/150], Step[0150/0157], Loss: 0.7017
2022-04-18 01:26:07,978 HashNet[91/150] bit:32, lr:0.001000000, scale:2.236, train loss:0.666
2022-04-18 01:26:12,595 Epoch[092/150], Step[0000/0157], Loss: 0.6456
2022-04-18 01:26:19,438 Epoch[092/150], Step[0050/0157], Loss: 0.6950
2022-04-18 01:26:26,420 Epoch[092/150], Step[0100/0157], Loss: 0.6816
2022-04-18 01:26:33,327 Epoch[092/150], Step[0150/0157], Loss: 0.6703
2022-04-18 01:26:34,146 HashNet[92/150] bit:32, lr:0.001000000, scale:2.236, train loss:0.670
2022-04-18 01:26:38,877 Epoch[093/150], Step[0000/0157], Loss: 0.6211
2022-04-18 01:26:45,689 Epoch[093/150], Step[0050/0157], Loss: 0.6712
2022-04-18 01:26:52,693 Epoch[093/150], Step[0100/0157], Loss: 0.6794
2022-04-18 01:26:59,557 Epoch[093/150], Step[0150/0157], Loss: 0.6742
2022-04-18 01:27:00,398 HashNet[93/150] bit:32, lr:0.001000000, scale:2.236, train loss:0.668
2022-04-18 01:27:04,945 Epoch[094/150], Step[0000/0157], Loss: 0.6456
2022-04-18 01:27:11,758 Epoch[094/150], Step[0050/0157], Loss: 0.6204
2022-04-18 01:27:18,728 Epoch[094/150], Step[0100/0157], Loss: 0.6875
2022-04-18 01:27:25,624 Epoch[094/150], Step[0150/0157], Loss: 0.6849
2022-04-18 01:27:26,456 HashNet[94/150] bit:32, lr:0.001000000, scale:2.236, train loss:0.664
2022-04-18 01:27:31,211 Epoch[095/150], Step[0000/0157], Loss: 0.6793
2022-04-18 01:27:38,053 Epoch[095/150], Step[0050/0157], Loss: 0.7025
2022-04-18 01:27:45,217 Epoch[095/150], Step[0100/0157], Loss: 0.6919
2022-04-18 01:27:52,128 Epoch[095/150], Step[0150/0157], Loss: 0.6557
2022-04-18 01:27:52,940 HashNet[95/150] bit:32, lr:0.001000000, scale:2.236, train loss:0.665
2022-04-18 01:27:57,487 Epoch[096/150], Step[0000/0157], Loss: 0.7000
2022-04-18 01:28:04,482 Epoch[096/150], Step[0050/0157], Loss: 0.6766
2022-04-18 01:28:11,359 Epoch[096/150], Step[0100/0157], Loss: 0.6700
2022-04-18 01:28:18,318 Epoch[096/150], Step[0150/0157], Loss: 0.6563
2022-04-18 01:28:19,133 HashNet[96/150] bit:32, lr:0.001000000, scale:2.236, train loss:0.665
2022-04-18 01:28:23,901 Epoch[097/150], Step[0000/0157], Loss: 0.6619
2022-04-18 01:28:31,062 Epoch[097/150], Step[0050/0157], Loss: 0.6573
2022-04-18 01:28:38,209 Epoch[097/150], Step[0100/0157], Loss: 0.6595
2022-04-18 01:28:45,151 Epoch[097/150], Step[0150/0157], Loss: 0.6692
2022-04-18 01:28:46,270 HashNet[97/150] bit:32, lr:0.001000000, scale:2.236, train loss:0.664
2022-04-18 01:28:50,812 Epoch[098/150], Step[0000/0157], Loss: 0.6738
2022-04-18 01:28:57,622 Epoch[098/150], Step[0050/0157], Loss: 0.6442
2022-04-18 01:29:04,717 Epoch[098/150], Step[0100/0157], Loss: 0.6521
2022-04-18 01:29:11,550 Epoch[098/150], Step[0150/0157], Loss: 0.6976
2022-04-18 01:29:12,374 HashNet[98/150] bit:32, lr:0.001000000, scale:2.236, train loss:0.668
2022-04-18 01:29:16,944 Epoch[099/150], Step[0000/0157], Loss: 0.6588
2022-04-18 01:29:23,772 Epoch[099/150], Step[0050/0157], Loss: 0.6693
2022-04-18 01:29:30,865 Epoch[099/150], Step[0100/0157], Loss: 0.6519
2022-04-18 01:29:37,686 Epoch[099/150], Step[0150/0157], Loss: 0.6799
2022-04-18 01:29:38,498 HashNet[99/150] bit:32, lr:0.001000000, scale:2.236, train loss:0.668
2022-04-18 01:29:43,089 Epoch[100/150], Step[0000/0157], Loss: 0.6681
2022-04-18 01:29:49,880 Epoch[100/150], Step[0050/0157], Loss: 0.7031
2022-04-18 01:29:56,998 Epoch[100/150], Step[0100/0157], Loss: 0.6991
2022-04-18 01:30:03,806 Epoch[100/150], Step[0150/0157], Loss: 0.6631
2022-04-18 01:30:04,651 HashNet[100/150] bit:32, lr:0.001000000, scale:2.236, train loss:0.666
2022-04-18 01:30:04,652 ----- Validation after Epoch: 100
2022-04-18 01:35:27,750 HashNet epoch:100, bit:32, dataset:coco, MAP:0.681, Best MAP(e70): 0.681
2022-04-18 01:35:32,322 Epoch[101/150], Step[0000/0157], Loss: 0.6344
2022-04-18 01:35:39,251 Epoch[101/150], Step[0050/0157], Loss: 0.6565
2022-04-18 01:35:46,183 Epoch[101/150], Step[0100/0157], Loss: 0.6597
2022-04-18 01:35:53,565 Epoch[101/150], Step[0150/0157], Loss: 0.6518
2022-04-18 01:35:54,378 HashNet[101/150] bit:32, lr:0.000500000, scale:2.449, train loss:0.664
2022-04-18 01:35:58,910 Epoch[102/150], Step[0000/0157], Loss: 0.6275
2022-04-18 01:36:05,690 Epoch[102/150], Step[0050/0157], Loss: 0.6858
2022-04-18 01:36:12,746 Epoch[102/150], Step[0100/0157], Loss: 0.6344
2022-04-18 01:36:19,565 Epoch[102/150], Step[0150/0157], Loss: 0.6789
2022-04-18 01:36:20,515 HashNet[102/150] bit:32, lr:0.000500000, scale:2.449, train loss:0.661
2022-04-18 01:36:25,103 Epoch[103/150], Step[0000/0157], Loss: 0.6547
2022-04-18 01:36:31,904 Epoch[103/150], Step[0050/0157], Loss: 0.6341
2022-04-18 01:36:38,914 Epoch[103/150], Step[0100/0157], Loss: 0.6369
2022-04-18 01:36:45,857 Epoch[103/150], Step[0150/0157], Loss: 0.6832
2022-04-18 01:36:46,711 HashNet[103/150] bit:32, lr:0.000500000, scale:2.449, train loss:0.663
2022-04-18 01:36:51,276 Epoch[104/150], Step[0000/0157], Loss: 0.6769
2022-04-18 01:36:58,139 Epoch[104/150], Step[0050/0157], Loss: 0.6773
2022-04-18 01:37:05,253 Epoch[104/150], Step[0100/0157], Loss: 0.6379
2022-04-18 01:37:12,103 Epoch[104/150], Step[0150/0157], Loss: 0.6652
2022-04-18 01:37:12,913 HashNet[104/150] bit:32, lr:0.000500000, scale:2.449, train loss:0.662
2022-04-18 01:37:17,490 Epoch[105/150], Step[0000/0157], Loss: 0.6467
2022-04-18 01:37:24,285 Epoch[105/150], Step[0050/0157], Loss: 0.6564
2022-04-18 01:37:31,380 Epoch[105/150], Step[0100/0157], Loss: 0.6877
2022-04-18 01:37:38,255 Epoch[105/150], Step[0150/0157], Loss: 0.6529
2022-04-18 01:37:39,083 HashNet[105/150] bit:32, lr:0.000500000, scale:2.449, train loss:0.663
2022-04-18 01:37:43,618 Epoch[106/150], Step[0000/0157], Loss: 0.6925
2022-04-18 01:37:50,439 Epoch[106/150], Step[0050/0157], Loss: 0.6552
2022-04-18 01:37:57,585 Epoch[106/150], Step[0100/0157], Loss: 0.6911
2022-04-18 01:38:04,470 Epoch[106/150], Step[0150/0157], Loss: 0.6699
2022-04-18 01:38:05,282 HashNet[106/150] bit:32, lr:0.000500000, scale:2.449, train loss:0.663
2022-04-18 01:38:09,967 Epoch[107/150], Step[0000/0157], Loss: 0.6851
2022-04-18 01:38:16,725 Epoch[107/150], Step[0050/0157], Loss: 0.6517
2022-04-18 01:38:23,796 Epoch[107/150], Step[0100/0157], Loss: 0.6739
2022-04-18 01:38:30,570 Epoch[107/150], Step[0150/0157], Loss: 0.6712
2022-04-18 01:38:31,398 HashNet[107/150] bit:32, lr:0.000500000, scale:2.449, train loss:0.662
2022-04-18 01:38:35,957 Epoch[108/150], Step[0000/0157], Loss: 0.6329
2022-04-18 01:38:43,196 Epoch[108/150], Step[0050/0157], Loss: 0.6855
2022-04-18 01:38:50,245 Epoch[108/150], Step[0100/0157], Loss: 0.6813
2022-04-18 01:38:57,767 Epoch[108/150], Step[0150/0157], Loss: 0.7030
2022-04-18 01:38:58,560 HashNet[108/150] bit:32, lr:0.000500000, scale:2.449, train loss:0.661
2022-04-18 01:39:03,123 Epoch[109/150], Step[0000/0157], Loss: 0.6892
2022-04-18 01:39:09,965 Epoch[109/150], Step[0050/0157], Loss: 0.6679
2022-04-18 01:39:16,966 Epoch[109/150], Step[0100/0157], Loss: 0.6661
2022-04-18 01:39:23,884 Epoch[109/150], Step[0150/0157], Loss: 0.6600
2022-04-18 01:39:24,708 HashNet[109/150] bit:32, lr:0.000500000, scale:2.449, train loss:0.662
2022-04-18 01:39:29,248 Epoch[110/150], Step[0000/0157], Loss: 0.6709
2022-04-18 01:39:36,031 Epoch[110/150], Step[0050/0157], Loss: 0.6634
2022-04-18 01:39:43,065 Epoch[110/150], Step[0100/0157], Loss: 0.6293
2022-04-18 01:39:49,988 Epoch[110/150], Step[0150/0157], Loss: 0.6821
2022-04-18 01:39:50,826 HashNet[110/150] bit:32, lr:0.000500000, scale:2.449, train loss:0.661
2022-04-18 01:39:50,826 ----- Validation after Epoch: 110
2022-04-18 01:45:13,145 HashNet epoch:110, bit:32, dataset:coco, MAP:0.680, Best MAP(e70): 0.681
2022-04-18 01:45:17,787 Epoch[111/150], Step[0000/0157], Loss: 0.6942
2022-04-18 01:45:24,592 Epoch[111/150], Step[0050/0157], Loss: 0.6451
2022-04-18 01:45:32,218 Epoch[111/150], Step[0100/0157], Loss: 0.6536
2022-04-18 01:45:39,075 Epoch[111/150], Step[0150/0157], Loss: 0.6515
2022-04-18 01:45:39,889 HashNet[111/150] bit:32, lr:0.000500000, scale:2.449, train loss:0.661
2022-04-18 01:45:44,576 Epoch[112/150], Step[0000/0157], Loss: 0.6527
2022-04-18 01:45:51,359 Epoch[112/150], Step[0050/0157], Loss: 0.6697
2022-04-18 01:45:58,493 Epoch[112/150], Step[0100/0157], Loss: 0.6235
2022-04-18 01:46:05,411 Epoch[112/150], Step[0150/0157], Loss: 0.6707
2022-04-18 01:46:06,401 HashNet[112/150] bit:32, lr:0.000500000, scale:2.449, train loss:0.661
2022-04-18 01:46:10,966 Epoch[113/150], Step[0000/0157], Loss: 0.6251
2022-04-18 01:46:17,756 Epoch[113/150], Step[0050/0157], Loss: 0.6583
2022-04-18 01:46:24,837 Epoch[113/150], Step[0100/0157], Loss: 0.6855
2022-04-18 01:46:31,718 Epoch[113/150], Step[0150/0157], Loss: 0.6593
2022-04-18 01:46:32,592 HashNet[113/150] bit:32, lr:0.000500000, scale:2.449, train loss:0.661
2022-04-18 01:46:37,152 Epoch[114/150], Step[0000/0157], Loss: 0.6630
2022-04-18 01:46:43,952 Epoch[114/150], Step[0050/0157], Loss: 0.6633
2022-04-18 01:46:51,053 Epoch[114/150], Step[0100/0157], Loss: 0.6680
2022-04-18 01:46:57,804 Epoch[114/150], Step[0150/0157], Loss: 0.6498
2022-04-18 01:46:58,646 HashNet[114/150] bit:32, lr:0.000500000, scale:2.449, train loss:0.661
2022-04-18 01:47:03,399 Epoch[115/150], Step[0000/0157], Loss: 0.6662
2022-04-18 01:47:10,166 Epoch[115/150], Step[0050/0157], Loss: 0.6322
2022-04-18 01:47:17,621 Epoch[115/150], Step[0100/0157], Loss: 0.6675
2022-04-18 01:47:24,580 Epoch[115/150], Step[0150/0157], Loss: 0.7012
2022-04-18 01:47:25,387 HashNet[115/150] bit:32, lr:0.000500000, scale:2.449, train loss:0.659
2022-04-18 01:47:29,918 Epoch[116/150], Step[0000/0157], Loss: 0.6260
2022-04-18 01:47:36,743 Epoch[116/150], Step[0050/0157], Loss: 0.6414
2022-04-18 01:47:43,758 Epoch[116/150], Step[0100/0157], Loss: 0.6292
2022-04-18 01:47:50,661 Epoch[116/150], Step[0150/0157], Loss: 0.6608
2022-04-18 01:47:51,514 HashNet[116/150] bit:32, lr:0.000500000, scale:2.449, train loss:0.658
2022-04-18 01:47:56,298 Epoch[117/150], Step[0000/0157], Loss: 0.6373
2022-04-18 01:48:02,987 Epoch[117/150], Step[0050/0157], Loss: 0.6660
2022-04-18 01:48:09,998 Epoch[117/150], Step[0100/0157], Loss: 0.6637
2022-04-18 01:48:16,776 Epoch[117/150], Step[0150/0157], Loss: 0.6579
2022-04-18 01:48:17,601 HashNet[117/150] bit:32, lr:0.000500000, scale:2.449, train loss:0.660
2022-04-18 01:48:22,136 Epoch[118/150], Step[0000/0157], Loss: 0.6691
2022-04-18 01:48:28,947 Epoch[118/150], Step[0050/0157], Loss: 0.6614
2022-04-18 01:48:36,083 Epoch[118/150], Step[0100/0157], Loss: 0.6533
2022-04-18 01:48:42,874 Epoch[118/150], Step[0150/0157], Loss: 0.6715
2022-04-18 01:48:43,691 HashNet[118/150] bit:32, lr:0.000500000, scale:2.449, train loss:0.657
2022-04-18 01:48:48,236 Epoch[119/150], Step[0000/0157], Loss: 0.6749
2022-04-18 01:48:55,061 Epoch[119/150], Step[0050/0157], Loss: 0.6786
2022-04-18 01:49:02,035 Epoch[119/150], Step[0100/0157], Loss: 0.6390
2022-04-18 01:49:08,911 Epoch[119/150], Step[0150/0157], Loss: 0.6210
2022-04-18 01:49:09,786 HashNet[119/150] bit:32, lr:0.000500000, scale:2.449, train loss:0.657
2022-04-18 01:49:14,340 Epoch[120/150], Step[0000/0157], Loss: 0.6559
2022-04-18 01:49:21,449 Epoch[120/150], Step[0050/0157], Loss: 0.6462
2022-04-18 01:49:28,548 Epoch[120/150], Step[0100/0157], Loss: 0.6773
2022-04-18 01:49:35,307 Epoch[120/150], Step[0150/0157], Loss: 0.6499
2022-04-18 01:49:36,118 HashNet[120/150] bit:32, lr:0.000500000, scale:2.449, train loss:0.659
2022-04-18 01:49:36,119 ----- Validation after Epoch: 120
2022-04-18 01:54:58,618 HashNet epoch:120, bit:32, dataset:coco, MAP:0.680, Best MAP(e70): 0.681
2022-04-18 01:55:03,428 Epoch[121/150], Step[0000/0157], Loss: 0.6434
2022-04-18 01:55:10,228 Epoch[121/150], Step[0050/0157], Loss: 0.6558
2022-04-18 01:55:17,420 Epoch[121/150], Step[0100/0157], Loss: 0.6628
2022-04-18 01:55:24,337 Epoch[121/150], Step[0150/0157], Loss: 0.6391
2022-04-18 01:55:25,143 HashNet[121/150] bit:32, lr:0.000500000, scale:2.646, train loss:0.659
2022-04-18 01:55:29,690 Epoch[122/150], Step[0000/0157], Loss: 0.6720
2022-04-18 01:55:36,477 Epoch[122/150], Step[0050/0157], Loss: 0.6703
2022-04-18 01:55:43,585 Epoch[122/150], Step[0100/0157], Loss: 0.6647
2022-04-18 01:55:50,425 Epoch[122/150], Step[0150/0157], Loss: 0.6722
2022-04-18 01:55:51,318 HashNet[122/150] bit:32, lr:0.000500000, scale:2.646, train loss:0.659
2022-04-18 01:55:55,855 Epoch[123/150], Step[0000/0157], Loss: 0.6831
2022-04-18 01:56:02,640 Epoch[123/150], Step[0050/0157], Loss: 0.6700
2022-04-18 01:56:09,774 Epoch[123/150], Step[0100/0157], Loss: 0.7022
2022-04-18 01:56:16,597 Epoch[123/150], Step[0150/0157], Loss: 0.6918
2022-04-18 01:56:17,423 HashNet[123/150] bit:32, lr:0.000500000, scale:2.646, train loss:0.658
2022-04-18 01:56:21,986 Epoch[124/150], Step[0000/0157], Loss: 0.6646
2022-04-18 01:56:28,791 Epoch[124/150], Step[0050/0157], Loss: 0.6751
2022-04-18 01:56:35,834 Epoch[124/150], Step[0100/0157], Loss: 0.6416
2022-04-18 01:56:42,750 Epoch[124/150], Step[0150/0157], Loss: 0.7007
2022-04-18 01:56:43,565 HashNet[124/150] bit:32, lr:0.000500000, scale:2.646, train loss:0.658
2022-04-18 01:56:48,136 Epoch[125/150], Step[0000/0157], Loss: 0.6790
2022-04-18 01:56:54,940 Epoch[125/150], Step[0050/0157], Loss: 0.6507
2022-04-18 01:57:02,101 Epoch[125/150], Step[0100/0157], Loss: 0.6527
2022-04-18 01:57:08,958 Epoch[125/150], Step[0150/0157], Loss: 0.6581
2022-04-18 01:57:09,765 HashNet[125/150] bit:32, lr:0.000500000, scale:2.646, train loss:0.658
2022-04-18 01:57:14,310 Epoch[126/150], Step[0000/0157], Loss: 0.6979
2022-04-18 01:57:21,090 Epoch[126/150], Step[0050/0157], Loss: 0.6730
2022-04-18 01:57:28,239 Epoch[126/150], Step[0100/0157], Loss: 0.6331
2022-04-18 01:57:35,014 Epoch[126/150], Step[0150/0157], Loss: 0.6357
2022-04-18 01:57:35,905 HashNet[126/150] bit:32, lr:0.000500000, scale:2.646, train loss:0.659
2022-04-18 01:57:40,464 Epoch[127/150], Step[0000/0157], Loss: 0.6740
2022-04-18 01:57:47,274 Epoch[127/150], Step[0050/0157], Loss: 0.6549
2022-04-18 01:57:54,360 Epoch[127/150], Step[0100/0157], Loss: 0.7029
2022-04-18 01:58:01,155 Epoch[127/150], Step[0150/0157], Loss: 0.6404
2022-04-18 01:58:01,991 HashNet[127/150] bit:32, lr:0.000500000, scale:2.646, train loss:0.658
2022-04-18 01:58:06,516 Epoch[128/150], Step[0000/0157], Loss: 0.6962
2022-04-18 01:58:13,338 Epoch[128/150], Step[0050/0157], Loss: 0.6515
2022-04-18 01:58:20,393 Epoch[128/150], Step[0100/0157], Loss: 0.6674
2022-04-18 01:58:27,258 Epoch[128/150], Step[0150/0157], Loss: 0.6517
2022-04-18 01:58:28,079 HashNet[128/150] bit:32, lr:0.000500000, scale:2.646, train loss:0.660
2022-04-18 01:58:32,656 Epoch[129/150], Step[0000/0157], Loss: 0.6405
2022-04-18 01:58:39,425 Epoch[129/150], Step[0050/0157], Loss: 0.6518
2022-04-18 01:58:46,464 Epoch[129/150], Step[0100/0157], Loss: 0.6750
2022-04-18 01:58:53,359 Epoch[129/150], Step[0150/0157], Loss: 0.6662
2022-04-18 01:58:54,188 HashNet[129/150] bit:32, lr:0.000500000, scale:2.646, train loss:0.658
2022-04-18 01:58:58,795 Epoch[130/150], Step[0000/0157], Loss: 0.6366
2022-04-18 01:59:05,619 Epoch[130/150], Step[0050/0157], Loss: 0.6469
2022-04-18 01:59:12,771 Epoch[130/150], Step[0100/0157], Loss: 0.6377
2022-04-18 01:59:19,617 Epoch[130/150], Step[0150/0157], Loss: 0.6842
2022-04-18 01:59:20,427 HashNet[130/150] bit:32, lr:0.000500000, scale:2.646, train loss:0.658
2022-04-18 01:59:20,428 ----- Validation after Epoch: 130
2022-04-18 02:04:43,240 save in checkpoints_new/bit_32
2022-04-18 02:04:45,896 Max mAP so far: 0.6816 at epoch_130
2022-04-18 02:04:45,896 ----- Save BEST model: checkpoints_new/bit_32/coco.pdparams
2022-04-18 02:04:45,896 ----- Save BEST optim: checkpoints_new/bit_32/coco.pdopt
2022-04-18 02:04:45,896 HashNet epoch:130, bit:32, dataset:coco, MAP:0.682, Best MAP(e130): 0.682
2022-04-18 02:04:50,521 Epoch[131/150], Step[0000/0157], Loss: 0.6554
2022-04-18 02:04:57,345 Epoch[131/150], Step[0050/0157], Loss: 0.6723
2022-04-18 02:05:04,464 Epoch[131/150], Step[0100/0157], Loss: 0.6427
2022-04-18 02:05:12,137 Epoch[131/150], Step[0150/0157], Loss: 0.6452
2022-04-18 02:05:12,952 HashNet[131/150] bit:32, lr:0.000500000, scale:2.646, train loss:0.658
2022-04-18 02:05:17,520 Epoch[132/150], Step[0000/0157], Loss: 0.6636
2022-04-18 02:05:24,349 Epoch[132/150], Step[0050/0157], Loss: 0.6788
2022-04-18 02:05:31,392 Epoch[132/150], Step[0100/0157], Loss: 0.6353
2022-04-18 02:05:38,276 Epoch[132/150], Step[0150/0157], Loss: 0.6478
2022-04-18 02:05:39,103 HashNet[132/150] bit:32, lr:0.000500000, scale:2.646, train loss:0.660
2022-04-18 02:05:43,729 Epoch[133/150], Step[0000/0157], Loss: 0.6455
2022-04-18 02:05:50,557 Epoch[133/150], Step[0050/0157], Loss: 0.7006
2022-04-18 02:05:57,582 Epoch[133/150], Step[0100/0157], Loss: 0.6933
2022-04-18 02:06:04,454 Epoch[133/150], Step[0150/0157], Loss: 0.6682
2022-04-18 02:06:05,273 HashNet[133/150] bit:32, lr:0.000500000, scale:2.646, train loss:0.658
2022-04-18 02:06:09,992 Epoch[134/150], Step[0000/0157], Loss: 0.6650
2022-04-18 02:06:16,765 Epoch[134/150], Step[0050/0157], Loss: 0.6350
2022-04-18 02:06:23,801 Epoch[134/150], Step[0100/0157], Loss: 0.5985
2022-04-18 02:06:30,593 Epoch[134/150], Step[0150/0157], Loss: 0.6797
2022-04-18 02:06:31,416 HashNet[134/150] bit:32, lr:0.000500000, scale:2.646, train loss:0.658
2022-04-18 02:06:36,108 Epoch[135/150], Step[0000/0157], Loss: 0.6999
2022-04-18 02:06:42,868 Epoch[135/150], Step[0050/0157], Loss: 0.6369
2022-04-18 02:06:49,970 Epoch[135/150], Step[0100/0157], Loss: 0.6394
2022-04-18 02:06:56,747 Epoch[135/150], Step[0150/0157], Loss: 0.6423
2022-04-18 02:06:57,565 HashNet[135/150] bit:32, lr:0.000500000, scale:2.646, train loss:0.659
2022-04-18 02:07:02,136 Epoch[136/150], Step[0000/0157], Loss: 0.6302
2022-04-18 02:07:08,924 Epoch[136/150], Step[0050/0157], Loss: 0.6407
2022-04-18 02:07:16,010 Epoch[136/150], Step[0100/0157], Loss: 0.6904
2022-04-18 02:07:22,916 Epoch[136/150], Step[0150/0157], Loss: 0.6863
2022-04-18 02:07:23,721 HashNet[136/150] bit:32, lr:0.000500000, scale:2.646, train loss:0.658
2022-04-18 02:07:28,274 Epoch[137/150], Step[0000/0157], Loss: 0.6482
2022-04-18 02:07:35,238 Epoch[137/150], Step[0050/0157], Loss: 0.6556
2022-04-18 02:07:42,245 Epoch[137/150], Step[0100/0157], Loss: 0.6513
2022-04-18 02:07:49,534 Epoch[137/150], Step[0150/0157], Loss: 0.6681
2022-04-18 02:07:50,326 HashNet[137/150] bit:32, lr:0.000500000, scale:2.646, train loss:0.659
2022-04-18 02:07:54,960 Epoch[138/150], Step[0000/0157], Loss: 0.6753
2022-04-18 02:08:01,766 Epoch[138/150], Step[0050/0157], Loss: 0.6783
2022-04-18 02:08:08,869 Epoch[138/150], Step[0100/0157], Loss: 0.6211
2022-04-18 02:08:15,700 Epoch[138/150], Step[0150/0157], Loss: 0.6809
2022-04-18 02:08:16,517 HashNet[138/150] bit:32, lr:0.000500000, scale:2.646, train loss:0.658
2022-04-18 02:08:21,404 Epoch[139/150], Step[0000/0157], Loss: 0.6723
2022-04-18 02:08:28,326 Epoch[139/150], Step[0050/0157], Loss: 0.6699
2022-04-18 02:08:35,489 Epoch[139/150], Step[0100/0157], Loss: 0.6304
2022-04-18 02:08:42,295 Epoch[139/150], Step[0150/0157], Loss: 0.6419
2022-04-18 02:08:43,129 HashNet[139/150] bit:32, lr:0.000500000, scale:2.646, train loss:0.657
2022-04-18 02:08:48,021 Epoch[140/150], Step[0000/0157], Loss: 0.6802
2022-04-18 02:08:54,932 Epoch[140/150], Step[0050/0157], Loss: 0.6396
2022-04-18 02:09:01,831 Epoch[140/150], Step[0100/0157], Loss: 0.6605
2022-04-18 02:09:08,775 Epoch[140/150], Step[0150/0157], Loss: 0.6386
2022-04-18 02:09:09,593 HashNet[140/150] bit:32, lr:0.000500000, scale:2.646, train loss:0.656
2022-04-18 02:09:09,593 ----- Validation after Epoch: 140
2022-04-18 02:14:31,307 HashNet epoch:140, bit:32, dataset:coco, MAP:0.681, Best MAP(e130): 0.682
2022-04-18 02:14:39,188 Epoch[141/150], Step[0000/0157], Loss: 0.6590
2022-04-18 02:14:45,996 Epoch[141/150], Step[0050/0157], Loss: 0.6649
2022-04-18 02:14:52,995 Epoch[141/150], Step[0100/0157], Loss: 0.6200
2022-04-18 02:14:59,963 Epoch[141/150], Step[0150/0157], Loss: 0.6869
2022-04-18 02:15:00,771 HashNet[141/150] bit:32, lr:0.000500000, scale:2.828, train loss:0.658
2022-04-18 02:15:05,409 Epoch[142/150], Step[0000/0157], Loss: 0.6609
2022-04-18 02:15:12,185 Epoch[142/150], Step[0050/0157], Loss: 0.6642
2022-04-18 02:15:19,286 Epoch[142/150], Step[0100/0157], Loss: 0.6357
2022-04-18 02:15:26,141 Epoch[142/150], Step[0150/0157], Loss: 0.6923
2022-04-18 02:15:26,970 HashNet[142/150] bit:32, lr:0.000500000, scale:2.828, train loss:0.658
2022-04-18 02:15:31,561 Epoch[143/150], Step[0000/0157], Loss: 0.6584
2022-04-18 02:15:38,354 Epoch[143/150], Step[0050/0157], Loss: 0.6863
2022-04-18 02:15:45,425 Epoch[143/150], Step[0100/0157], Loss: 0.6401
2022-04-18 02:15:52,285 Epoch[143/150], Step[0150/0157], Loss: 0.6615
2022-04-18 02:15:53,101 HashNet[143/150] bit:32, lr:0.000500000, scale:2.828, train loss:0.655
2022-04-18 02:15:57,830 Epoch[144/150], Step[0000/0157], Loss: 0.6866
2022-04-18 02:16:04,602 Epoch[144/150], Step[0050/0157], Loss: 0.6415
2022-04-18 02:16:11,653 Epoch[144/150], Step[0100/0157], Loss: 0.6247
2022-04-18 02:16:18,545 Epoch[144/150], Step[0150/0157], Loss: 0.6285
2022-04-18 02:16:19,371 HashNet[144/150] bit:32, lr:0.000500000, scale:2.828, train loss:0.657
2022-04-18 02:16:23,950 Epoch[145/150], Step[0000/0157], Loss: 0.6819
2022-04-18 02:16:30,740 Epoch[145/150], Step[0050/0157], Loss: 0.6399
2022-04-18 02:16:37,809 Epoch[145/150], Step[0100/0157], Loss: 0.6649
2022-04-18 02:16:44,678 Epoch[145/150], Step[0150/0157], Loss: 0.6525
2022-04-18 02:16:45,518 HashNet[145/150] bit:32, lr:0.000500000, scale:2.828, train loss:0.657
2022-04-18 02:16:50,190 Epoch[146/150], Step[0000/0157], Loss: 0.6578
2022-04-18 02:16:57,011 Epoch[146/150], Step[0050/0157], Loss: 0.6807
2022-04-18 02:17:03,955 Epoch[146/150], Step[0100/0157], Loss: 0.6636
2022-04-18 02:17:10,829 Epoch[146/150], Step[0150/0157], Loss: 0.6410
2022-04-18 02:17:11,683 HashNet[146/150] bit:32, lr:0.000500000, scale:2.828, train loss:0.655
2022-04-18 02:17:16,554 Epoch[147/150], Step[0000/0157], Loss: 0.6818
2022-04-18 02:17:23,347 Epoch[147/150], Step[0050/0157], Loss: 0.6661
2022-04-18 02:17:30,325 Epoch[147/150], Step[0100/0157], Loss: 0.6720
2022-04-18 02:17:37,060 Epoch[147/150], Step[0150/0157], Loss: 0.6482
2022-04-18 02:17:37,986 HashNet[147/150] bit:32, lr:0.000500000, scale:2.828, train loss:0.657
2022-04-18 02:17:42,761 Epoch[148/150], Step[0000/0157], Loss: 0.6400
2022-04-18 02:17:49,553 Epoch[148/150], Step[0050/0157], Loss: 0.6596
2022-04-18 02:17:56,665 Epoch[148/150], Step[0100/0157], Loss: 0.6954
2022-04-18 02:18:03,533 Epoch[148/150], Step[0150/0157], Loss: 0.6305
2022-04-18 02:18:04,344 HashNet[148/150] bit:32, lr:0.000500000, scale:2.828, train loss:0.655
2022-04-18 02:18:08,912 Epoch[149/150], Step[0000/0157], Loss: 0.6417
2022-04-18 02:18:15,688 Epoch[149/150], Step[0050/0157], Loss: 0.6413
2022-04-18 02:18:22,793 Epoch[149/150], Step[0100/0157], Loss: 0.6817
2022-04-18 02:18:29,736 Epoch[149/150], Step[0150/0157], Loss: 0.6419
2022-04-18 02:18:30,570 HashNet[149/150] bit:32, lr:0.000500000, scale:2.828, train loss:0.657
2022-04-18 02:18:35,405 Epoch[150/150], Step[0000/0157], Loss: 0.6431
2022-04-18 02:18:42,772 Epoch[150/150], Step[0050/0157], Loss: 0.6595
2022-04-18 02:18:49,913 Epoch[150/150], Step[0100/0157], Loss: 0.6270
2022-04-18 02:18:56,621 Epoch[150/150], Step[0150/0157], Loss: 0.6671
2022-04-18 02:18:57,410 HashNet[150/150] bit:32, lr:0.000500000, scale:2.828, train loss:0.656
2022-04-18 02:18:57,411 ----- Validation after Epoch: 150
2022-04-18 02:24:19,306 HashNet epoch:150, bit:32, dataset:coco, MAP:0.680, Best MAP(e130): 0.682
2022-04-18 02:24:19,306 Training completed for HashNet(32).
2022-04-18 02:24:19,306 Best MAP(e130): 0.682
